<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Antonio's Portfolio - Python</title><link href="https://antoniocampos13.github.io/" rel="alternate"></link><link href="https://antoniocampos13.github.io/feeds/python.atom.xml" rel="self"></link><id>https://antoniocampos13.github.io/</id><updated>2024-04-22T14:30:00-03:00</updated><subtitle>PhD in Genetics</subtitle><entry><title>Training and Evaluating a Neural NetworkÂ Model</title><link href="https://antoniocampos13.github.io/training-and-evaluating-a-neural-network-model.html" rel="alternate"></link><published>2024-04-22T14:30:00-03:00</published><updated>2024-04-22T14:30:00-03:00</updated><author><name>Antonio Victor Campos Coelho</name></author><id>tag:antoniocampos13.github.io,2024-04-22:/training-and-evaluating-a-neural-network-model.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In my &lt;a href="https://antoniocampos13.github.io/analyzing-scrna-seq-data-with-xgboost.html#analyzing-scrna-seq-data-with-xgboost"&gt;previous post&lt;/a&gt;, I trained an XGBoost machine-learning model with single-cell &lt;span class="caps"&gt;RNA&lt;/span&gt;-Seq (scRNA-Seq) data to differentiate cell identity (parental cells versus paclitaxel-resistant cells) based on transcriptomic&amp;nbsp;patterns.&lt;/p&gt;
&lt;p&gt;As an exercise, I decided to use the same input data to experiment with other machine-learning models. In this post â€¦&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In my &lt;a href="https://antoniocampos13.github.io/analyzing-scrna-seq-data-with-xgboost.html#analyzing-scrna-seq-data-with-xgboost"&gt;previous post&lt;/a&gt;, I trained an XGBoost machine-learning model with single-cell &lt;span class="caps"&gt;RNA&lt;/span&gt;-Seq (scRNA-Seq) data to differentiate cell identity (parental cells versus paclitaxel-resistant cells) based on transcriptomic&amp;nbsp;patterns.&lt;/p&gt;
&lt;p&gt;As an exercise, I decided to use the same input data to experiment with other machine-learning models. In this post, I will demonstrate how I trained a neural network model with the &lt;a href="https://pytorch.org/"&gt;&lt;code&gt;PyTorch&lt;/code&gt;&lt;/a&gt; framework. &lt;code&gt;PyTorch&lt;/code&gt; is a module designed for the development of deep learning using GPUs and CPUs in&amp;nbsp;Python.&lt;/p&gt;
&lt;p&gt;As usual, the code is available on my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2024_04_19_Training_and_Evaluating_a_Neural_Network_Model"&gt;code portfolio at GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Code&amp;nbsp;outline&lt;/h2&gt;
&lt;p&gt;The code for the demonstration is divided into two files: &lt;code&gt;src/pytorch_nn_demo.py&lt;/code&gt; and &lt;code&gt;main.py&lt;/code&gt;. The first contains classes, function definitions, and overall configurations, whereas the second contains the data import and actual execution of the code. If you would like to recap concepts about machine learning and neural networks before continuing, you may check the Appendix&amp;nbsp;section.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s start with &lt;code&gt;src/pytorch_nn_demo.py&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;&lt;code&gt;src/pytorch_nn_demo.py&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;I recommend setting up a Python &lt;a href="https://docs.python.org/3/library/venv.html"&gt;virtual environment&lt;/a&gt;. After you create the environment, install the required modules listed in the &lt;code&gt;requirements.txt&lt;/code&gt; file. Install &lt;a href="https://developer.nvidia.com/cuda-downloads"&gt;&lt;span class="caps"&gt;CUDA&lt;/span&gt;&lt;/a&gt; for your platform. I used version 12.4.1. I believe the code will work without &lt;span class="caps"&gt;CUDA&lt;/span&gt;, but I have not tested the code in this&amp;nbsp;situation.&lt;/p&gt;
&lt;p&gt;I start the script by importing all necessary&amp;nbsp;modules:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# src/pytorch_nn_demo.py&lt;/span&gt;
&lt;span class="c1"&gt;# %% Python 3.10.4 | Pytorch 1.12.1 | CUDA 12.4.1&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;copy&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections.abc&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;chain&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;typing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Type&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nn&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;roc_auc_score&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.utils.data&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DataLoader&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I set the computation device depending on the available&amp;nbsp;hardware:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# src/pytorch_nn_demo.py&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_available&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;device&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cuda:0&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;device&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;cpu&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If a &lt;span class="caps"&gt;GPU&lt;/span&gt; is present and is compatible with &lt;span class="caps"&gt;CUDA&lt;/span&gt;, then all computations will be performed with it, and if not, the &lt;span class="caps"&gt;CPU&lt;/span&gt; will be used&amp;nbsp;instead.&lt;/p&gt;
&lt;p&gt;Next, I configured the neural network model, which I named &lt;code&gt;NeuralNetwork&lt;/code&gt;, which inherits from a base module from&amp;nbsp;PyTorch:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# src/pytorch_nn_demo.py&lt;/span&gt;
&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;NeuralNetwork&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Module&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;n_neurons&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;n_output&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;activation_function&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;bool&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;super&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_features&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_neurons&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_neurons&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_output&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;activation_function&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;activation_function&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_neurons&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_neurons&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_neurons&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Linear&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_neurons&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Sigmoid&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;double&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;forward&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;fwd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;activation_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;fwd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fwd&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;fwd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;activation_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fwd&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;fwd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fwd&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;fwd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sigmoid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;layer3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fwd&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;fwd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can see that I defined two methods for the class: &lt;code&gt;__init__()&lt;/code&gt; and &lt;code&gt;forward()&lt;/code&gt;. The first serves to instantiate both the base module (see the &lt;code&gt;super().__init__()&lt;/code&gt; snippet) and our modified module. The &lt;code&gt;__init__()&lt;/code&gt; takes as&amp;nbsp;arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;n_features: the number of independent variables (columns of the dataset&amp;nbsp;matrix);&lt;/li&gt;
&lt;li&gt;n_neurons: the desired number of neurons on the hidden&amp;nbsp;layers;&lt;/li&gt;
&lt;li&gt;n_output: the dimensionality the of output (dependent&amp;nbsp;variable);&lt;/li&gt;
&lt;li&gt;activation_function: one of several activation functions available in&amp;nbsp;PyTorch;&lt;/li&gt;
&lt;li&gt;dropout: a proportion of neurons to be randomly discarded in each layer&amp;nbsp;independently.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;__init__()&lt;/code&gt; instantiates all given arguments as attributes of the class. Additionally, I&amp;nbsp;instantiate:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;a series of linear functions that will compose the layers of my&amp;nbsp;model.&lt;/li&gt;
&lt;li&gt;the &lt;a href="https://en.wikipedia.org/wiki/Sigmoid_function"&gt;sigmoid&lt;/a&gt; function: it takes the output of the last hidden layer (which is a matrix) and converts it to a vector/matrix of probabilities, depending on the number of output classes. For binary classification problems (which is my case), there are two classes: parental tumor cells and paclitaxel-resistant tumor cells. Then, the analysts can choose between &lt;a href="https://datascience.stackexchange.com/a/54606"&gt;soft and hard classification&lt;/a&gt;. In soft classification, you have a continuous distribution on the two classes (a vector with &lt;span class="math"&gt;\(S\)&lt;/span&gt; rows &lt;span class="math"&gt;\(\times\)&lt;/span&gt; 1 column with the probabilities, which is my case), whereas in hard classification the output is a matrix with &lt;span class="math"&gt;\(S\)&lt;/span&gt; rows &lt;span class="math"&gt;\(\times\)&lt;/span&gt; &lt;span class="math"&gt;\(C\)&lt;/span&gt; columns, where &lt;span class="math"&gt;\(S\)&lt;/span&gt; is the number of samples and &lt;span class="math"&gt;\(C\)&lt;/span&gt; the number of classes (2 or&amp;nbsp;more).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;forward()&lt;/code&gt; method is responsible for performing the neural network forward pass calculations. See that it follows a specific order: the first two layers are my hidden layers, whereas the final one is my output layer. The first hidden layer takes the input and performs the linear combination of feature values and weights. The linear output is then handed over to the activation function, which is a non-linear function. The output of the first activation function is handed over to the second hidden layer, which performs the same steps, and, finally, the output layer takes the non-linear output of the second layer and converts it to a vector of probabilities with the help of the sigmoid&amp;nbsp;function.&lt;/p&gt;
&lt;p&gt;Notice the &lt;code&gt;self.dropout(fwd)&lt;/code&gt; &lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;sandwiched&amp;#8221; between the layer&amp;#8217;s calls. This indicates to the model that some of the weights set for each layer should be discarded. Since there are two dropout calls, they are performed on the weights of the first and second hidden layers. By the way, since my model contains two hidden layers, technically, we may call it a &lt;strong&gt;deep learning&lt;/strong&gt;&amp;nbsp;model.&lt;/p&gt;
&lt;p&gt;Now, I will skip ahead to the core training/testing function I devised, the &lt;code&gt;model_train_eval()&lt;/code&gt; function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# src/pytorch_nn_demo.py&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;model_train_eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;NeuralNetwork&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;n_epochs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;train_dataloader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;test_dataloader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;output_path&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;complementary_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;bool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exists&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mkdir&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;best_roc_auc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inf&lt;/span&gt;
    &lt;span class="n"&gt;best_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_epochs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;train_dataloader&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_dataloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;test_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;roc_auc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;model_weights&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model_weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;test_dataloader&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test_dataloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;complementary_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;complementary_prob&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Epoch &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;/&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;n_epochs&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;. Train loss: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;. Test loss: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;test_loss&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;. ROC AUC: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;roc_auc&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;roc_auc&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;best_roc_auc&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;best_roc_auc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;roc_auc&lt;/span&gt;
            &lt;span class="n"&gt;best_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_weights&lt;/span&gt;

            &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;best_weights&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;path&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;best_weights.pth&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Current best epoch: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This functions take as&amp;nbsp;inputs:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;our configured neural network/deep learning&amp;nbsp;model;&lt;/li&gt;
&lt;li&gt;the number of training/testing iterations&amp;nbsp;(epochs);&lt;/li&gt;
&lt;li&gt;two &lt;strong&gt;dataloaders&lt;/strong&gt;: two objects for handling the datasets, one for training and another for testing dataset (more on that&amp;nbsp;later);&lt;/li&gt;
&lt;li&gt;the desired loss&amp;nbsp;function;&lt;/li&gt;
&lt;li&gt;the desired weight&amp;nbsp;optimizer;&lt;/li&gt;
&lt;li&gt;an output path to save the final model weights after finishing&amp;nbsp;training;&lt;/li&gt;
&lt;li&gt;a Boolean to indicate if complementary probabilities of the predictions should be used during testing (more on that later). Defaults to &lt;code&gt;False&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The gist of this function is that, during the specified number of epochs, it will train the model, calculate the loss metric, and update the weights for each hidden layer. At the end of each epoch, it will then evaluate the current state of the model to assess the performance of the model, by producing predictions with the testing dataset and then comparing with the actual labels. Then, it will save the weights of the best model achieved at the desired output path. The model will print to standard output the current training and testing loss metrics, the current &lt;span class="caps"&gt;ROC&lt;/span&gt; &lt;span class="caps"&gt;AUC&lt;/span&gt;, and the epoch with the current best&amp;nbsp;performance.&lt;/p&gt;
&lt;p&gt;What happens during each epoch? In each epoch, the &lt;code&gt;DataLoader&lt;/code&gt; object will take a subset, a batch, of samples from the dataset in use and perform the forward and backward pass calculations, as well as update the weights of each sample in the batch. In this way, models may be efficiently trained with large datasets without loading everything at once on memory. Thus, an epoch consists of a series of iterations on the dataset, until all computations are performed on all samples. In other words, an epoch is an iteration of&amp;nbsp;iterations.&lt;/p&gt;
&lt;p&gt;Now that I presented the overall function, I will then show the subfunctions: &lt;code&gt;train()&lt;/code&gt; and &lt;code&gt;test()&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# src/pytorch_nn_demo.py&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;train&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;NeuralNetwork&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;train_dataloader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;running_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;train_dataloader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;
        &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zero_grad&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;backward&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;running_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;train_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;running_loss&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_dataloader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;model_weights&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;deepcopy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;state_dict&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;train_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_weights&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This &lt;code&gt;train()&lt;/code&gt; function was inspired by &lt;a href="https://www.kaggle.com/code/robikscube/train-your-first-pytorch-model-card-classifier/notebook"&gt;Rob Mulla&amp;#8217;s tutorial&lt;/a&gt;, with modifications. The function performs one round of training (one epoch), by taking one batch from the training dataset at a time and performing the forward pass calculations with our model definition (&lt;code&gt;outputs = model(features)&lt;/code&gt;) and the loss metric calculation (by comparing the predictions with actual labels). PyTorch&amp;#8217;s methods &lt;code&gt;backward()&lt;/code&gt; and &lt;code&gt;step()&lt;/code&gt; are responsible for automatically calculating optimized weights for all previous layers. The function then returns the loss metric and the current model weights and biases for one round of training. As you probably noticed, we simply had to configure the forward pass calculations, since PyTorch handles most of the more complicated calculations&amp;nbsp;automatically.&lt;/p&gt;
&lt;p&gt;Now let&amp;#8217;s see the &lt;code&gt;train()&lt;/code&gt; function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# src/pytorch_nn_demo.py&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;NeuralNetwork&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;model_weights&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;FloatTensor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;test_dataloader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Type&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Callable&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;complementary_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;bool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;

    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_state_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model_weights&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;running_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;
    &lt;span class="n"&gt;labels_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="n"&gt;predictions_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;no_grad&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;test_dataloader&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;batch&lt;/span&gt;
            &lt;span class="n"&gt;labels_list&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
            &lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;labels&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;outputs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;y_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;predictions_list&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

            &lt;span class="n"&gt;loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outputs&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;running_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;labels&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="n"&gt;test_loss&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;running_loss&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_dataloader&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;labels_flat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chain&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_iterable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labels_list&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;complementary_prob&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;predictions_flat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;el&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;el&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chain&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_iterable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions_list&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;predictions_flat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chain&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_iterable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions_list&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="n"&gt;roc_auc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;roc_auc_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labels_flat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_score&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predictions_flat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;test_loss&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;roc_auc&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The train function loads the weights and biases produced during the training round and loads them into the model class object. For each batch of the testing dataset, the function stores the actual labels and the predicted labels in two separate lists, &lt;code&gt;labels_list&lt;/code&gt; and &lt;code&gt;predictions_list&lt;/code&gt;, respectively. The predictions are generated by using the features of the batch as input for the model. The function then returns the loss metric of the model and the &lt;span class="caps"&gt;ROC&lt;/span&gt; &lt;span class="caps"&gt;AUC&lt;/span&gt; performance&amp;nbsp;metric.&lt;/p&gt;
&lt;p&gt;Thus, by chaining &lt;code&gt;train()&lt;/code&gt; and &lt;code&gt;test()&lt;/code&gt; into &lt;code&gt;model_train_eval()&lt;/code&gt;, I can perform any number of epochs and keep track of how the model performs. Next, let&amp;#8217;s put the code to&amp;nbsp;work.&lt;/p&gt;
&lt;h3&gt;&lt;code&gt;main.py&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;As usual, I start by importing all necessary modules, including the objects defined in &lt;code&gt;src/pytorch_nn_demo.py&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# main.py&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;itertools&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;chain&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.nn&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;nn&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;torch.optim&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;optim&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;roc_auc_score&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.utils.data&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DataLoader&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.utils.data.dataset&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;TensorDataset&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;src.pytorch_nn_demo&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;NeuralNetwork&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_train_eval&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I set up the file&amp;nbsp;paths:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;ROOT_DIR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cwd&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;TRAIN_METADATA_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ROOT_DIR&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;input/train_metadata.tsv&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;TEST_METADATA_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ROOT_DIR&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;input/test_metadata_filtered.tsv&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;TRAIN_COUNT_MATRIX_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ROOT_DIR&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;input/train_counts_transformed_scaled.tsv.gz&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;TEST_COUNT_MATRIX_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ROOT_DIR&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;input/test_counts_transformed_scaled.tsv.gz&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;FINAL_GENE_LIST_PATH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ROOT_DIR&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;input/final_gene_list.tsv&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;OUTPUT_DIR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ROOT_DIR&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;output&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Remember that the input files come from my &lt;a href="https://antoniocampos13.github.io/analyzing-scrna-seq-data-with-xgboost.html"&gt;previous post&lt;/a&gt;. Therefore, I will re-use some code, with little modifications to load all dataset objects into &lt;code&gt;pandas.DataFrame&lt;/code&gt; objects:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# main.py&lt;/span&gt;
&lt;span class="c1"&gt;# %% Import refined gene list&lt;/span&gt;
&lt;span class="n"&gt;gene_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;FINAL_GENE_LIST_PATH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;gene_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gene_list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;index&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_list&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# %% Import train dataset&lt;/span&gt;
&lt;span class="n"&gt;train_count_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TRAIN_COUNT_MATRIX_PATH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_col&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;train_count_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_count_matrix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_count_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene_list&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;train_count_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_count_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;train_count_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_count_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;train_metadata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TRAIN_METADATA_PATH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;train_metadata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;label&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_metadata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;label&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# %% Import test dataset&lt;/span&gt;
&lt;span class="n"&gt;test_count_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TEST_COUNT_MATRIX_PATH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index_col&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;test_count_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_count_matrix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;test_count_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene_list&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;test_count_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_count_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_index&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;test_count_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_count_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transpose&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;test_metadata&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TEST_METADATA_PATH&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;test_metadata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;label&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;test_metadata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;label&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Remember that the datasets have the samples (single breast tumor cells) as rows and features as columns (in this case, gene expression). Therefore, our trained neural network/deep learning model will differentiate (classify) between parental and paclitaxel-resistant&amp;nbsp;cells.&lt;/p&gt;
&lt;p&gt;To work with PyTorch, our data must be transformed into the &lt;code&gt;torch&lt;/code&gt; tensor format. In my case, I will convert the two datasets into two &lt;code&gt;TensorDataset&lt;/code&gt; objects:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;train_tensor_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TensorDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_count_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_metadata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;label&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;test_tensor_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;TensorDataset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_count_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_metadata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;label&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, I can generate two &lt;code&gt;DataLoader&lt;/code&gt; objects so the datasets are amenable for batching during training and testing&amp;nbsp;rounds:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# main.py&lt;/span&gt;
&lt;span class="c1"&gt;# %% Prepare datasets for batching&lt;/span&gt;
&lt;span class="n"&gt;train_dataloader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_tensor_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;test_dataloader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_tensor_dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;512&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Notice that &lt;code&gt;batch_size&lt;/code&gt; argument controls the size of the batch. Thus, I defined 512 observations per batch. Notice as well that the training dataset may be shuffled because it helps prevent the model from memorizing the order of the data. See Hengtao Tantai&amp;#8217;s review and tips &lt;a href="https://medium.com/@zergtant/improving-control-and-reproducibility-of-pytorch-dataloader-with-sampler-instead-of-shuffle-7f795490256e"&gt;here&lt;/a&gt;. No shuffling is necessary on the testing&amp;nbsp;dataset.&lt;/p&gt;
&lt;p&gt;Next, I configured the neural network/deep learning&amp;nbsp;model:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# main.py&lt;/span&gt;
&lt;span class="c1"&gt;# %% Configure neural network&lt;/span&gt;
&lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_count_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;n_output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="n"&gt;n_neurons&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;n_output&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# rule of thumb&lt;/span&gt;
&lt;span class="n"&gt;dropout_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;
&lt;span class="n"&gt;activation_function&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ReLU&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;NeuralNetwork&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_neurons&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_neurons&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_output&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n_output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;activation_function&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;activation_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;dropout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dropout_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The datasets have 1253 features (gene expression columns) and have two classes: parental or paclitaxel-resistant, therefore, it can by represented by a single column of probabilities varying between 0 and 1. The higher the probability, the higher the chance that the tumor cells is&amp;nbsp;paclitaxel-resistant.&lt;/p&gt;
&lt;p&gt;There a few rules of thumb to determine the number of neurons in the hidden layers. See &lt;a href="https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3"&gt;Sandhya Krishan&amp;#8217;s post&lt;/a&gt; for a review. Here, I chose to use two thirds of the sum of inputs plus outputs (&lt;span class="math"&gt;\((1253 + 1) \times 2/3 = 836\)&lt;/span&gt;). I could have set different number of neurons for each layer, but to keep the model simple, they will have the same number of&amp;nbsp;neurons.&lt;/p&gt;
&lt;p&gt;I chose the rectified linear activation function or &lt;strong&gt;ReLU&lt;/strong&gt; as the activation function of the hidden layers. Briefly, the ReLU is a non-linear function that returns the input as is only if its is higher than zero. Otherwise, if the input is zero or less, it will always return zero. See a brief review about ReLU &lt;a href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/"&gt;here&lt;/a&gt; and why its characteristics are desirable during neural networks&amp;nbsp;training.&lt;/p&gt;
&lt;p&gt;Next, I defined the adaptive moment estimation algorithm (&lt;a href="https://medium.com/@francescofranco_39234/adam-optimization-in-machine-learning-cfeb10a27a86"&gt;&lt;span class="caps"&gt;ADAM&lt;/span&gt;&lt;/a&gt;) with its default learning rateas the weights and bias optimizer, and the binary cross-entropy (&lt;a href="https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/"&gt;&lt;span class="caps"&gt;BCE&lt;/span&gt;&lt;/a&gt;) as the loss&amp;nbsp;function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# main.py&lt;/span&gt;
&lt;span class="c1"&gt;# %% Run training/evaluation iterations&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;loss_function&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BCELoss&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, I can train/test the model for 250 epochs, and saving the best weights during all epochs into the &lt;code&gt;output&lt;/code&gt; folder:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# main.py&lt;/span&gt;
&lt;span class="n"&gt;model_train_eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;train_dataloader&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_dataloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;test_dataloader&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test_dataloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;output_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;output&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;While the training is occurring, summary of each epoch is printed to the standard&amp;nbsp;output:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Printing epochs summary during training of the neural network model" src="https://antoniocampos13.github.io/images/training_neural_network.png"&gt;&lt;/p&gt;
&lt;p&gt;As you may see on the image above, the &lt;span class="caps"&gt;ROC&lt;/span&gt; &lt;span class="caps"&gt;AUC&lt;/span&gt; was trending around 0.23 during the epochs. Since it is &lt;span class="math"&gt;\(&amp;lt;\)&lt;/span&gt; 0.50, the model is actualy a bad predictor for the positive class (paclitaxel-resistant cells). Apparently, it captured the patterns from parental cells better than the resistant ones. However, if we invert the model output by using their complementary probabilities, we can achieve &lt;span class="caps"&gt;ROC&lt;/span&gt; &lt;span class="caps"&gt;AUC&lt;/span&gt; &lt;span class="math"&gt;\(&amp;gt;\)&lt;/span&gt;&amp;nbsp;0.50:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;
&lt;span class="c1"&gt;# %% Check evaluation using complementary probabilities&lt;/span&gt;
&lt;span class="n"&gt;model_train_eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;n_epochs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;250&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;train_dataloader&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;train_dataloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;test_dataloader&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test_dataloader&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;loss_function&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;output_path&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;output&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;complementary_prob&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;During the new training, we see the &lt;span class="caps"&gt;AUC&lt;/span&gt; &lt;span class="caps"&gt;ROC&lt;/span&gt; score&amp;nbsp;improved:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Printing epochs summary during training of the neural network model (complementary probability of the output)" src="https://antoniocampos13.github.io/images/training_neural_network_comp_prob.png"&gt;&lt;/p&gt;
&lt;p&gt;As I mentioned above, the weights and biases of the best model were saved in the &lt;code&gt;output&lt;/code&gt; folder. To load them into the model and perform predictions, follow the steps&amp;nbsp;below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# main.py&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load_state_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;output/best_weights.pth&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The model is now ready and loaded. Let&amp;#8217;s evaluate the model. Obtain predictions by running the model with a feature matrix. For this example, I will use again the features from test dataset, after converting them to a PyTorch&amp;nbsp;tensor:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# main.py&lt;/span&gt;
&lt;span class="n"&gt;out&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_count_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;device&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;predictions_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="c1"&gt;# %% Convert PyTorch Tensor to Python list&lt;/span&gt;
&lt;span class="n"&gt;y_preds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;squeeze&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;out&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;predictions_list&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_preds&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tolist&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="c1"&gt;## Use Complementary probabilities (this step is specific for this demonstration only. Do not perform on your data, unless you know what you are doing)&lt;/span&gt;
&lt;span class="n"&gt;predictions_flat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;el&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;el&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chain&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_iterable&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;predictions_list&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;

&lt;span class="c1"&gt;# %% Calculate ROC AUC score&lt;/span&gt;
&lt;span class="n"&gt;eval_roc_score&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;roc_auc_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_true&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;test_metadata&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;label&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;y_score&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;predictions_flat&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;eval_roc_score&lt;/span&gt;
&lt;span class="c1"&gt;# 0.7445068061155242&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So I can see the final &lt;span class="caps"&gt;ROC&lt;/span&gt; &lt;span class="caps"&gt;AUC&lt;/span&gt; score converged to around 0.74. You may obtain a slightly different number due to the randomness introduced during shuffling of the drain dataset during&amp;nbsp;epochs.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post I demonstrated how to configure a neural network/deep learning model through PyTorch, as well as how to configure a simple function to perform training and testing rounds of the&amp;nbsp;model.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;&lt;span class="caps"&gt;APPENDIX&lt;/span&gt; - Neural network and deep learning models: a brief&amp;nbsp;recap&lt;/h2&gt;
&lt;p&gt;Machine-learning models, in general, aim to capture patterns in a dataset composed of &lt;strong&gt;features&lt;/strong&gt; (independent variables) to predict the behavior of another variable, the &lt;strong&gt;output&lt;/strong&gt;, &lt;strong&gt;dependent&lt;/strong&gt; or &lt;strong&gt;status&lt;/strong&gt; variable. Usually, we select a series of examples of &lt;strong&gt;labeled&lt;/strong&gt; observations (samples) of those features (i.e. we know the status for each sample) and apply the desired model. This dataset is called the &lt;strong&gt;training dataset&lt;/strong&gt;, and because of that, this rationale is called &lt;strong&gt;supervised learning&lt;/strong&gt;. The objective of supervised learning is to obtain a statistical model capable of predicting the status of any other observation. We assess the model by applying the model to a &lt;strong&gt;test dataset&lt;/strong&gt;, a completely separate collection of observations to ensure that the model works even with samples not previously&amp;nbsp;encountered.&lt;/p&gt;
&lt;p&gt;We assess the utility of the model during both phases: training and testing. During training, we compare the predictions of the model with the actual labels and calculate a &lt;strong&gt;loss (or cost) metric&lt;/strong&gt;, which is a global metric that denotes how wrong was the model. Thus, all machine learning models strive to &lt;strong&gt;minimize&lt;/strong&gt; this loss metric as much as possible during training. With neural networks/deep learning models, this is performed iteratively via a &lt;a href="https://medium.com/@yennhi95zz/4-a-beginners-guide-to-gradient-descent-in-machine-learning-773ba7cd3dfe"&gt;gradient descent algorithm&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Briefly, the gradient descent algorithm works by optimizing a series of &lt;strong&gt;weights&lt;/strong&gt;, i.e., &lt;strong&gt;parameters (or coefficients)&lt;/strong&gt; (usually represented by the symbols &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; or &lt;span class="math"&gt;\(w\)&lt;/span&gt;) for each observation. As a simple example, suppose that &lt;span class="math"&gt;\(y\)&lt;/span&gt; represents a probability and &lt;span class="math"&gt;\(\{x_1, x_2, x_3, ..., x_n\}\)&lt;/span&gt; represent the values of &lt;span class="math"&gt;\(n\)&lt;/span&gt; features. Therefore, the&amp;nbsp;formula:&lt;/p&gt;
&lt;div class="math"&gt;$$y = w_1 \times x_1 + w_1 \times x_2 + w_3 \times x_3 + ... + w_n \times x_n$$&lt;/div&gt;
&lt;p&gt;relates the influence (hence the name &lt;em&gt;weight&lt;/em&gt;) of each feature over the final probability. Thus, at each iteration, the model updates those weights based on the loss metric of the last iteration. Typically, we choose a number of iterations, or &lt;strong&gt;epochs&lt;/strong&gt;, and assess at each epoch (or every few epochs, given a pre-specified interval), if the loss metric is satisfactorily decreasing. Neural network models also optimize &lt;strong&gt;biases&lt;/strong&gt;, which are numeric constants that represent systematic errors in the data. The biases are added to each feature/weight multiplication product. For the sake of simplicity, I will not explore biases further, but keep in mind that whenever I mention weights, assume that biases are also&amp;nbsp;involved.&lt;/p&gt;
&lt;p&gt;How do neural networks derive those weights? We may conceptualize a neural network model as a series of sets of mathematical functions chained together, where the outputs of one set of functions are passed to the next set, and so on, until the last step, in which the loss metric is calculated (hence the name &lt;em&gt;neural network&lt;/em&gt;, each set of functions are connected and share information in the same way that biological neurons do). Each set of functions is called a &lt;strong&gt;layer&lt;/strong&gt;. Usually, each layer contains a linear combination (multiplication) of feature values times the weights. This combination is then handed over to a non-linear function, called the &lt;strong&gt;activation&lt;/strong&gt; function. The activation function processes the input and hands it over to the next layer of the model. After the weights of all layers are computed (the &lt;strong&gt;forward pass&lt;/strong&gt; computation), then we take the partial derivatives of the loss function with respect to all other operations (the &lt;strong&gt;backward pass&lt;/strong&gt; computation). The result is a matrix of &lt;strong&gt;gradients&lt;/strong&gt;, numerical values representing the influence of each weight on the final value of the loss function. Since the derivative of a function helps to find the minimum value of the original function, the model can update the starting weights for all layers in the next epoch, adjusting each weight in the direction that will minimize the loss metric, as mentioned above. Rinse and repeat until the pre-specified epochs end or the analyst is satisfied with the loss metric&amp;nbsp;achieved.&lt;/p&gt;
&lt;p&gt;The layers of a neural network are usually&amp;nbsp;called:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Input layer&lt;/strong&gt;: contains the feature&amp;nbsp;matrix&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hidden layer(s)&lt;/strong&gt;: contain(s) neurons, activation functions work here. Each layer has its particular weight&amp;nbsp;matrix.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Output layer&lt;/strong&gt;: the loss metric calculation takes place here. The start of the backward pass takes place here. The weights for each previous layer are then&amp;nbsp;updated.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The input layer is the &amp;#8220;zeroeth&amp;#8221; layer: it is represented by the feature values matrix, and no operations are performed&amp;nbsp;here.&lt;/p&gt;
&lt;p&gt;The hidden layers are the core of the model. All linear and activation functions are set at the hidden layers. They are called hidden layers because, usually, we do not check the numbers generated there, they are simply intermediate values for the output layer. The hidden layers operate by setting weight matrices with the &lt;a href="https://www.oreilly.com/library/view/deep-learning-from/9781492041405/"&gt;dimensionality of the vector&lt;/a&gt; that represents each observation in the layer&amp;#8217;s output. The number that represents this dimensionality is called the &lt;strong&gt;neurons&lt;/strong&gt;. For example, imagine a neural network with one input layer, one hidden layer and one output layer. If we say that the input layer is a &lt;span class="math"&gt;\(S\)&lt;/span&gt; (samples) X &lt;span class="math"&gt;\(F\)&lt;/span&gt; (features) matrix, the output of the first hidden layer will have &lt;span class="math"&gt;\(N\)&lt;/span&gt; (neurons) X &lt;span class="math"&gt;\(F\)&lt;/span&gt; (features) dimensions. The number of neurons is pre-specified by the analyst. There are some &lt;a href="https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3"&gt;rules of thumb&lt;/a&gt; for choosing the number of neurons in each layer, because too few neurons may result in suboptimal training, whereas too many neurons may cause overfitting of the model. The analyst may choose to apply neuron &lt;strong&gt;dropout&lt;/strong&gt; rates. By pre-specifying a dropout rate (&lt;span class="caps"&gt;DR&lt;/span&gt;, in %), the analyst randomly discards the weights of &lt;span class="caps"&gt;DR&lt;/span&gt;% of the neurons at each layer independently (usually). Discard means the weights receive a value of zero. The neuron dropout is one of the most frequently used methods to reduce the risk of&amp;nbsp;overfitting.&lt;/p&gt;
&lt;p&gt;The output layer aggregates the computations from the previous layers and calculates the loss metric, which is used to update the weights. The weights are updated with the help of functions called &lt;strong&gt;optimizers&lt;/strong&gt;, which use a &lt;strong&gt;learning rate&lt;/strong&gt; to gauge how strongly the update in the value of the weights will be. A learning rate too small will result in a model taking longer to converge to a minimum value of the loss function, whereas a learning rate too large may find suboptimal results during&amp;nbsp;training.&lt;/p&gt;
&lt;p&gt;A neural network with two or more layers is called a &lt;strong&gt;deep learning model&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;During testing, we assess &lt;strong&gt;performance metrics&lt;/strong&gt;. Those metrics include, for example, &lt;a href="https://en.wikipedia.org/wiki/Accuracy_and_precision"&gt;accuracy&lt;/a&gt;, &lt;a href="https://neptune.ai/blog/performance-metrics-in-machine-learning-complete-guide"&gt;regression metrics&lt;/a&gt;, and the area under the receiver operating characteristic curve &lt;a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic"&gt;&lt;span class="caps"&gt;ROC&lt;/span&gt; &lt;span class="caps"&gt;AUC&lt;/span&gt;&lt;/a&gt;. We can even evaluate the model performance at each epoch (or every few epochs, given a pre-specified interval), in pace with the training&amp;nbsp;procedure.&lt;/p&gt;
&lt;p&gt;In summary, neural networks are nothing more than a series of linear matrix multiplication operations interspersed with non-linear operations. These procedural steps help capture linear and non-linear patterns in the data. The final result is a matrix of weights. The linear combination of feature values and weights (multiplication) captures all those patterns, helping predictions. We assess those predictions by using a separate dataset to evaluate if the model can correctly predict most of the previously known&amp;nbsp;labels.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://pytorch.org/"&gt;PyTorch&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/venv.html"&gt;venv â€” Creation of virtual&amp;nbsp;environments&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://developer.nvidia.com/cuda-downloads"&gt;&lt;span class="caps"&gt;CUDA&lt;/span&gt; Toolkit 12.1&amp;nbsp;Downloads&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Sigmoid_function"&gt;Sigmoid function |&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://datascience.stackexchange.com/a/54606"&gt;Data Science Exchange | Answer to &amp;#8220;Binary classification as a 2-class classification&amp;nbsp;problem&amp;#8221;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/code/robikscube/train-your-first-pytorch-model-card-classifier/notebook"&gt;Train Your first PyTorch Model [Card Classifier] | Rob&amp;nbsp;Mulla&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@zergtant/improving-control-and-reproducibility-of-pytorch-dataloader-with-sampler-instead-of-shuffle-7f795490256e"&gt;Improving Control and Reproducibility of PyTorch DataLoader with Sampler Instead of Shuffle | Hengtao&amp;nbsp;Tantai&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3"&gt;How do determine the number of layers and neurons in the hidden layer? | Sandhya&amp;nbsp;Krishnan&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/"&gt;A Gentle Introduction to the Rectified Linear Unit (ReLU) | Jason&amp;nbsp;Brownlee&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@francescofranco_39234/adam-optimization-in-machine-learning-cfeb10a27a86"&gt;&lt;span class="caps"&gt;ADAM&lt;/span&gt; optimization in machine learning | Francesco&amp;nbsp;Franco&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.analyticsvidhya.com/blog/2021/03/binary-cross-entropy-log-loss-for-binary-classification/"&gt;Binary Cross Entropy/Log Loss for Binary Classification | Shipra&amp;nbsp;Saxena&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@yennhi95zz/4-a-beginners-guide-to-gradient-descent-in-machine-learning-773ba7cd3dfe"&gt;A Beginnerâ€™s Guide to Gradient Descent in Machine Learning | Yenn&amp;nbsp;Hi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.oreilly.com/library/view/deep-learning-from/9781492041405/"&gt;Deep Learning from Scratch | Seth&amp;nbsp;Weidman&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Accuracy_and_precision"&gt;Accuracy and precision |&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://neptune.ai/blog/performance-metrics-in-machine-learning-complete-guide"&gt;Performance Metrics in Machine Learning [Complete Guide] |&amp;nbsp;neptune.ai&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic"&gt;Receiver operating characteristic |&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Python"></category><category term="PyTorch"></category><category term="machine learning"></category><category term="transcriptomics"></category></entry><entry><title>Parsing the ClinVar XML file withÂ pandas</title><link href="https://antoniocampos13.github.io/parsing-the-clinvar-xml-file-with-pandas.html" rel="alternate"></link><published>2023-02-04T16:25:00-03:00</published><updated>2023-02-04T16:25:00-03:00</updated><author><name>Antonio Victor Campos Coelho</name></author><id>tag:antoniocampos13.github.io,2023-02-04:/parsing-the-clinvar-xml-file-with-pandas.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/intro/"&gt;ClinVar&lt;/a&gt; is one of the &lt;span class="caps"&gt;USA&lt;/span&gt;&amp;#8217;s National Center for Biotechnology Information (&lt;span class="caps"&gt;NCBI&lt;/span&gt;) databases. ClinVar archives reports of relationships among human genetic variants and phenotypes (usually genetic disorders). Any organization, such as a laboratory, hospital, clinic etc can submit data to ClinVar. The core idea of ClinVar is aggregate â€¦&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/intro/"&gt;ClinVar&lt;/a&gt; is one of the &lt;span class="caps"&gt;USA&lt;/span&gt;&amp;#8217;s National Center for Biotechnology Information (&lt;span class="caps"&gt;NCBI&lt;/span&gt;) databases. ClinVar archives reports of relationships among human genetic variants and phenotypes (usually genetic disorders). Any organization, such as a laboratory, hospital, clinic etc can submit data to ClinVar. The core idea of ClinVar is aggregate evidence for the clinical significance of any genetic variant concerning any disorder. Over 2,400 organizations contributed more than 2 million 600 thousand &lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/submitters/"&gt;records to ClinVar&lt;/a&gt;, representing more than 1 million 600 thousand unique&amp;nbsp;variants.&lt;/p&gt;
&lt;p&gt;Anyone can freely search ClinVar through their &lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/"&gt;website&lt;/a&gt;, using gene symbols, genomic coordinates, &lt;a href="https://varnomen.hgvs.org/"&gt;&lt;span class="caps"&gt;HGVS&lt;/span&gt; expressions&lt;/a&gt;, phenotypes, and more. If you want to perform a few queries, the online search tool does a good job. However, if you are pursuing more complex scientific questions, or are intending to download batches of data, the search tool will not suffice. Other &lt;span class="caps"&gt;NCBI&lt;/span&gt; databases can be queried via the command line with the &lt;a href="https://www.ncbi.nlm.nih.gov/books/NBK179288/"&gt;Entrez Direct (EDirect) utilities&lt;/a&gt; (in a &lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-2.html"&gt;previous post&lt;/a&gt; I mention how to work with the EDirect utilities). Unfortunately, ClinVar does not currently support a batch query interface via EDirect&amp;nbsp;utilities.&lt;/p&gt;
&lt;p&gt;However, ClinVar provides &lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/docs/maintenance_use/"&gt;other approaches&lt;/a&gt; for the access and use of their data. One of these approaches is the provisioning of the complete public data set in the form of an &lt;a href="https://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/"&gt;&lt;span class="caps"&gt;XML&lt;/span&gt; file stored at the ClinVar &lt;span class="caps"&gt;FTP&lt;/span&gt; server&lt;/a&gt;. The &lt;code&gt;ClinVarFullRelease&lt;/code&gt; &lt;span class="caps"&gt;XML&lt;/span&gt; file is updated weekly, and every release happening on the first Thursday of the month is&amp;nbsp;archived.&lt;/p&gt;
&lt;h2&gt;Parsing the ClinVar &lt;span class="caps"&gt;XML&lt;/span&gt;&amp;nbsp;file&lt;/h2&gt;
&lt;p&gt;Recently, I started assisting my team in uploading variant/phenotype interpretations to ClinVar. I wanted to find a way to gather all our submissions into a spreadsheet so every team member could easily check whenever necessary. Thus, I downloaded the full ClinVar release &lt;span class="caps"&gt;XML&lt;/span&gt; file and tried to parse it with the &lt;span class="caps"&gt;XML&lt;/span&gt;-handling &lt;a href="https://docs.python.org/3/library/xml.etree.elementtree.html"&gt;&lt;code&gt;ElementTree&lt;/code&gt; module&lt;/a&gt;. However, I had limited success. I could extract some information, but the output did not turn out exactly the way I was intending, so I set out to find working&amp;nbsp;alternatives.&lt;/p&gt;
&lt;p&gt;Eventually, I found out that the &lt;a href="https://pandas.pydata.org/"&gt;&lt;code&gt;pandas&lt;/code&gt; module&lt;/a&gt; has a method to convert &lt;span class="caps"&gt;XML&lt;/span&gt;-stored data into traditional data frames. Moreover, since September 2022, their &lt;code&gt;read_xml()&lt;/code&gt; function supports large &lt;span class="caps"&gt;XML&lt;/span&gt; files via the &lt;code&gt;iterparse&lt;/code&gt; argument (read an excerpt of the release note &lt;a href="https://pandas.pydata.org/docs/whatsnew/v1.5.0.html#read-xml-now-supports-large-xml-using-iterparse"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The function documentation states that the &lt;code&gt;iterparse&lt;/code&gt; argument is a memory-efficient method for handling big &lt;span class="caps"&gt;XML&lt;/span&gt; files without storing all data elements within memory. This was exactly my case, so I tried the &lt;code&gt;read_xml()&lt;/code&gt; function &amp;mdash; it worked quite&amp;nbsp;well!&lt;/p&gt;
&lt;p&gt;I wrote a small script that you can use to parse the ClinVar &lt;span class="caps"&gt;XML&lt;/span&gt; file. Of course, when you get acquainted with the &lt;code&gt;read_xml()&lt;/code&gt;, you may use it for parsing any other &lt;span class="caps"&gt;XML&lt;/span&gt; you wish. I used an &lt;a href="https://aws.amazon.com/ec2/?nc1=h_ls"&gt;&lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;EC2&lt;/span&gt; instance&lt;/a&gt; with 90 &lt;span class="caps"&gt;GB&lt;/span&gt; &lt;span class="caps"&gt;RAM&lt;/span&gt; while working on this tutorial. I did not try to process the ClinVar file in less powerful systems. &lt;em&gt;Try at your own risk&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I uploaded the script (named &lt;code&gt;clinvar_pandas_xml_parser.py&lt;/code&gt;) to &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2023_02_04_Parsing_ClinVar_XML_with_pandas"&gt;the corresponding folder on my portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Downloading the full ClinVar release &lt;span class="caps"&gt;XML&lt;/span&gt;&amp;nbsp;file&lt;/h3&gt;
&lt;p&gt;You can download the latest &lt;code&gt;.gz&lt;/code&gt;-compressed &lt;span class="caps"&gt;XML&lt;/span&gt; files via the following links (&lt;strong&gt;&lt;span class="caps"&gt;WARNING&lt;/span&gt;:&lt;/strong&gt; the release files are &lt;span class="caps"&gt;HUGE&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/weekly_release/ClinVarFullRelease_00-latest_weekly.xml.gz"&gt;Weekly&amp;nbsp;release&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/ClinVarFullRelease_00-latest.xml.gz"&gt;Monthly&amp;nbsp;release&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Go to a convenient directory on your system and download one of the files above. I downloaded the most recent monthly release and decompressed it soon&amp;nbsp;after:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;wget https://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/ClinVarFullRelease_00-latest.xml.gz

gunzip ClinVarFullRelease_00-latest.xml.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you want to check the file integrity, compare your checksum against the corresponding ClinVar-provided checksum&amp;nbsp;file:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/weekly_release/ClinVarFullRelease_00-latest_weekly.xml.gz.md5"&gt;Weekly release (&lt;span class="caps"&gt;MD5&lt;/span&gt; checksum&amp;nbsp;file)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/ClinVarFullRelease_00-latest.xml.gz.md5"&gt;Monthly release (&lt;span class="caps"&gt;MD5&lt;/span&gt; checksum&amp;nbsp;file)&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Installing&amp;nbsp;modules&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;iterparse&lt;/code&gt; argument in the &lt;code&gt;read_xml()&lt;/code&gt; function was introduced in &lt;code&gt;pandas&lt;/code&gt; version 1.5.0 and requires the &lt;code&gt;lxml&lt;/code&gt; or &lt;code&gt;ElementTree&lt;/code&gt; modules to work. In this tutorial, I will use &lt;code&gt;lxml&lt;/code&gt; (the default). Therefore, install the necessary modules via &lt;code&gt;pip&lt;/code&gt; or &lt;code&gt;conda&lt;/code&gt; (see my &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;previous post&lt;/a&gt; on how to configure &lt;code&gt;conda&lt;/code&gt; virtual environments in a Unix system). For&amp;nbsp;example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda activate env_name
conda install -c conda-forge &lt;span class="nv"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.5.0 lxml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Running the &lt;code&gt;clinvar_pandas_xml_parser.py&lt;/code&gt; script&lt;/h3&gt;
&lt;p&gt;Finally, let&amp;#8217;s walk through the script. First, I import the &lt;code&gt;pandas&lt;/code&gt; module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I saved the &lt;span class="caps"&gt;XML&lt;/span&gt; file path into the &lt;code&gt;xml_file_path&lt;/code&gt; object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;xml_file_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ClinVarFullRelease_00-latest.xml&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, I investigated the &lt;span class="caps"&gt;XML&lt;/span&gt; using &lt;code&gt;grep&lt;/code&gt; commands to match specific strings of interest to get a feel of how the &lt;span class="caps"&gt;XML&lt;/span&gt; file was structured. I am sure that are better ways to assess the &lt;span class="caps"&gt;XML&lt;/span&gt; elements structure, but I am not an expert in &lt;span class="caps"&gt;XML&lt;/span&gt;&amp;nbsp;files.&lt;/p&gt;
&lt;p&gt;Through my investigation of the file, I concluded that the &lt;code&gt;ClinVarAssertion&lt;/code&gt; elements within the &lt;span class="caps"&gt;XML&lt;/span&gt; structure contained all information I was needing at the moment. Thus, I created a Python dictionary object named &lt;code&gt;iterparse_dict&lt;/code&gt; with the string &amp;#8220;&lt;code&gt;ClinVarAssertion&lt;/code&gt;&amp;#8221; as a &lt;em&gt;key&lt;/em&gt; and a Python list as its corresponding &lt;em&gt;value&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;iterparse_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ClinVarAssertion&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[]}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;em&gt;key&lt;/em&gt; represents the parent &lt;span class="caps"&gt;XML&lt;/span&gt; node tag. The &lt;em&gt;value&lt;/em&gt; is a list containing all child or grandchild nodes, tags, or attributes at any node level inside the main &lt;span class="caps"&gt;XML&lt;/span&gt; node &amp;mdash; simple as that. I chose the&amp;nbsp;following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;iterparse_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;ClinVarAssertion&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;ID&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;SubmissionName&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;localKey&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;submittedAssembly&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;submitter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;submitterDate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;Acc&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;RecordStatus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;OrgID&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;DateCreated&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;DateUpdated&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;Version&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;DateLastEvaluated&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;Description&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;ReviewStatus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;Comment&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, I passed the &lt;code&gt;iterparse_dict&lt;/code&gt; as the value for the &lt;code&gt;iterparse&lt;/code&gt; argument of the &lt;code&gt;read_xml()&lt;/code&gt; function and stored the output as the &lt;code&gt;df&lt;/code&gt; object &amp;mdash; a &lt;code&gt;pandas.DataFrame&lt;/code&gt;. The columns of the data frame will correspond to the information stored at each &lt;code&gt;ClinVarAssertion&lt;/code&gt; tag,&amp;nbsp;attributes&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_xml&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xml_file_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;iterparse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;iterparse_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After some time, the function returns a data frame that you can further filter to search for information. For now, I saved the data frame as a pickled&amp;nbsp;object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pandas_parsed.pkl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;At any moment, I can restore the data frame through &lt;code&gt;pandas&lt;/code&gt; as well (&lt;strong&gt;&lt;span class="caps"&gt;REMEMBER&lt;/span&gt;:&lt;/strong&gt; Loading pickled data received from untrusted sources can be&amp;nbsp;unsafe):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pandas_parsed.pkl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I demonstrated one way of exploring the full release of the ClinVar database, through an up-to-date &lt;code&gt;pandas&lt;/code&gt; method that can deal with big &lt;span class="caps"&gt;XML&lt;/span&gt;&amp;nbsp;files.&lt;/p&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;p&gt;A brief explanation of what each &lt;em&gt;value&lt;/em&gt; in the &lt;code&gt;iterparse_dict&lt;/code&gt; dictionary object&amp;nbsp;represents:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ID&lt;/code&gt;: A unique numeric id representing a submission (a single submission usually contains many variant/phenotype&amp;nbsp;interpretations).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SubmissionName&lt;/code&gt;: A unique string representing a&amp;nbsp;submission.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;localKey&lt;/code&gt;: The &lt;span class="caps"&gt;HGVS&lt;/span&gt; expression representing each variant within a single&amp;nbsp;submission.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;submittedAssembly&lt;/code&gt;: The assembly (genome reference build) that was used for variant calling, annotation and localization. Usually is &amp;#8220;GRCh37&amp;#8221; or&amp;nbsp;&amp;#8220;GRCh38&amp;#8221;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;submitter&lt;/code&gt;: The organization that was responsible for the&amp;nbsp;submission.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;submitterDate&lt;/code&gt;: The date when the submission was uploaded to&amp;nbsp;ClinVar.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Acc&lt;/code&gt;: A ClinVar identifier string. As stated in the &lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/docs/identifiers/"&gt;ClinVar identifiers documentation&lt;/a&gt;: &amp;#8220;Accession numbers in ClinVar have the pattern of 3 letters and 9 numerals. The letters are either &lt;span class="caps"&gt;SCV&lt;/span&gt; (think of it as Submitted record in ClinVar), &lt;span class="caps"&gt;RCV&lt;/span&gt; (Reference ClinVar record) or &lt;span class="caps"&gt;VCV&lt;/span&gt; (Variation ClinVar&amp;nbsp;record).&amp;#8221;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RecordStatus&lt;/code&gt;: The status of the record, whether current, deleted or secondary&amp;nbsp;(merged).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;OrgID&lt;/code&gt;: A unique numeric identifier for each organization that was responsible for the&amp;nbsp;submission.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DateCreated&lt;/code&gt;: The date when the submission was accepted and integrated into the&amp;nbsp;database.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DateUpdated&lt;/code&gt;: The date when the submitter updated the&amp;nbsp;record.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Version&lt;/code&gt;: The version assigned to a record. As stated in the &lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/docs/identifiers/"&gt;ClinVar identifiers documentation&lt;/a&gt;: &amp;#8220;The version number is incremented when a submitter updates a record or when the contents of a reference or variation record change because of addition to, updates of, or deletion of the &lt;span class="caps"&gt;SCV&lt;/span&gt; accessions on which it is&amp;nbsp;based.&amp;#8221;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DateLastEvaluated&lt;/code&gt;: The date when the organization evaluated the clinical significance of any given variant in the context of any given&amp;nbsp;phenotype.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Description&lt;/code&gt;: A description of the clinical significance of any given variant in the context of any given phenotype, such as &amp;#8220;Pathogenic&amp;#8221;, &amp;#8220;Likely pathogenic&amp;#8221;, &amp;#8220;Benign&amp;#8221;,&amp;nbsp;etc.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ReviewStatus&lt;/code&gt;: As stated in the &lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/docs/review_status/"&gt;ClinVar review status documentation&lt;/a&gt;: &amp;#8220;The level of review supporting the assertion of clinical significance for the&amp;nbsp;variation.&amp;#8221;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Comment&lt;/code&gt;: Any additional (free-text) comments the organization that was responsible for the submission provided regarding the interpretation of any given variant in the context of any given&amp;nbsp;phenotype.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/intro/"&gt;ClinVar | Documentation | What is&amp;nbsp;ClinVar?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/submitters/"&gt;ClinVar | Documentation |&amp;nbsp;Submitters&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/"&gt;ClinVar | Search&amp;nbsp;Tool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://varnomen.hgvs.org/"&gt;Sequence Variant&amp;nbsp;Nomenclature&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/books/NBK179288/"&gt;Entrez Direct: E-utilities on the Unix Command Line - Entrez Programming Utilities Help - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;nbsp;Bookshelf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-2.html"&gt;&lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;2)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/docs/maintenance_use/"&gt;Accessing and using data in&amp;nbsp;ClinVar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/"&gt;ClinVar | &lt;span class="caps"&gt;FTP&lt;/span&gt; server | Index of&amp;nbsp;/pub/clinvar/xml&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/xml.etree.elementtree.html"&gt;xml.etree.ElementTree â€” The ElementTree &lt;span class="caps"&gt;XML&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pandas.pydata.org/"&gt;pandas - Python Data Analysis&amp;nbsp;Library&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pandas.pydata.org/docs/whatsnew/v1.5.0.html#read-xml-now-supports-large-xml-using-iterparse"&gt;pandas | Documentation | Whatâ€™s new in 1.5.0 (September 19,&amp;nbsp;2022)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://aws.amazon.com/ec2/?nc1=h_ls"&gt;Secure and resizable cloud compute â€“ Amazon &lt;span class="caps"&gt;EC2&lt;/span&gt; â€“ Amazon Web&amp;nbsp;Services&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;Setting Up Your Unix Computer for Bioinformatics&amp;nbsp;Analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/docs/identifiers/"&gt;ClinVar | Documentation | Identifiers in&amp;nbsp;ClinVar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/docs/review_status/"&gt;ClinVar | Documentation | Review&amp;nbsp;status&lt;/a&gt;&lt;/p&gt;</content><category term="Python"></category><category term="pandas"></category><category term="ClinVar"></category><category term="genomics"></category><category term="variants"></category></entry><entry><title>Opening files of size larger than RAM withÂ pandas</title><link href="https://antoniocampos13.github.io/opening-files-of-size-larger-than-ram-with-pandas.html" rel="alternate"></link><published>2022-06-27T10:00:00-03:00</published><updated>2022-06-27T10:00:00-03:00</updated><author><name>Antonio Victor Campos Coelho</name></author><id>tag:antoniocampos13.github.io,2022-06-27:/opening-files-of-size-larger-than-ram-with-pandas.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Dealing with big files is a routine for everyone working in genomics. &lt;span class="caps"&gt;FASTQ&lt;/span&gt;, &lt;span class="caps"&gt;VCF&lt;/span&gt;, &lt;span class="caps"&gt;BAM&lt;/span&gt;, and &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; files, to name a few, can range from some hundreds of megabytes to several gigabytes in size. Usually, we can use cloud services to configure computing instances with a lot of â€¦&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Dealing with big files is a routine for everyone working in genomics. &lt;span class="caps"&gt;FASTQ&lt;/span&gt;, &lt;span class="caps"&gt;VCF&lt;/span&gt;, &lt;span class="caps"&gt;BAM&lt;/span&gt;, and &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; files, to name a few, can range from some hundreds of megabytes to several gigabytes in size. Usually, we can use cloud services to configure computing instances with a lot of &lt;span class="caps"&gt;RAM&lt;/span&gt;, but we may use some ways to read and manipulate large-than-&lt;span class="caps"&gt;RAM&lt;/span&gt; files in our personal/work&amp;nbsp;machines.&lt;/p&gt;
&lt;p&gt;This post will demonstrate how to work with big tabular data using the &lt;code&gt;chunksize&lt;/code&gt; option with &lt;code&gt;pandas&lt;/code&gt;. You can find the code in my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2022_06_27_Opening_files_of_size_larger_than_RAM_with_pandas"&gt;portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Chunking: divide and&amp;nbsp;conquer&lt;/h3&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Chunking&amp;#8221; means splitting the big file into chunks (partitions) so the Python session can work with each part separately, meaning it would not need to hold the big data in memory all at once. Keep in mind that not every problem can be solved by chunking. Therefore, if your goal does &lt;span class="caps"&gt;NOT&lt;/span&gt; involve coordination between chunks, such as some filtering and little edition, chunking could help. However, if your task is more complicated than this, other modules such as &lt;a href="https://www.dask.org/"&gt;&lt;code&gt;dask&lt;/code&gt;&lt;/a&gt; are the better option. The panda&amp;#8217;s documentation has an excellent &lt;a href="https://pandas.pydata.org/docs/user_guide/scale.html#"&gt;chapter&lt;/a&gt; explaining ways to go when scaling to large&amp;nbsp;datasets.&lt;/p&gt;
&lt;h2&gt;The input: the human reference genome &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt;&amp;nbsp;file&lt;/h2&gt;
&lt;p&gt;I downloaded the human reference genome &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; file &lt;code&gt;GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gff.gz&lt;/code&gt; file at the &lt;a href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/"&gt;&lt;span class="caps"&gt;NCBI&lt;/span&gt; &lt;span class="caps"&gt;FTP&lt;/span&gt; server&lt;/a&gt; containing files preformatted for use in Bioinformatic analysis pipelines. Next, I extracted the contents of the&amp;nbsp;file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;gunzip GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gff.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The extracted file has a size of about 1 &lt;span class="caps"&gt;GB&lt;/span&gt;. Decently big for my demonstrational&amp;nbsp;purposes.&lt;/p&gt;
&lt;p&gt;But what is a &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; file? The Gene Transfer Format or General Feature Format is a tab-delimited text file format. Bioinformaticians use it to describe genomic features such as genes, exons, introns, putative protein-coding sequences (&lt;span class="caps"&gt;CDS&lt;/span&gt;), transcription factor binding sites, etc. The first two versions (&lt;span class="caps"&gt;GTF&lt;/span&gt; and &lt;span class="caps"&gt;GFF2&lt;/span&gt;) had deficiencies, and &lt;span class="caps"&gt;GFF3&lt;/span&gt; was developed to address them. You can read more about &lt;span class="caps"&gt;GFF3&lt;/span&gt; on &lt;a href="https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md"&gt;Lincoln Stein&amp;#8217;s GitHub page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Every &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; file has nine fields (columns). A dot &lt;code&gt;.&lt;/code&gt; represents missing or null data. The nine columns&amp;nbsp;are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;seqid&lt;/code&gt;: the name of the sequence where the feature is located. For example, a chromosome or&amp;nbsp;contig;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;source&lt;/code&gt;: the program or organization, laboratory, etc. that generated the information regarding the&amp;nbsp;feature;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;type&lt;/code&gt;: qualifiers like &amp;#8220;gene&amp;#8221;, &amp;#8220;exon&amp;#8221;, &amp;#8220;&lt;span class="caps"&gt;CDS&lt;/span&gt;&amp;#8221;. Features can have children: for example, the exons of a gene refer to its gene (their parent). Ideally, all the children features must follow their parents after the parents&amp;#8217; initial definition in the&amp;nbsp;file.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;start&lt;/code&gt;: the base position in the sequence where the feature starts. It has a 1-base offset, in contrast to the &lt;span class="caps"&gt;BED&lt;/span&gt; format, which is&amp;nbsp;0-offset.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;end&lt;/code&gt;: the base position in the sequence where the feature&amp;nbsp;ends.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;score&lt;/code&gt;: numeric value representing the quality of the&amp;nbsp;sequence.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;strand&lt;/code&gt;: indicates the strand of the feature: &lt;code&gt;+&lt;/code&gt; (the sense strand is the default 5&amp;#8217;-3&amp;#8217; representation of the feature), &lt;code&gt;-&lt;/code&gt; (the sense strand is the reverse complement strand of the sequence representation), or &lt;code&gt;.&lt;/code&gt; (undetermined).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;phase&lt;/code&gt;: used to indicate the reading frame of the features that are &lt;span class="caps"&gt;CDS&lt;/span&gt;. Can be &lt;code&gt;0&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;2&lt;/code&gt; or &lt;code&gt;.&lt;/code&gt; (undetermined).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;attributes&lt;/code&gt;: all other information relevant for describind the&amp;nbsp;feature.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I will open this file using &lt;code&gt;pandas&lt;/code&gt; and keep just the exons of all annotated human&amp;nbsp;genes.&lt;/p&gt;
&lt;h2&gt;Chunking with &lt;code&gt;pandas&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;I create a python script and import the &lt;code&gt;pandas&lt;/code&gt; module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I define some variables to store the path of the &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; file, the name of the output table, and the chunk&amp;nbsp;size:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;TABLE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gff&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;EDITED_TABLE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;human_gene_exons_GRCh38.gff&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;CHUNKSIZE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20000000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The chunk size must be an integer because it represents the number of lines each chunk will have. In the example above, I will tell &lt;code&gt;pandas&lt;/code&gt; to partition the file into parts containing two million lines until it finishes processing the whole&amp;nbsp;dataset.&lt;/p&gt;
&lt;p&gt;Next, I define a list with the column&amp;nbsp;names:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;column_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;seqid&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;source&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;start&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;end&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;score&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;strand&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;phase&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;attributes&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Since the file is tab-delimited, I can use the &lt;code&gt;pd.read_table()&lt;/code&gt; function, passing the &lt;code&gt;column_names&lt;/code&gt; list as the value for the &lt;code&gt;names&lt;/code&gt; argument and the &lt;code&gt;chunksize&lt;/code&gt; as&amp;nbsp;well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;chunks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TABLE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;column_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;chunksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;CHUNKSIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;comment&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;#&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Or:&lt;/span&gt;
&lt;span class="n"&gt;chunks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TABLE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;column_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;chunksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;CHUNKSIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;comment&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;#&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If I had not decompressed the file, I could also use the argument &lt;code&gt;compression="infer"&lt;/code&gt; so &lt;code&gt;pandas&lt;/code&gt; would decompress it&amp;nbsp;on-the-fly.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;chunksize&lt;/code&gt; argument makes the &lt;code&gt;pd.read_table()&lt;/code&gt; return an &lt;strong&gt;iterator&lt;/strong&gt; object (&lt;code&gt;TextFileReader&lt;/code&gt;). What is an iterator? In Python, an iterator is an object we can traverse through all its values. Python lists, dictionaries, and tuples are all Pythonic iterators. In our case, each element of this iterator is a &lt;code&gt;pd.Dataframe&lt;/code&gt; instead.&lt;/p&gt;
&lt;p&gt;Therefore, the &lt;code&gt;for&lt;/code&gt; loop I wrote will perform the same action on every &lt;code&gt;pd.DataFrame&lt;/code&gt; in the&amp;nbsp;iterator.&lt;/p&gt;
&lt;h2&gt;Filtering each&amp;nbsp;chunk&lt;/h2&gt;
&lt;p&gt;These are the steps I will perform with each&amp;nbsp;chunk:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Filter for rows with &amp;#8220;exon&amp;#8221; values in the &lt;code&gt;type&lt;/code&gt; column;&lt;/li&gt;
&lt;li&gt;Drop (remove) all columns except for &lt;code&gt;seqid&lt;/code&gt;, &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;start&lt;/code&gt;, &lt;code&gt;end&lt;/code&gt; and &lt;code&gt;attributes&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Save the edited chunk directly to disk by appending the chunk to a tab-delimited&amp;nbsp;file.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Below is the loop&amp;nbsp;code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;chunk&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;chunks&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# Step 1&lt;/span&gt;
    &lt;span class="n"&gt;temp_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;chunk&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;chunk&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;exon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# Step 2&lt;/span&gt;
    &lt;span class="n"&gt;temp_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;source&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;score&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;strand&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;phase&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="c1"&gt;# Step 3&lt;/span&gt;
    &lt;span class="n"&gt;temp_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EDITED_TABLE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I should explain step 3 in more detail. Since the &lt;code&gt;for&lt;/code&gt; loop will dump each part immediately to disk after finishing my edits, Python&amp;#8217;s &lt;span class="caps"&gt;RAM&lt;/span&gt; usage will be more or less constant during the file processing. You may see other tutorials appending the data frames to a list and concatenating them into a final &lt;code&gt;pd.DataFrame&lt;/code&gt;, but in my opinion, this kind of defeats the purpose of chunking since Python will have to hold everything in memory, risking &lt;span class="caps"&gt;RAM&lt;/span&gt; overuse and killing the&amp;nbsp;process.&lt;/p&gt;
&lt;p&gt;Let me explain the &lt;code&gt;pd.to_csv()&lt;/code&gt; arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;EDITED_TABLE&lt;/code&gt;: the file output&amp;nbsp;name/path;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;header=False&lt;/code&gt;: do not output the column names to&amp;nbsp;file;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mode="a"&lt;/code&gt;: append each chunk on the output&amp;nbsp;file;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sep="\t"&lt;/code&gt;: write tab-delimited columns on the output&amp;nbsp;file;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;index=False&lt;/code&gt;: do not output index column. Since I did not set the index, it would print the row numbers, which would be undesirable (it would violate &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; format&amp;nbsp;specifications).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Observe that I defined the column names to make filtering/editing easier. The &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; format specifications do not require the header names to be present in the file. Therefore, I removed them during Step&amp;nbsp;3.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I demonstrated one way of dealing with big files by chunking with &lt;code&gt;pandas&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;p&gt;I already have written about &lt;code&gt;dask&lt;/code&gt; on my machine learning tutorials. See part 1 &lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-1-making-features-datasets.html"&gt;here&lt;/a&gt; and part 2 &lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-2-making-a-model.html"&gt;here&lt;/a&gt;. Check &lt;a href="https://pandas.pydata.org/docs/index.html"&gt;&lt;code&gt;pandas&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt; documentation&lt;/a&gt; and &lt;a href="https://docs.dask.org/en/stable/"&gt;dask&amp;#8217;s&lt;/a&gt; as well for more&amp;nbsp;information.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.dask.org/"&gt;Dask | Scale the Python tools you&amp;nbsp;love&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pandas.pydata.org/docs/user_guide/scale.html#"&gt;Scaling to large datasets &amp;#8212; pandas 1.4.3&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/"&gt;Index of /genomes/all/&lt;span class="caps"&gt;GCA&lt;/span&gt;/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md"&gt;Specifications/gff3.md at master Â·&amp;nbsp;The-Sequence-Ontology/Specifications&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-1-making-features-datasets.html"&gt;Machine Learning with Python: Supervised Classification of &lt;span class="caps"&gt;TCGA&lt;/span&gt; Prostate Cancer Data (Part 1 - Making Features&amp;nbsp;Datasets)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-2-making-a-model.html"&gt;Machine Learning with Python: Supervised Classification of &lt;span class="caps"&gt;TCGA&lt;/span&gt; Prostate Cancer Data (Part 2 - Making a&amp;nbsp;Model)&lt;/a&gt;&lt;/p&gt;</content><category term="Python"></category><category term="pandas"></category><category term="Genomics"></category><category term="Bioinformatics"></category></entry><entry><title>Genomic Analysis WithÂ Hail</title><link href="https://antoniocampos13.github.io/genomic-analysis-with-hail.html" rel="alternate"></link><published>2021-07-09T16:45:00-03:00</published><updated>2021-07-09T16:45:00-03:00</updated><author><name>Antonio Victor Campos Coelho</name></author><id>tag:antoniocampos13.github.io,2021-07-09:/genomic-analysis-with-hail.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hello, long time no see! Since I lasted posted, many things happened. Since March I have been working as Post-Doc Researcher, hired by the &lt;a href="https://www.einstein.br/Pages/Home.aspx"&gt;Hospital Israelita Albert Einstein (&lt;span class="caps"&gt;HIAE&lt;/span&gt;, SÃ£o Paulo, Brazil)&lt;/a&gt; to work for the Projeto Genomas Raros (&amp;#8220;Rare Genomes Project&amp;#8221;, &lt;span class="caps"&gt;GRAR&lt;/span&gt; from here on), a public-private partnership â€¦&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hello, long time no see! Since I lasted posted, many things happened. Since March I have been working as Post-Doc Researcher, hired by the &lt;a href="https://www.einstein.br/Pages/Home.aspx"&gt;Hospital Israelita Albert Einstein (&lt;span class="caps"&gt;HIAE&lt;/span&gt;, SÃ£o Paulo, Brazil)&lt;/a&gt; to work for the Projeto Genomas Raros (&amp;#8220;Rare Genomes Project&amp;#8221;, &lt;span class="caps"&gt;GRAR&lt;/span&gt; from here on), a public-private partnership between &lt;span class="caps"&gt;HIAE&lt;/span&gt; and the Brazilian Health Ministry to further the implementation of genomic analysis into the Brazilian public healthcare system (&lt;span class="caps"&gt;SUS&lt;/span&gt;), with the intention of improve diagnostic rates of rare diseases in Brazil. Since 2020, thousands of genomes of Brazilian patients with suspected rare diseases have been sequenced, and many more will come in the next two&amp;nbsp;years.&lt;/p&gt;
&lt;p&gt;Thus, I have been tasked to develop/adapt analysis pipelines to handle whole-genome data at large scales compatible with the scope of &lt;span class="caps"&gt;GRAR&lt;/span&gt;. My team asked me to explore &lt;a href="https://hail.is/"&gt;Hail, a Python/Spark framework for scalable genomic analysis&lt;/a&gt;. It has been developed at &lt;a href="https://www.broadinstitute.org/"&gt;Broad Institute&lt;/a&gt; and was used to generate the &lt;a href="https://gnomAD.broadinstitute.org/"&gt;Genome Aggregation Database (gnomAD)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post I will share some use cases of this tool full of potential. Please notice that it is not intended to substitute the official documentation of Hail, &lt;span class="caps"&gt;GATK&lt;/span&gt; and other software used here. Just consider it as a demonstration of Hail use cases with commentaries. Also, notice two important things: first, Hail implements &amp;#8220;lazy evaluation&amp;#8221;; new users may think a specific command run blazingly fast, but in reality, Hail just mapped the execution order of functions needed and only will compute anything when necessary, such as saving results to disk and printing the first few rows of a dataset to Python&amp;#8217;s standard output stream. Second, Hail need a lot of &lt;span class="caps"&gt;RAM&lt;/span&gt; and &lt;span class="caps"&gt;CPU&lt;/span&gt; to work properly with big datasets. Hail is intended to be used in cloud/cluster computing environments, but a computer with relatively good hardware configuration can run small&amp;nbsp;datasets.&lt;/p&gt;
&lt;h2&gt;Installing&amp;nbsp;software&lt;/h2&gt;
&lt;p&gt;Prepare your Unix computing environment by installing &lt;a href="https://hail.is/#install"&gt;Hail&lt;/a&gt;, &lt;a href="https://pypi.org/project/gnomAD/"&gt;gnomAD utilities for Hail&lt;/a&gt; and &lt;a href="https://gatk.broadinstitute.org/hc/en-us"&gt;Genome Analysis Toolkit version 4 (&lt;span class="caps"&gt;GATK4&lt;/span&gt;)&lt;/a&gt;. Tools for manipulating &lt;span class="caps"&gt;VCF&lt;/span&gt; files are essential too, such as &lt;a href="http://samtools.github.io/bcftools/bcftools.html"&gt;bcftools&lt;/a&gt;. I advise installing the tools into a virtual environment for convenience (see &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;my previous post&lt;/a&gt; for some pointers on how to use conda&amp;nbsp;environments).&lt;/p&gt;
&lt;h2&gt;Preparing the multi-sample &lt;span class="caps"&gt;VCF&lt;/span&gt;&amp;nbsp;input&lt;/h2&gt;
&lt;p&gt;Since the &lt;span class="caps"&gt;GRAR&lt;/span&gt; data is coming from whole-genome sequencing (&lt;span class="caps"&gt;WGS&lt;/span&gt;), we have been generating one &lt;a href="https://gatk.broadinstitute.org/hc/en-us/articles/360035531812-GVCF-Genomic-Variant-Call-Format"&gt;genomic variant call format (gVCF) file&lt;/a&gt; per participant through &lt;a href="https://www.illumina.com/products/by-type/informatics-products/dragen-bio-it-platform.html"&gt;Illumina&amp;#8217;s &lt;span class="caps"&gt;DRAGEN&lt;/span&gt; sequencing read analysis platform&lt;/a&gt;. A gVCF file has all the characteristics of a &lt;a href="https://samtools.github.io/hts-specs/VCFv4.2.pdf"&gt;&lt;span class="caps"&gt;VCF&lt;/span&gt; file&lt;/a&gt;, the difference being that the gVCF files have information for all sites in the genome, which give more precision during the integration of variant calls coming from several&amp;nbsp;samples.&lt;/p&gt;
&lt;p&gt;By integration I mean &lt;strong&gt;combining&lt;/strong&gt; several gVCF files into a single multi-sample &lt;span class="caps"&gt;VCF&lt;/span&gt; file so we can perform analysis (calculate allelic frequencies, assess genotyping quality, and so on) with the &lt;strong&gt;whole cohort&lt;/strong&gt;. We are currently using &lt;span class="caps"&gt;GATK&lt;/span&gt;&amp;#8217;s GenomicsDBImport tool. See &lt;a href="https://gatk.broadinstitute.org/hc/en-us/articles/360036883491-GenomicsDBImport"&gt;here&lt;/a&gt; for a tutorial of how to use it. Briefly, we intend to create a GenomicsDB object and we will update it regularly by appending new gVCFs as they become available, until the last participant is recruited and we have their genome sequenced. When this moment comes, we will extract a multi-sample &lt;span class="caps"&gt;VCF&lt;/span&gt; with &lt;span class="caps"&gt;GATK&lt;/span&gt;&amp;#8217;s &lt;code&gt;GenotypeGVCFs&lt;/code&gt; tool:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;gatk GenotypeGVCFs -R &lt;span class="nv"&gt;$REF&lt;/span&gt; -V &lt;span class="nv"&gt;$DBPATH&lt;/span&gt; -G StandardAnnotation -O cohort.vcf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Where &lt;code&gt;$REF&lt;/code&gt; and &lt;code&gt;$DBPATH&lt;/code&gt; are the paths of the genome reference (the same used during the variant call process) and the GenomicsDB, respectively (which should have a &lt;code&gt;gendb://&lt;/code&gt; prefix as noted in the GenomicsDBImport tutorial, something like &lt;code&gt;gendb://my_database&lt;/code&gt;). The &lt;code&gt;-O&lt;/code&gt; flag indicates the output name. If you wish to compress the &lt;span class="caps"&gt;VCF&lt;/span&gt; file, you may use &lt;code&gt;bcftools&lt;/code&gt; or &lt;code&gt;bgzip&lt;/code&gt;. Remember to index the compressed&amp;nbsp;file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;bcftools view cohort.vcf -Oz -o cohort.vcf.gz
bcftools index cohort.vcf.gz

&lt;span class="c1"&gt;# or&lt;/span&gt;
bgzip -@ &lt;span class="m"&gt;4&lt;/span&gt; cohort.vcf
tabix cohort.vcf.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The next step is to perform &lt;span class="caps"&gt;GATK&lt;/span&gt;&amp;#8217;s Variant Quality Score Recalibration (&lt;span class="caps"&gt;VQSR&lt;/span&gt;) in the output &lt;span class="caps"&gt;VCF&lt;/span&gt; with &lt;span class="caps"&gt;GATK&lt;/span&gt;&amp;#8217;s &lt;code&gt;VariantRecalibrator&lt;/code&gt;.  See their original tutorial &lt;a href="https://gatk.broadinstitute.org/hc/en-us/articles/360035531112--How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering"&gt;here&lt;/a&gt;. To put it simply, &lt;span class="caps"&gt;VQSR&lt;/span&gt; works by comparing the detected variants with high-confidence variant sites observed by several consortia (HapMap, 1000 Genomes etc.) and applies a filter deeming the variant a true positive (i.e. the observed variation is a true biological event) or a false positive (i.e. the observed variation is not real, it is in a fact sequencing artifact). To this end, I downloaded high-confidence datasets to perform the &lt;span class="caps"&gt;VQSR&lt;/span&gt;. The datasets can be found at the &lt;a href="https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0;tab=objects?prefix=&amp;amp;forceOnObjectsSortingFiltering=false"&gt;&lt;span class="caps"&gt;GATK&lt;/span&gt; Google Cloud Storage (Resource Bundle)&lt;/a&gt; and can be downloaded with &lt;a href="https://cloud.google.com/storage/docs/gsutil"&gt;&lt;code&gt;gsutil&lt;/code&gt; application&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I will post a script with slight modification in the steps of the &lt;span class="caps"&gt;VQSR&lt;/span&gt; tutorial in my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2021_07_07_Genomic_Analysis_With_Hail"&gt;portfolio&lt;/a&gt; (I was having errors so I noticed that I had to put spaces after the &lt;code&gt;-resource&lt;/code&gt; flags of the &lt;code&gt;VariantRecalibrator&lt;/code&gt; command). Briefly, the steps&amp;nbsp;are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Filtering samples with excess of heterozygotes (recommended when working with thousands of&amp;nbsp;samples);&lt;/li&gt;
&lt;li&gt;Make a sites-only &lt;span class="caps"&gt;VCF&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;Recalibration step (separately by indels/mixed and &lt;span class="caps"&gt;SNP&lt;/span&gt;&amp;nbsp;loci);&lt;/li&gt;
&lt;li&gt;Apply the recalibration&amp;nbsp;filters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The output of the &lt;span class="caps"&gt;VQSR&lt;/span&gt; is the &lt;code&gt;snp.recalibrated.vcf.gz&lt;/code&gt; file (despite the name, the indels/mixed variants in the file have been recalibrated as well). We can now import the dataset into&amp;nbsp;Hail.&lt;/p&gt;
&lt;h2&gt;Initiating&amp;nbsp;Hail&lt;/h2&gt;
&lt;p&gt;Hail&amp;#8217;s frontend is written in Python. Simply importing the Hail module is not sufficient. We must initiate it so it can communicate with Spark. I created the &lt;code&gt;hail_demo_init.py&lt;/code&gt; file as an &amp;#8220;init template&amp;#8221; that can be reused between Hail&amp;nbsp;scripts/sessions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# hail_demo_init.py&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;hail&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;hl&lt;/span&gt;

&lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;GRCh38&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;idempotent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;quiet&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;skip_logging_configuration&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;default_reference&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;spark_conf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;spark.executor.cores&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;4&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;spark.driver.memory&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;16g&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Notice the &lt;code&gt;DEFAULT_REF&lt;/code&gt; variable: it establishes that I intend to use the genomic coordinates considering the Genome Reference Consortium Human Reference 38 (GRCh38), the most recent genome version being accepted by the human genome community. Otherwise Hail would use GRCh37 as default. The &lt;code&gt;spark_conf&lt;/code&gt; argument where I setup Spark so it uses four &lt;span class="caps"&gt;CPU&lt;/span&gt; cores and 16 &lt;span class="caps"&gt;GB&lt;/span&gt; of &lt;span class="caps"&gt;RAM&lt;/span&gt;. Change these values as&amp;nbsp;appropriate.&lt;/p&gt;
&lt;h2&gt;Importing the &lt;span class="caps"&gt;VCF&lt;/span&gt;&amp;nbsp;input&lt;/h2&gt;
&lt;p&gt;I am now ready to import &lt;code&gt;snp.recalibrated.vcf.gz&lt;/code&gt; into the Hail session and convert it to Hail&amp;#8217;s &lt;code&gt;MatrixTable&lt;/code&gt; object and write it to the disk. In simple terms, a &lt;code&gt;MatrixTable&lt;/code&gt; is a representation of a &lt;span class="caps"&gt;VCF&lt;/span&gt; file that is amenable to be manipulated by Spark. Read the &lt;a href="https://hail.is/docs/0.2/index.html"&gt;Hail Docs&lt;/a&gt; for more details. I initiate Hail and then import and convert the dataset with two chained steps (&lt;code&gt;hl.import_vcf().write()&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# hail_demo_import_vcf.py&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;hail&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;hl&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;hail_demo_init&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt;

&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;import_vcf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;snp.recalibrated.vcf.gz&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;force_bgz&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;reference_genome&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;array_elements_required&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;recalibrated.mt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;overwrite&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I can now proceed to prepare the dataset for sample and variant quality control (&lt;span class="caps"&gt;QC&lt;/span&gt;).&lt;/p&gt;
&lt;h2&gt;Preparing for Sample &lt;span class="caps"&gt;QC&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Before reading the created matrix table I create some &amp;#8220;magic numbers&amp;#8221; variables to hold some quality metrics thresholds for sample &lt;span class="caps"&gt;QC&lt;/span&gt;&amp;nbsp;later:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# hail_demo_sample_qc.py&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;hail&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;hl&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;hail_demo_init&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;gnomAD.utils.filtering&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;filter_to_autosomes&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;gnomAD.utils.annotations&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;add_variant_type&lt;/span&gt;

&lt;span class="c1"&gt;# Magic numbers&lt;/span&gt;
&lt;span class="n"&gt;CALL_RATE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.90&lt;/span&gt;  &lt;span class="c1"&gt;# Hail team value = 0.97. gnomAD value = 0.99&lt;/span&gt;
&lt;span class="n"&gt;RELATEDNESS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.088&lt;/span&gt;  &lt;span class="c1"&gt;# Hail team value. gnomAD value = 0.08838835&lt;/span&gt;
&lt;span class="n"&gt;READ_DEPTH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;  &lt;span class="c1"&gt;# Hail team value.&lt;/span&gt;
&lt;span class="n"&gt;FREEMIX_CONTAMINATION&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;  &lt;span class="c1"&gt;# gnomAD value.&lt;/span&gt;
&lt;span class="n"&gt;CHIMERIC_READS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;  &lt;span class="c1"&gt;# gnomAD value.&lt;/span&gt;
&lt;span class="n"&gt;MEDIAN_LENGTH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;250&lt;/span&gt;  &lt;span class="c1"&gt;# gnomAD value.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The comment in each line contains the value used by Hail team or gnomAD team. Since this is a demonstration, I may have used different values. Change these values as you feel appropriate as&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;I will now read the matrix table from disk and assign it to the &lt;code&gt;mt&lt;/code&gt; object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_matrix_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;recalibrated.mt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To check the first few sample ids, I use the &lt;code&gt;show()&lt;/code&gt; method (I explain the &lt;code&gt;s&lt;/code&gt; here&amp;nbsp;later):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To check the matrix table structure, I use the &lt;code&gt;describe()&lt;/code&gt; method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;describe&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output is as&amp;nbsp;follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;----------------------------------------
Global fields:
    None
----------------------------------------
Column fields:
    &amp;#39;s&amp;#39;: str
----------------------------------------
Row fields:
    &amp;#39;locus&amp;#39;: locus&amp;lt;GRCh38&amp;gt;
    &amp;#39;alleles&amp;#39;: array&amp;lt;str&amp;gt;
    &amp;#39;rsid&amp;#39;: str
    &amp;#39;qual&amp;#39;: float64
    &amp;#39;filters&amp;#39;: set&amp;lt;str&amp;gt;
    &amp;#39;info&amp;#39;: struct {
        AC: array&amp;lt;int32&amp;gt;, 
        AF: array&amp;lt;float64&amp;gt;, 
        AN: int32, 
        BaseQRankSum: float64, 
        DB: bool, 
        DP: int32, 
        END: int32, 
        ExcessHet: float64, 
        FS: float64, 
        FractionInformativeReads: float64, 
        InbreedingCoeff: float64, 
        LOD: float64, 
        MLEAC: array&amp;lt;int32&amp;gt;, 
        MLEAF: array&amp;lt;float64&amp;gt;, 
        MQ: float64, 
        MQRankSum: float64, 
        NEGATIVE_TRAIN_SITE: bool, 
        POSITIVE_TRAIN_SITE: bool, 
        QD: float64, 
        R2_5P_bias: float64, 
        ReadPosRankSum: float64, 
        SOR: float64, 
        VQSLOD: float64, 
        culprit: str
    }
----------------------------------------
Entry fields:
    &amp;#39;AD&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;AF&amp;#39;: array&amp;lt;float64&amp;gt;
    &amp;#39;DP&amp;#39;: int32
    &amp;#39;F1R2&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;F2R1&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;GP&amp;#39;: array&amp;lt;float64&amp;gt;
    &amp;#39;GQ&amp;#39;: int32
    &amp;#39;GT&amp;#39;: call
    &amp;#39;ICNT&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;MB&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;MIN_DP&amp;#39;: int32
    &amp;#39;PL&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;PRI&amp;#39;: array&amp;lt;float64&amp;gt;
    &amp;#39;PS&amp;#39;: int32
    &amp;#39;RGQ&amp;#39;: int32
    &amp;#39;SB&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;SPL&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;SQ&amp;#39;: float64
----------------------------------------
Column key: [&amp;#39;s&amp;#39;]
Row key: [&amp;#39;locus&amp;#39;, &amp;#39;alleles&amp;#39;]
----------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can see that a Hail &lt;code&gt;MatrixTable&lt;/code&gt; objects has four types of information&amp;nbsp;&amp;#8220;compartments&amp;#8221;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Global fields: information values that are identical for every&amp;nbsp;row&lt;/li&gt;
&lt;li&gt;Column fields: sample-level information (id, phenotype, sex&amp;nbsp;etc.)&lt;/li&gt;
&lt;li&gt;Row fields: variant-level information (locus, alleles, type of variant, allele frequency&amp;nbsp;etc.)&lt;/li&gt;
&lt;li&gt;Entry fields: variant-by-sample-level (genotype, genotype quality,&amp;nbsp;etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also notice that the &lt;code&gt;MatrixTable&lt;/code&gt; is keyed by column (&lt;code&gt;s&lt;/code&gt;: &lt;strong&gt;s&lt;/strong&gt;ample id field) and rows (locus and allele row fields) allowing us to perform &lt;span class="caps"&gt;SQL&lt;/span&gt;-style table joins. For each field type you can see the name of each field. The info field contains the &lt;span class="caps"&gt;INFO&lt;/span&gt; field from the input &lt;span class="caps"&gt;VCF&lt;/span&gt;. The entries fields contain the values listed into the &lt;span class="caps"&gt;FORMAT&lt;/span&gt; &lt;span class="caps"&gt;VCF&lt;/span&gt; field. For more details check the &lt;a href="https://hail.is/docs/0.2/tutorials/07-matrixtable.html"&gt;&lt;code&gt;MatrixTable&lt;/code&gt; Hail Docs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now I will modify an entry field and create other row fields that I will need to use later during variant &lt;span class="caps"&gt;QC&lt;/span&gt;. The commands to create/modify fields is &lt;code&gt;hl.annotate_cols()&lt;/code&gt;, &lt;code&gt;hl.annotate_rows()&lt;/code&gt; or &lt;code&gt;hl.annotate_entries()&lt;/code&gt; depending on the field type (columns, rows, entries, respectively). Notice that in one of them I used a gnomAD&amp;nbsp;function.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Mixture of non-empty with empty PL fields causes problems with sample QC for some reason; setting field to all empty&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_entries&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;missing&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PL&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Add variant-level annotations necessary for variant QC later&lt;/span&gt;
&lt;span class="c1"&gt;## Annotate variants in one of the categories: SNV, multi-SNV, indel, multi-indel, mixed&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;add_variant_type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alleles&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# gnomAD function&lt;/span&gt;

&lt;span class="c1"&gt;## Number of alleles at the site&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_alleles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alleles&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;## Mixed sites (SNVs and indels present at the site)&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mixed_site&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;if_else&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variant_type&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;mixed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;## Spanning deletions&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;spanning_deletion&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alleles&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To check the dimensions of the &lt;code&gt;MatrixTable&lt;/code&gt;, I can use the following&amp;nbsp;commands:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Number of Rows, Columns&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# Number of Columns&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count_cols&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To see a variant breakdown (types, number of variants per chromosome and several other information), there is the following&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summarize_variants&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;A recommended step for further downstream analyses is to split multiallelic variants into biallelic&amp;nbsp;configuration:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split_multi_hts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To remove any monomorphic (invariant) loci I&amp;nbsp;use:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_alleles&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Try the &lt;code&gt;hl.summarize_variants()&lt;/code&gt; to check how the numbers changed after&amp;nbsp;splitting.&lt;/p&gt;
&lt;p&gt;Up until this moment, I have only genetics-related information into the &lt;code&gt;MatrixTable&lt;/code&gt;. I can instruct Hail to import &lt;span class="caps"&gt;TSV&lt;/span&gt;/&lt;span class="caps"&gt;CSV&lt;/span&gt; file with sample-level (column) annotations with the &lt;code&gt;hl.import_table()&lt;/code&gt; command and then associate each observation by keying the &lt;code&gt;s&lt;/code&gt; field with &lt;code&gt;hl.annotate_cols()&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;sa&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;import_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sample_info.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;impute&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;s&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_cols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_info&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sa&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;sample_info.txt&lt;/code&gt; example file has the following&amp;nbsp;format:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;s sex phenotype mean_coverage chimeric_reads contamination median_length
sample1 XX case 25 0.02 0.00 300
sample2 XY case 22 0.01 0.00 250
sample3 XY control 30 0.03 0.00 265
sample4 XX control 29 0.01 0.00 250
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this mock sample information file I included the sex karyotype, a fictitious phenotype and sequencing metrics to support the sample &lt;span class="caps"&gt;QC&lt;/span&gt; procedure. As you may have guessed, any number and type (numeric, string, float, integer, Boolean) of important sample metadata columns can be&amp;nbsp;included.&lt;/p&gt;
&lt;p&gt;Whenever we annotate columns, rows or entries in Hail, we must provide the name of the new field. In this case is &lt;code&gt;sample_info&lt;/code&gt;. So if I wanted to manipulate, say the sex karyotype field, I would refer the field name this&amp;nbsp;way:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_info&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sex&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is because the information in the file will be assigned to a dictionary-like field named &lt;code&gt;sample_info&lt;/code&gt; within the &lt;code&gt;MatrixTable&lt;/code&gt;; the &lt;code&gt;sex&lt;/code&gt;, &lt;code&gt;phenotype&lt;/code&gt;, etc. fields are &lt;em&gt;inside&lt;/em&gt; &lt;code&gt;sample_info&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Performing Sample &lt;span class="caps"&gt;QC&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;Plotting quality&amp;nbsp;metrics&lt;/h3&gt;
&lt;p&gt;Hail has a very convenient function to calculate descriptive statistics of fields in a &lt;code&gt;MatrixTable&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_qc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This function will calculate overall call rate per sample, mean read depth (coverage) and other &lt;a href="https://hail.is/docs/0.2/methods/genetics.html#hail.methods.sample_qc"&gt;things&lt;/a&gt;. We can plot the calculated fields to assess call rate and coverage (read depth) across samples. The code below will output a &lt;span class="caps"&gt;HTML&lt;/span&gt; plot to disk (make sure you have the &lt;a href="https://docs.bokeh.org/en/latest/index.html"&gt;bokeh&lt;/a&gt; Python module&amp;nbsp;installed).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bokeh.embed&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;file_html&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bokeh.resources&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;CDN&lt;/span&gt;

&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_qc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dp_stats&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_qc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;call_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Mean DP&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Call Rate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;hover_fields&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ID&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;html&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;file_html&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CDN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Chart&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Call Rate by Mean DP.html&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;html&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Filter samples by quality&amp;nbsp;thresholds&lt;/h3&gt;
&lt;p&gt;I can finally filter out samples that do not meet the quality thresholds I established before. For example, the lines below will keep only samples that meet overall call rate and mean coverage quality&amp;nbsp;criteria:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_cols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_qc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;call_rate&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;CALL_RATE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_cols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_qc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dp_stats&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;READ_DEPTH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you have other quality criteria, you could filter in a similar way by correctly referring the field name (remember that dictionary-like fields contain other fields, as is the case of &lt;code&gt;sample_info&lt;/code&gt; I mentioned earlier and &lt;code&gt;sample_qc&lt;/code&gt; above) and a logical expression (which must follow the Python syntax for equalities and&amp;nbsp;inequalities).&lt;/p&gt;
&lt;h3&gt;Principal component analysis (&lt;span class="caps"&gt;PCA&lt;/span&gt;) to filter related&amp;nbsp;samples&lt;/h3&gt;
&lt;p&gt;To ensure that each sample in the cohort is unrelated to any other sample, we can run a principal component analysis (&lt;span class="caps"&gt;PCA&lt;/span&gt;) to evidence samples with kinship too higher according to our chosen threshold. The &lt;span class="caps"&gt;PCA&lt;/span&gt; will only work with autosome (diploid) biallelic variants. The code below will filter our &lt;code&gt;MatrixTable&lt;/code&gt; keeping only variants meeting these&amp;nbsp;conditions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;for_pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;filter_to_autosomes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# gnomAD function. Will keep variants in chromosomes 1 to 22 only.&lt;/span&gt;
&lt;span class="n"&gt;for_pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;for_pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;for_pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_alleles&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# will remove sites that were multi-allelic before splitting as well to ensure &amp;quot;pure&amp;quot; biallelic sites&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, I determine the sample number to calculate &lt;code&gt;k&lt;/code&gt;, the number of principal&amp;nbsp;components.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;sample_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;for_pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cols&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, using the &lt;a href="https://hail.is/docs/0.2/methods/genetics.html#hail.methods.hwe_normalized_pca"&gt;&lt;code&gt;hl.hwe_normalized_pca()&lt;/code&gt; function&lt;/a&gt; I calculate the principal component&amp;nbsp;scores:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hwe_normalized_pca&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;for_pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_num&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;compute_loadings&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With big sample sizes, the code above will restrict &lt;code&gt;k&lt;/code&gt; to the maximum of 10 principal&amp;nbsp;components.&lt;/p&gt;
&lt;p&gt;The scores are one of the inputs of the &lt;a href="https://hail.is/docs/0.2/methods/relatedness.html#hail.methods.pc_relate"&gt;&lt;code&gt;hl.pc_relate()&lt;/code&gt; function&lt;/a&gt; that will estimate the relatedness between samples in a pairwise manner. To speed things up, I will estimate only the kinship statistic (&lt;code&gt;statistics="kin"&lt;/code&gt;, while the default is &lt;code&gt;statistics="all"&lt;/code&gt;. Check the function documentation in the previous link for more&amp;nbsp;details).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;relatedness_ht&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pc_relate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;for_pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;min_individual_maf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;scores_expr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;for_pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;col_key&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;block_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4096&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;min_kinship&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;statistics&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;kin&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;relatedness_ht&lt;/code&gt; object is a Hail &lt;code&gt;Table&lt;/code&gt;. It differs from a &lt;code&gt;MatrixTable&lt;/code&gt; by not having column nor entries fields. We determine related samples by filtering this table to keep only the samples above the kinship threshold and then passing the object to &lt;a href="https://hail.is/docs/0.2/methods/misc.html#hail.methods.maximal_independent_set"&gt;&lt;code&gt;hl.maximal_independent_set()&lt;/code&gt; function&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;pairs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;relatedness_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;relatedness_ht&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;kin&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;RELATEDNESS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;related_samples_to_remove&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximal_independent_set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pairs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pairs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;related_samples_to_remove&lt;/code&gt; object will contain a selection of samples to be removed from the dataset because they come from related individuals in the sample. We perform this filtering with the command below. Notice the use of the keyword &lt;code&gt;keep=False&lt;/code&gt; to &lt;em&gt;negate&lt;/em&gt; the selection (I do &lt;em&gt;not&lt;/em&gt; want to keep related&amp;nbsp;samples).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_cols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_defined&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;related_samples_to_remove&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;col_key&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Use &lt;code&gt;mt.count_cols()&lt;/code&gt; to assess if any sample was&amp;nbsp;removed.&lt;/p&gt;
&lt;h3&gt;Wrapping up the sample &lt;span class="caps"&gt;QC&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Here I finish the sample &lt;span class="caps"&gt;QC&lt;/span&gt; process. I then save the &lt;code&gt;relatedness_ht&lt;/code&gt; and the dataset object &lt;code&gt;mt&lt;/code&gt; to disk with &lt;code&gt;write()&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;relatedness_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relatedness.ht&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;overwrite&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sampleqc_pass.mt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;overwrite&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I can now proceed to variant &lt;span class="caps"&gt;QC&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Variant &lt;span class="caps"&gt;QC&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;I init Hail again if needed, import some more functions from gnomAD, create variables with variant quality thresholds, read the &lt;code&gt;MatrixTable&lt;/code&gt; with the data passing sample &lt;span class="caps"&gt;QC&lt;/span&gt; and calculate common variant statistics (such as allelic frequency) with &lt;a href="https://hail.is/docs/0.2/methods/genetics.html#hail.methods.variant_qc"&gt;&lt;code&gt;hl.variant_qc()&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# hail_demo_variant_qc.py&lt;/span&gt;
&lt;span class="c1"&gt;# Import modules and init Hail&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;hail&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;hl&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;gnomAD.utils.annotations&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;bi_allelic_site_inbreeding_expr&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;gnomAD.variant_qc.random_forest&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;apply_rf_model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;median_impute_features&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;gnomAD.variant_qc.pipeline&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_rf_model&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;hail_init&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt;

&lt;span class="c1"&gt;# Variant Quality hard filters&lt;/span&gt;
&lt;span class="n"&gt;INBR_COEFF&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;
&lt;span class="n"&gt;AB_LOWER_LIM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;
&lt;span class="n"&gt;AB_UPPER_LIM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;AB_LOWER_LIM&lt;/span&gt;

&lt;span class="c1"&gt;# Read MatrixTable with sample QC-passing dataset&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_matrix_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sampleqc_pass.mt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variant_qc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Even though allelic frequency may be already been lift over from the original &lt;span class="caps"&gt;VCF&lt;/span&gt; input, I recommend using &lt;code&gt;hl.variant_qc()&lt;/code&gt; since it calculates potentially useful information besides allelic frequency, such as p-values from the test of Hardy-Weinberg equilibrium. Check the function documentation at Hail to see the complete list of&amp;nbsp;statistics.&lt;/p&gt;
&lt;h3&gt;Filter variants by genotype-related quality&amp;nbsp;thresholds&lt;/h3&gt;
&lt;p&gt;Next, I calculate two variant-level metrics: inbreeding coefficient and the maximum p-value for sampling the observed allele balance under a binomial&amp;nbsp;model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inbr_coeff&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;bi_allelic_site_inbreeding_expr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GT&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;pab_max&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binom_test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AD&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DP&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;two-sided&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I remove variants with excess of heterozygotes by inbreeding coefficient and variants for which no sample had high-quality genotypes by evaluating allele balance (the proportion of sequencing reads that support the&amp;nbsp;variant):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inbr_coeff&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;INBR_COEFF&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Removing variants for which no sample had high quality genotypes with hl.any()&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GQ&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DP&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_entries&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AD&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AD&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="c1"&gt;# AB = allele balance&lt;/span&gt;

&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GT&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_hom_ref&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;AB_LOWER_LIM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GT&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_het&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;AB_LOWER_LIM&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;AB_UPPER_LIM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GT&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_hom_var&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;AB_UPPER_LIM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Variant &lt;span class="caps"&gt;QC&lt;/span&gt; by random forest&amp;nbsp;model&lt;/h3&gt;
&lt;p&gt;The gnomAD team adopted a &lt;a href="https://gnomAD.broadinstitute.org/news/2018-10-gnomAD-v2-1/"&gt;random forest model&lt;/a&gt; to filter out sequencing artifacts. Briefly, they labelled variants passing quality thresholds (such as &lt;span class="caps"&gt;GATK&lt;/span&gt;&amp;#8217;s &lt;span class="caps"&gt;VQSR&lt;/span&gt;) as true positives and variants not passing as false positives. Next, they performed a supervised random forest training with some variant-level features. From now on, I try to replicate their method with the best of my&amp;nbsp;understanding.&lt;/p&gt;
&lt;p&gt;I label the variants as true and false&amp;nbsp;positives:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;if_else&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;if_else&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The logic is that if the &lt;code&gt;filters&lt;/code&gt; field (which is carried over from the GenomicsDB-exported &lt;span class="caps"&gt;VCF&lt;/span&gt;) is empty, it indicates the variant passed the &lt;span class="caps"&gt;VQSR&lt;/span&gt; filter and is a false positive otherwise. Thus, I create two Boolean-type columns indicating it. Next, I create a Hail &lt;code&gt;Table&lt;/code&gt; extracting the needed features and the &lt;code&gt;tp&lt;/code&gt; and &lt;code&gt;fp&lt;/code&gt; fields and assigning it to the &lt;code&gt;rf_ht&lt;/code&gt; object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;rf_ht&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inbr_coeff&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SOR&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ReadPosRankSum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MQRankSum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;QD&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pab_max&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variant_type&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_alleles&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mixed_site&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;spanning_deletion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Remember that most of these features are brought over from the &lt;span class="caps"&gt;VCF&lt;/span&gt; &lt;span class="caps"&gt;INFO&lt;/span&gt; field, while the others were generated with the help of Hail. I also generate a Python list with the features&amp;nbsp;names:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;inbr_coeff&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;SOR&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;ReadPosRankSum&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;MQRankSum&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;QD&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;pab_max&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;variant_type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;n_alleles&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;mixed_site&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;spanning_deletion&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Since random forest models do not tolerate missing data, I use a gnomAD function to impute any missing data with the median of the&amp;nbsp;field:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;rf_ht&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;median_impute_features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rf_ht&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I will reserve all variants located in chromosome 20 to perform model evaluation with the help of the &lt;a href="https://hail.is/docs/0.2/functions/genetics.html#hail.expr.functions.parse_locus_interval"&gt;&lt;code&gt;hl.parse_locus_interval()&lt;/code&gt; function&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;test_intervals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;chr20&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;test_intervals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_locus_interval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reference_genome&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;GRCh38&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;test_intervals&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I now may train the model with the help of gnomAD&amp;#8217;s &lt;code&gt;train_rf_model()&lt;/code&gt; function. Internally, the function will select a balanced dataset of true positives and false positives to train the&amp;nbsp;model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;rf_trained_ht&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rf_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_rf_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;rf_ht&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;rf_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;tp_expr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;rf_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;fp_expr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;rf_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;test_expr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;literal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_intervals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;interval&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;interval&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contains&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rf_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;locus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;rf_trained_ht&lt;/code&gt; output is a Hail &lt;code&gt;Table&lt;/code&gt; with annotations related with the random forest model training. The &lt;code&gt;rf_model&lt;/code&gt; object is the model binary generated by Spark. The inputs include the &lt;code&gt;rf_ht Table&lt;/code&gt;, the &lt;code&gt;features&lt;/code&gt; list, the &lt;code&gt;rf_ht.tp&lt;/code&gt; and &lt;code&gt;rf_ht.fp&lt;/code&gt; Boolean columns and a &lt;code&gt;test_expr&lt;/code&gt; argument receives a expression that will ensure that the loci contained in the interval object &lt;code&gt;test_intervals&lt;/code&gt; will be used for model&amp;nbsp;evaluation.&lt;/p&gt;
&lt;p&gt;After model training, I left join the &lt;code&gt;rf_ht&lt;/code&gt; with the model-annotated &lt;code&gt;rf_trained_ht&lt;/code&gt; into the &lt;code&gt;ht Table&lt;/code&gt;. I use it as the input for the gnomAD&amp;#8217;s &lt;code&gt;apply_rf_model()&lt;/code&gt; function. It will apply the random forest model in all variants in the genome, including those not selected for&amp;nbsp;training.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;ht&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rf_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rf_trained_ht&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;left&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;rf_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;apply_rf_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;ht&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ht&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;rf_model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;rf_model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rf_trained_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rf_label&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;prediction_col_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rf_prediction&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I then write to disk the Hail &lt;code&gt;Table&lt;/code&gt; containing a summary of the number of variants originally labeled as true or false positives and the prediction by the model. In other words - a confusion&amp;nbsp;matrix:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;rf_summary_ht&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rf_results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;tp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;fp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rf_train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rf_label&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rf_prediction&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;aggregate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="n"&gt;rf_summary_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rf_summary.ht&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;overwrite&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I unpack the &lt;code&gt;rf_results Table&lt;/code&gt; fields and join them in the sample &lt;span class="caps"&gt;QC&lt;/span&gt; &lt;code&gt;MatrixTable&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;variantqc_pass&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;rf_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;locus&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alleles&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It can be easily filtered to keep only variants predicted to be true positives by the model and then written to&amp;nbsp;disk:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;variantqc_pass&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;variantqc_pass&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rf_prediction&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;TP&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;variantqc_pass&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;variantqc_pass.mt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;overwrite&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Filter loci by coordinates or allelic&amp;nbsp;frequency&lt;/h2&gt;
&lt;p&gt;Now the dataset has passed all &lt;span class="caps"&gt;QC&lt;/span&gt;, I may query it to search for new variants or answer other scientific questions. To illustrate that, I will filter the dataset to contain variants in delimited regions of the genome with a certain range of allelic frequencies. To parse specific regions from a genome, I can create a Python list of strings representing exact or approximate&amp;nbsp;coordinates:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;intervals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;chr10:52765380-52772784&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;chr1:100M-200M&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I will apply the filter to a &lt;code&gt;MatrixTable&lt;/code&gt; with &lt;code&gt;hl.parse_locus_interval()&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;filtered_mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_intervals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;variantqc_pass&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_locus_interval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reference_genome&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;intervals&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I can also pinpoint an individual locus and create a window of nucleotides before and after it. In the code below I create a window of 100,000 nucleotides before and after a specific position in chromosome&amp;nbsp;X.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;locus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_locus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;chrX:23833353&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;window&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;locus&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;filtered_mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;variantqc_pass&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contains&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;variantqc_pass&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;locus&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If I wanted to check the first few genotypes of the filtered &lt;code&gt;MatrixTable&lt;/code&gt; by the specified window I would use the command&amp;nbsp;below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;filtered_mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GT&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Since this coordinate is not on the pseudoautosomal region (&lt;span class="caps"&gt;PAR&lt;/span&gt;) of the X chromosomes, karyotypically normal male individuals will be haploid around this region, and Hail would correctly show only one allele instead of two in the &lt;code&gt;GT&lt;/code&gt; call.&lt;/p&gt;
&lt;p&gt;Since I used &lt;code&gt;hl.variant_qc()&lt;/code&gt; at the beginning of variant &lt;span class="caps"&gt;QC&lt;/span&gt;, I may filter the variants by their allelic frequency. For example, If I wanted to keep only the variants with less than 1% frequency I would&amp;nbsp;do:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;filtered_mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;filtered_mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filtered_mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variant_qc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AF&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Remember to &lt;code&gt;write()&lt;/code&gt; the &lt;code&gt;MatrixTable&lt;/code&gt; if you want to save the dataset with any applied&amp;nbsp;filters.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post&amp;nbsp;I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introduced the Hail&amp;nbsp;framework;&lt;/li&gt;
&lt;li&gt;Mentioned software used for preparing multi-sample &lt;span class="caps"&gt;VCF&lt;/span&gt; input starting with multiple gVCF&amp;nbsp;files;&lt;/li&gt;
&lt;li&gt;Demonstrated how to perform sample and variant &lt;span class="caps"&gt;QC&lt;/span&gt; with&amp;nbsp;Hail;&lt;/li&gt;
&lt;li&gt;Demonstrated how filter dataset according to quality metrics, locus or loci&amp;nbsp;interval.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This demonstration was made possible with the help of by insights acquired by reading through&amp;nbsp;the:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://discuss.hail.is/"&gt;Hail Discussion Forum&lt;/a&gt;&amp;nbsp;posts;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/populationgenomics/joint-calling"&gt;Centre for Population Genomics GitHub repository&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/broadinstitute/gnomad_methods"&gt;gnomADs&amp;#8217; utilities GitHub repository&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;gnomAD team&amp;#8217;s supplementary material from their &lt;a href="https://www.nature.com/articles/s41586-020-2308-7"&gt;2020 Nature paper&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;p&gt;In this post I gave general directions how to combine multiple gVCF files into one single &lt;span class="caps"&gt;VCF&lt;/span&gt; input. Hail actually has an experimental function that has this very purpose: &lt;a href="https://hail.is/docs/0.2/experimental/vcf_combiner.html"&gt;&lt;code&gt;hl.experimental.run_combiner()&lt;/code&gt;&lt;/a&gt;. However, I tried to use this function and had problems with it. It generates a &amp;#8220;sparse&amp;#8221; &lt;code&gt;MatrixTable&lt;/code&gt; and unfortunately I found the function documentation insufficiently clear on how to work with this slightly different form of intermediate input, so I resorted to &lt;span class="caps"&gt;GATK&lt;/span&gt;&amp;#8217;s &lt;code&gt;GenomicsDBImport&lt;/code&gt; as stated. Since Hail is in active development, I expect improvement on both the function and on its&amp;nbsp;documentation.&lt;/p&gt;
&lt;p&gt;Throughout the demonstration I used &lt;code&gt;write()&lt;/code&gt; method to write &lt;code&gt;MatrixTable&lt;/code&gt;s to disk and later read them back into the session with &lt;code&gt;read_matrix_table()&lt;/code&gt;. Alternatively I could have used Hail&amp;#8217;s &lt;code&gt;checkpoint()&lt;/code&gt; method as an alias for these sequential operations. Read the documentation &lt;a href="https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.checkpoint"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.einstein.br/Pages/Home.aspx"&gt;Hospital Israelita Albert&amp;nbsp;Einstein&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/"&gt;Hail |&amp;nbsp;Index&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.broadinstitute.org/"&gt;Broad&amp;nbsp;Institute&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gnomAD.broadinstitute.org/"&gt;gnomAD&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/#install"&gt;Hail |&amp;nbsp;Index&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pypi.org/project/gnomAD/"&gt;gnomAD module |&amp;nbsp;PyPi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gatk.broadinstitute.org/hc/en-us"&gt;Genomic Analysis&amp;nbsp;Toolkit&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://samtools.github.io/bcftools/bcftools.html"&gt;bcftools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;Setting Up Your Unix Computer for Bioinformatics&amp;nbsp;Analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gatk.broadinstitute.org/hc/en-us/articles/360035531812-GVCF-Genomic-Variant-Call-Format"&gt;Genomic Variant Call Format&amp;nbsp;(gVCF)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.illumina.com/products/by-type/informatics-products/dragen-bio-it-platform.html"&gt;Illumina &lt;span class="caps"&gt;DRAGEN&lt;/span&gt; Bio-&lt;span class="caps"&gt;IT&lt;/span&gt; Platform| Variant calling &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; secondary genomic analysis software&amp;nbsp;tool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://samtools.github.io/hts-specs/VCFv4.2.pdf"&gt;Variant Call Format specification | version&amp;nbsp;4.2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gatk.broadinstitute.org/hc/en-us/articles/360036883491-GenomicsDBImport"&gt;&lt;span class="caps"&gt;GATK&lt;/span&gt; |&amp;nbsp;GenomicsDBImport&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gatk.broadinstitute.org/hc/en-us/articles/360035531112--How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering"&gt;&lt;span class="caps"&gt;GATK&lt;/span&gt; | Variant Qualit Score Recalibration (&lt;span class="caps"&gt;VQSR&lt;/span&gt;)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0;tab=objects?prefix=&amp;amp;forceOnObjectsSortingFiltering=false"&gt;&lt;span class="caps"&gt;GATK&lt;/span&gt; Resource Bundle at Google&amp;nbsp;Cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/storage/docs/gsutil"&gt;gsutil tool | Google&amp;nbsp;Cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/index.html"&gt;Hail | Hail&amp;nbsp;0.2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/tutorials/07-matrixtable.html"&gt;Hail | MatrixTable&amp;nbsp;Tutorial&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/methods/genetics.html#hail.methods.sample_qc"&gt;Hail | Genetics |&amp;nbsp;sample_qc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.bokeh.org/en/latest/index.html"&gt;Bokeh&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/methods/genetics.html#hail.methods.hwe_normalized_pca"&gt;Hail | Genetics |&amp;nbsp;hwe_normalized_pca&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/methods/relatedness.html#hail.methods.pc_relate"&gt;Hail |&amp;nbsp;Relatedness&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/methods/misc.html#hail.methods.maximal_independent_set"&gt;Hail |&amp;nbsp;Miscellaneous&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/methods/genetics.html#hail.methods.variant_qc"&gt;Hail | Genetics |&amp;nbsp;variant_qc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gnomAD.broadinstitute.org/news/2018-10-gnomAD-v2-1/"&gt;gnomAD v2.1 | gnomAD&amp;nbsp;news&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/functions/genetics.html#hail.expr.functions.parse_locus_interval"&gt;Hail | Genetics&amp;nbsp;functions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://discuss.hail.is/"&gt;Hail&amp;nbsp;Discussion&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/populationgenomics/joint-calling"&gt;Centre for Population Genomics GitHub&amp;nbsp;repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/broadinstitute/gnomad_methods"&gt;gnomADs&amp;#8217; utilities GitHub&amp;nbsp;repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.nature.com/articles/s41586-020-2308-7"&gt;The mutational constraint spectrum quantified from variation in 141,456&amp;nbsp;humans&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/experimental/vcf_combiner.html"&gt;Hail | &lt;span class="caps"&gt;VCF&lt;/span&gt;&amp;nbsp;Combiner&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.checkpoint"&gt;Hail | MatrixTable |&amp;nbsp;checkpoint&lt;/a&gt;&lt;/p&gt;</content><category term="Python"></category><category term="Bioinformatics"></category><category term="Genomics"></category><category term="Hail"></category></entry><entry><title>How to Query Ensembl BioMart withÂ Python</title><link href="https://antoniocampos13.github.io/how-to-query-ensembl-biomart-with-python.html" rel="alternate"></link><published>2021-01-19T10:20:00-03:00</published><updated>2021-01-19T10:20:00-03:00</updated><author><name>Antonio Victor Campos Coelho</name></author><id>tag:antoniocampos13.github.io,2021-01-19:/how-to-query-ensembl-biomart-with-python.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently, me and my colleagues wrote a manuscript involving meta-analysis of &lt;span class="caps"&gt;RNA&lt;/span&gt;-Seq studies. One of my tasks of this project was to perform a Gene Ontology (&lt;span class="caps"&gt;GO&lt;/span&gt;) enrichment analysis: &lt;a href="http://geneontology.org/docs/go-enrichment-analysis/"&gt;&amp;#8220;[G]iven a set of genes that are up-regulated under certain conditions, an enrichment analysis will find which &lt;span class="caps"&gt;GO â€¦&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently, me and my colleagues wrote a manuscript involving meta-analysis of &lt;span class="caps"&gt;RNA&lt;/span&gt;-Seq studies. One of my tasks of this project was to perform a Gene Ontology (&lt;span class="caps"&gt;GO&lt;/span&gt;) enrichment analysis: &lt;a href="http://geneontology.org/docs/go-enrichment-analysis/"&gt;&amp;#8220;[G]iven a set of genes that are up-regulated under certain conditions, an enrichment analysis will find which &lt;span class="caps"&gt;GO&lt;/span&gt; terms are over-represented (or under-represented) using annotations for that gene set&amp;#8221;&lt;/a&gt;. In other words, I could verify which cellular pathways were in action during the experimental&amp;nbsp;conditions.&lt;/p&gt;
&lt;p&gt;After I finished the &lt;span class="caps"&gt;GO&lt;/span&gt; analysis, I got a spreadsheet with a list of &lt;span class="caps"&gt;GO&lt;/span&gt; terms &amp;mdash; brief descriptions of the cellular process performed by each pathway. When I was thinking about the pathways, I started to wonder: &amp;#8220;which proteins participate into each pathway?&amp;#8221; So I decided to go data mining the &lt;a href="https://m.ensembl.org/biomart/martview"&gt;Ensembl BioMart&lt;/a&gt; to find out those protein&amp;nbsp;genes.&lt;/p&gt;
&lt;p&gt;Ensembl is a huge project by the European Bioinformatics Institute and the Wellcome Trust Sanger Institute to provide databases of annotated genomes for several (mainly vertebrate) species. BioMart is one of their data mining tools. In this post, I will describe how I used Python to query BioMart. I will introduce a simple function to generate &lt;a href="https://www.gnu.org/software/wget/"&gt;&lt;code&gt;GNU Wget&lt;/code&gt;&lt;/a&gt; commands to retrieve query results via &lt;a href="https://en.wikipedia.org/wiki/Representational_state_transfer"&gt;RESTful&lt;/a&gt; access. Then, I will show how I aggregated the data to met my objective (i.e. list all genes participating in a biological pathway represented by a &lt;span class="caps"&gt;GO&lt;/span&gt;&amp;nbsp;term).&lt;/p&gt;
&lt;p&gt;The code and example files presented here are available in my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2021_01_18_How_to_Query_BioMart_Python"&gt;portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prepare&amp;nbsp;identifiers&lt;/h2&gt;
&lt;p&gt;I created a project folder where I saved the Python script with this demonstration&amp;#8217;s code (&lt;code&gt;ensembl_rest.py&lt;/code&gt;) and two subfolders. The &lt;code&gt;data&lt;/code&gt; folder holds the example data: a spreadsheet named &lt;code&gt;go_demo.xlsx&lt;/code&gt;. The &lt;code&gt;src&lt;/code&gt; folder contains the functions&amp;#8217; code. I will write about them&amp;nbsp;later.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
â”œâ”€â”€ data
â”‚   â””â”€â”€ go_demo.xlsx
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ files_to_pandas.py
â”‚   â”œâ”€â”€ lists_ensembl.R
â”‚   â””â”€â”€ query_biomart.py
â””â”€â”€ ensembl_rest.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The spreadsheet contains just two rows of data (so that the computation can be completed quickly). They are derived from a &lt;span class="caps"&gt;GO&lt;/span&gt; enrichment analysis output performed by &lt;a href="https://www.rdocumentation.org/packages/limma/versions/3.28.14/topics/goana"&gt;&lt;code&gt;limma&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s package &lt;code&gt;goana&lt;/code&gt; function&lt;/a&gt; available for R software. The &lt;span class="caps"&gt;GO&lt;/span&gt; ids were the &lt;strong&gt;identifiers&lt;/strong&gt; (search keywords) I used to query BioMart. You may use whatever identifier recognized by BioMart (more on that later). Just be sure they are on a nicely-named column so you can read it into&amp;nbsp;Python.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Print screen of identifiers file" src="https://antoniocampos13.github.io/images/go_demo_xlsx.png"&gt;&lt;/p&gt;
&lt;h2&gt;Load modules and set file&amp;nbsp;paths&lt;/h2&gt;
&lt;p&gt;These are the modules I&amp;nbsp;used:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;glob&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;subprocess&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;dask.dataframe&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;dd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dask&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;delayed&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;src.files_to_pandas&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;files_to_pandas&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;src.query_biomart&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;query_biomart&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first four modules are from Python&amp;#8217;s standard library. I already used &lt;code&gt;dask&lt;/code&gt; &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-2.html"&gt;before&lt;/a&gt;. If you do not have &lt;code&gt;dask&lt;/code&gt; or &lt;code&gt;pandas&lt;/code&gt; installed, do it now with &lt;code&gt;pip&lt;/code&gt;. I installed &lt;a href="https://openpyxl.readthedocs.io/en/stable/"&gt;&lt;code&gt;openpyxl&lt;/code&gt;&lt;/a&gt; as well, since it serves to read/write Excel&amp;nbsp;spreadsheets.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install &lt;span class="s2"&gt;&amp;quot;dask[complete]&amp;quot;&lt;/span&gt; pandas openpyxl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Observe that I am explicitly loading the functions by prefixing the modules names with &lt;code&gt;src.&lt;/code&gt; (remember that we can call every Python script a&amp;nbsp;module).&lt;/p&gt;
&lt;p&gt;Using &lt;a href="https://docs.python.org/3/library/pathlib.html"&gt;&lt;code&gt;pathlib.Path&lt;/code&gt;&lt;/a&gt; I nicely set the folders and files paths, using the project folder as the&amp;nbsp;root.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resolve&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# project root&lt;/span&gt;
&lt;span class="n"&gt;data_folder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;go_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;go_demo.xlsx&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I loaded the identifiers file into a &lt;code&gt;pandas.DataFrame&lt;/code&gt; with the help of &lt;code&gt;openpyxl&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_excel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;go_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;engine&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;openpyxl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Configure the queries: datasets, filters, values and&amp;nbsp;attributes&lt;/h2&gt;
&lt;p&gt;Now let&amp;#8217;s examine an excerpt from the &lt;code&gt;query_biomart()&lt;/code&gt; function code. It contains the necessary modules, the function&amp;#8217;s arguments and some of their types hinted with the help of &lt;code&gt;typing&lt;/code&gt; module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;re&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;typing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;query_biomart&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;filter_names&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="n"&gt;attribute_names&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="n"&gt;dataset_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hsapiens_gene_ensembl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;formatter&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;TSV&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;bool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;uniqueRows&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;bool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

&lt;span class="c1"&gt;# ...Function here ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There is a description of each argument&amp;nbsp;below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;filter_names&lt;/code&gt; (List[str]): A list of strings that define restrictions on the&amp;nbsp;query.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;values&lt;/code&gt; (List[str]): A list of strings containing the values that will be used for the filters. Must have same length of &lt;code&gt;filter_names&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;attribute_names&lt;/code&gt; (List[str]): A list of strings that define the data characteristics that must be retrieved for the filtered&amp;nbsp;data.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dataset_name&lt;/code&gt; (str, optional): A string indicating which dataset will be queried. Each species has their respective dataset. Defaults to &amp;#8220;hsapiens_gene_ensembl&amp;#8221;&amp;nbsp;(humans).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;formatter&lt;/code&gt; (str, optional): A string to indicate how output must be formatted. Options: &amp;#8220;&lt;code&gt;HTML&lt;/code&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;, &amp;#8220;&lt;code&gt;CSV&lt;/code&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;,&amp;#8221;&lt;code&gt;TSV&lt;/code&gt;&amp;#8221; and &amp;#8220;&lt;code&gt;XLS&lt;/code&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;. Defaults to &amp;#8220;&lt;code&gt;TSV&lt;/code&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;header&lt;/code&gt; (bool, optional): A Boolean indicating if the output must have a header. Defaults to &lt;code&gt;False&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;uniqueRows&lt;/code&gt; (bool, optional): A Boolean indicating if the output must have unique rows (deduplicate data). Defaults to &lt;code&gt;False&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus I searched&amp;nbsp;BioMart:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Into &lt;em&gt;Homo sapiens&lt;/em&gt; &lt;strong&gt;dataset&lt;/strong&gt;&amp;nbsp;(&amp;#8220;hsapiens_gene_ensembl&amp;#8221;),&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Filtering&lt;/strong&gt; by &lt;span class="caps"&gt;GO&lt;/span&gt; terms&amp;nbsp;(&amp;#8220;go_parent_term&amp;#8221;),&lt;/li&gt;
&lt;li&gt;Using &lt;span class="caps"&gt;GO&lt;/span&gt; ids (for example, &lt;span class="caps"&gt;GO&lt;/span&gt;:0002790) as search keywords (&lt;strong&gt;values&lt;/strong&gt;),&lt;/li&gt;
&lt;li&gt;And wanted to retrieve the gene name &lt;strong&gt;attribute&lt;/strong&gt;&amp;nbsp;(&amp;#8220;external_gene_name&amp;#8221;).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here is an example of the function with the inputs above. All defaults were maintained, except for &lt;code&gt;uniqueRows&lt;/code&gt;, which I set to &lt;code&gt;True&lt;/code&gt; to remove duplicate&amp;nbsp;data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;query_biomart&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filter_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go_parent_term&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;GO:0002790&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;attribute_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;external_gene_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;uniqueRows&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;See the Appendix to help you see which information can be searched in&amp;nbsp;BioMart.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The output of the function is a string &amp;dash; a ready-to-use &lt;code&gt;GNU Wget&lt;/code&gt; Unix program command. The query string inside the command has a &lt;a href="https://m.ensembl.org/info/data/biomart/biomart_restful.html"&gt;&lt;code&gt;XML&lt;/code&gt; format&lt;/a&gt; that is sent to Ensembl&amp;#8217;s servers. The query result will then be saved on a file named based on the values (search keywords) of the query. The extension of the file depends on the &lt;code&gt;formatter&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;I created a &lt;code&gt;Bash&lt;/code&gt; script named &lt;code&gt;commands.sh&lt;/code&gt; inside the &lt;code&gt;data&lt;/code&gt; folder to backup and document my work. I did this with a &lt;code&gt;for&lt;/code&gt; loop. In other words, Python wrote for me a series of &lt;code&gt;Wget&lt;/code&gt; commands, one for each keyword in the identifiers data frame (&lt;code&gt;go_ids&lt;/code&gt; column &amp;dash; &lt;code&gt;df["go_ids"]&lt;/code&gt;) alongside the desired filter and&amp;nbsp;attribute:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;commands.sh&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;newline&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;commands&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

&lt;span class="n"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;#!/bin/bash&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;go_id&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go_ids&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
   &lt;span class="n"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query_biomart&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filter_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go_parent_term&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
   &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;go_id&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;attribute_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;external_gene_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
   &lt;span class="n"&gt;uniqueRows&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;See a print screen of the &lt;code&gt;commands.sh&lt;/code&gt; file below. Notice the shebang (&lt;code&gt;#!&lt;/code&gt;) line: it ensures the file can be executed by &lt;code&gt;Bash&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Print screen commands.sh file" src="https://antoniocampos13.github.io/images/commands_print.png"&gt;&lt;/p&gt;
&lt;h2&gt;Execute the&amp;nbsp;queries&lt;/h2&gt;
&lt;p&gt;I executed the &lt;code&gt;commands.sh&lt;/code&gt; file with the help of Python&amp;#8217;s &lt;code&gt;subprocess&lt;/code&gt; library. Notice I indicated the path of the file by converting the &lt;code&gt;libpath.Path()&lt;/code&gt; address to a string with &lt;code&gt;str()&lt;/code&gt; and changed directories with the &lt;code&gt;cwd&lt;/code&gt; argument so that the queries results would be downloaded into the &lt;code&gt;data&lt;/code&gt; folder as&amp;nbsp;well.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;commands.sh&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;cwd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data_folder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I waited for the download completion. The print screen below shows part of the &lt;code&gt;wget&lt;/code&gt; progress.&lt;/p&gt;
&lt;p&gt;&lt;img alt="wget download progress" src="https://antoniocampos13.github.io/images/wget_download.png"&gt;&lt;/p&gt;
&lt;p&gt;After a while, two files, named &lt;code&gt;GO0002790.txt&lt;/code&gt; and &lt;code&gt;GO0019932.txt&lt;/code&gt; were created, each containing a list of several genes related with the &lt;span class="caps"&gt;GO&lt;/span&gt;&amp;nbsp;ids:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ commands.sh
â”‚   â”œâ”€â”€ go_demo.xlsx
â”‚   â”œâ”€â”€ GO0002790.txt
â”‚   â”œâ”€â”€ GO0019932.txt
â”‚   â””â”€â”€ lists_ensembl.xlsx
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ files_to_pandas.py
â”‚   â”œâ”€â”€ lists_ensembl.R
â”‚   â””â”€â”€ query_biomart.py
â””â”€â”€ ensembl_rest.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Result excerpt" src="https://antoniocampos13.github.io/images/go0002790.png"&gt;&lt;/p&gt;
&lt;h2&gt;Label and unify data into a &lt;code&gt;pandas.DataFrame&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Similarly to my &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-2.html"&gt;previous post&lt;/a&gt; I needed to make a relation of &lt;span class="caps"&gt;GO&lt;/span&gt; id &amp;#8212;&amp;gt; genes, so I adapted and renamed the old function I used on that occasion. The new function is named &lt;code&gt;files_to_pandas&lt;/code&gt;. It takes a file and converts it into a &lt;code&gt;pandas.DataFrame&lt;/code&gt; while creating a new column to accommodate the file name into a new column (in our case, the files were named after the values &amp;dash; &lt;span class="caps"&gt;GO&lt;/span&gt;&amp;nbsp;ids).&lt;/p&gt;
&lt;p&gt;The idea is that I needed to create one data frame for each file. Thus, I used the &lt;code&gt;glob&lt;/code&gt; method to get an iterable of file names and used &lt;code&gt;dask&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;a href="https://docs.dask.org/en/latest/delayed.html"&gt;&lt;code&gt;delayed&lt;/code&gt;&lt;/a&gt; method to assemble the several &lt;code&gt;pandas.DataFrames&lt;/code&gt; generated by the &lt;code&gt;files_to_pandas&lt;/code&gt; function loop into a unified &lt;code&gt;dask.DataFrame&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;file_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_folder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;glob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;*.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dfs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;delayed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;files_to_pandas&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;file_list&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;ddf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_delayed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dfs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Reorganize data with &lt;code&gt;defaultdict&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;I could have ended in the previous step, but I surmised that I could condense the data frame into fewer rows, by aggregating all genes into the same row of the equivalent &lt;span class="caps"&gt;GO&lt;/span&gt; id. So I had the idea to use Python&amp;#8217;s &lt;a href="https://docs.python.org/3/library/collections.html#collections.defaultdict"&gt;&lt;code&gt;defaultdict&lt;/code&gt;&lt;/a&gt;. It is a subclass of the &lt;code&gt;dictionary&lt;/code&gt; class. First, I initialize an empty &lt;code&gt;defaultdict&lt;/code&gt;. It will have &lt;span class="caps"&gt;GO&lt;/span&gt; ids as keys and a list of gene names as&amp;nbsp;values:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;go_to_genes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I simply had to loop through the rows of the &lt;code&gt;dask.DataFrame&lt;/code&gt;, insert the keys into the &lt;code&gt;defaultdict&lt;/code&gt; and appending the gene names into the value-list one by&amp;nbsp;one.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;go_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gene&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ddf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;filename&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ddf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gene_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="n"&gt;go_to_genes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;go_id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Save the &lt;code&gt;defaultdict&lt;/code&gt; into a new &lt;code&gt;pandas.DataFrame&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Finally, I created another data frame to store the contents of the &lt;code&gt;defaultdict&lt;/code&gt;. Notice how I used the &lt;code&gt;.items()&lt;/code&gt; method. It is the correct way to access the contents of a &lt;code&gt;defaultdict&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;go_genes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;go_to_genes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go_ids&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gene_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Error&amp;nbsp;checking&lt;/h3&gt;
&lt;p&gt;Just to be sure that all queries worked correctly, a used a simple loop to check the contents of the gene names&amp;nbsp;column:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;go_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gene&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;go_genes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go_ids&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;go_genes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gene_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Query&amp;quot;&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Error in &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;go_id&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Error check done.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I am using &amp;#8220;Query&amp;#8221; is used as an example, because BioMart errors may contain this word in their error messages, such&amp;nbsp;as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;quot;Query ERROR: caught BioMart::Exception::Database: Could not connect to mysql database ...&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Of course, other strings can be used. In retrospect, &amp;#8220;&lt;span class="caps"&gt;ERROR&lt;/span&gt;&amp;#8221; would have been an even better option than &amp;#8220;Query&amp;#8221;. If some error were detected, the loop would print the &lt;span class="caps"&gt;GO&lt;/span&gt; id that failed the query (due to a network error, for example), so I would have to retry the&amp;nbsp;query.&lt;/p&gt;
&lt;h2&gt;Annotate original data&amp;nbsp;frame&lt;/h2&gt;
&lt;p&gt;No errors were found, so I finally could annotate my original data frame by joining them by the &lt;code&gt;"go_ids"&lt;/code&gt; column. I even created a new column (&lt;code&gt;"gene_string"&lt;/code&gt;) to convert the Python lists into a nicely formatted comma-delimited string of gene&amp;nbsp;names:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df_annotated&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go_ids&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;go_genes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go_ids&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;df_annotated&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gene_string&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;, &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;element&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;element&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;df_annotated&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gene_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I then saved the result into a new tab (named &lt;code&gt;go annotated&lt;/code&gt;) inside my &lt;code&gt;go_demo.xlsx&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ExcelWriter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;go_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;df_annotated&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_excel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sheet_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go annotated&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post,&amp;nbsp;I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introduced a function to create customized ready-to-use query strings via &lt;span class="caps"&gt;REST&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;&amp;nbsp;access;&lt;/li&gt;
&lt;li&gt;Provided a file with the descriptors of data deposited in BioMart (&lt;code&gt;list_ensembl.xlsx&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;Demonstrated how to use Python to invoke &lt;code&gt;GNU Wget&lt;/code&gt; to download the the queries&amp;#8217;&amp;nbsp;results;&lt;/li&gt;
&lt;li&gt;Demonstrated how to aggregate and manipulate the queries results using &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;dask&lt;/code&gt; and &lt;code&gt;defaultdict&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Feel free to use the &lt;code&gt;query_biomart()&lt;/code&gt; function to data mine BioMart as you&amp;nbsp;wish!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;p&gt;I prepared a spreadsheet named &lt;code&gt;lists_ensembl.xlsx&lt;/code&gt; also stored into the &lt;code&gt;data&lt;/code&gt; folder. Ensembl has a lot of datasets, filters and attributes, so examine the other rows if you are interested. I produced the spreadsheet with the help of &lt;a href="https://bioconductor.org/packages/release/bioc/vignettes/biomaRt/inst/doc/biomaRt.html"&gt;&lt;code&gt;biomaRt&lt;/code&gt; R package&lt;/a&gt;. If you prefer R over Python, be sure to try querying BioMart with it. The &lt;code&gt;lists_ensembl.R&lt;/code&gt; file contains the R code that generated the&amp;nbsp;spreadsheet.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://geneontology.org/docs/go-enrichment-analysis/"&gt;&lt;span class="caps"&gt;GO&lt;/span&gt; enrichment&amp;nbsp;analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://m.ensembl.org/biomart/martview"&gt;Ensembl&amp;nbsp;BioMart&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://m.ensembl.org/info/data/biomart/biomart_restful.html"&gt;BioMart RESTful&amp;nbsp;access&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.gnu.org/software/wget/"&gt;Wget - &lt;span class="caps"&gt;GNU&lt;/span&gt; Project - Free Software&amp;nbsp;Foundation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Representational_state_transfer"&gt;Representational state transfer -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.rdocumentation.org/packages/limma/versions/3.28.14/topics/goana"&gt;goana function | R&amp;nbsp;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-2.html"&gt;Working with Cancer Genomics Cloud datasets in a PostgreSQL database (Part&amp;nbsp;2)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/pathlib.html"&gt;pathlib â€” Object-oriented filesystem paths &amp;#8212; Python 3.9.1&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://openpyxl.readthedocs.io/en/stable/"&gt;openpyxl - A Python library to read/write Excel 2010 xlsx/xlsm files &amp;mdash; openpyxl 3.0.6&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bioconductor.org/packages/release/bioc/vignettes/biomaRt/inst/doc/biomaRt.html"&gt;The biomaRt users&amp;nbsp;guide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.dask.org/en/latest/delayed.html"&gt;Dask  documentation -&amp;nbsp;Delayed&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/collections.html#collections.defaultdict"&gt;collections â€” Container datatypes | Python 3.9.1&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;</content><category term="Python"></category><category term="Bioinformatics"></category><category term="Ensembl"></category><category term="BioMart"></category><category term="omics"></category><category term="data mining"></category></entry><entry><title>Machine Learning with Python: Supervised Classification of TCGA Prostate Cancer Data (Part 1 - Making FeaturesÂ Datasets)</title><link href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-1-making-features-datasets.html" rel="alternate"></link><published>2020-11-05T14:50:00-03:00</published><updated>2020-11-05T14:50:00-03:00</updated><author><name>Antonio Victor Campos Coelho</name></author><id>tag:antoniocampos13.github.io,2020-11-05:/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-1-making-features-datasets.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In a &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html"&gt;previous post&lt;/a&gt;, I showed how to retrieve &lt;a href="https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga"&gt;The Cancer Genome Atlas (&lt;span class="caps"&gt;TCGA&lt;/span&gt;)&lt;/a&gt; data from the &lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics Cloud (&lt;span class="caps"&gt;CGC&lt;/span&gt;) platform&lt;/a&gt;. I downloaded gene expression quantification data, created a relational database with PostgreSQL, and created a dataset uniting the raw quantification data for 675 differentially expressed genes &lt;a href="https://antoniocampos13.github.io/differential-expression-analysis-with-edger-in-r.html#differential-expression-analysis-with-edger-in-r"&gt;identified â€¦&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In a &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html"&gt;previous post&lt;/a&gt;, I showed how to retrieve &lt;a href="https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga"&gt;The Cancer Genome Atlas (&lt;span class="caps"&gt;TCGA&lt;/span&gt;)&lt;/a&gt; data from the &lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics Cloud (&lt;span class="caps"&gt;CGC&lt;/span&gt;) platform&lt;/a&gt;. I downloaded gene expression quantification data, created a relational database with PostgreSQL, and created a dataset uniting the raw quantification data for 675 differentially expressed genes &lt;a href="https://antoniocampos13.github.io/differential-expression-analysis-with-edger-in-r.html#differential-expression-analysis-with-edger-in-r"&gt;identified by edgeR&lt;/a&gt;, race, age at diagnosis and tumor size at&amp;nbsp;diagnosis.&lt;/p&gt;
&lt;p&gt;In this post, I will use Python to prepare features datasets to use them to produce a classification model using machine learning tools, especially the &lt;code&gt;scikit-learn&lt;/code&gt; module. Check its documentation &lt;a href="https://scikit-learn.org/stable/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The dataset and code presented here are available in my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2020_11_05_Supervised_Machine_Leaning_TCGA_Data"&gt;portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prepare&amp;nbsp;workspace&lt;/h2&gt;
&lt;p&gt;Since I will generate a statistical model, it is important to test it in never-seen before data, to assess its prediction validity. Thus, I will use a customized script, &lt;code&gt;make_features.py&lt;/code&gt;, to transform and split the dataset into separate train and test datasets. I will fit the model and then use it to predict the risk of prostate cancer of the subjects included in the test dataset and then compare with actual status (control: prostate cancer in remission/cases: prostate cancer progressing) to see how well it&amp;nbsp;predicted.&lt;/p&gt;
&lt;p&gt;Check the &lt;code&gt;make_features.py&lt;/code&gt;. Let&amp;#8217;s examine it. First, I import the necessary&amp;nbsp;modules:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;janitor&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;jn&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;preprocessing&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_selection&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;code&gt;pathlib&lt;/code&gt; is a module from the Python standard library. It helps file path management. &lt;code&gt;janitor&lt;/code&gt; is a module for data clean-up and manipulation. &lt;code&gt;pandas&lt;/code&gt; is also used for data manipulation and transformation, especially if it is contained on data frames. &lt;code&gt;sklearn&lt;/code&gt; is an alias for &lt;code&gt;scikit-learn&lt;/code&gt;. It is a powerhouse of several machine learning functions and utilities. Install all non-standard library modules using your Python package manager, usually &lt;code&gt;pip&lt;/code&gt; or through anaconda, if you are using a conda environment with&amp;nbsp;Python.&lt;/p&gt;
&lt;p&gt;Next, I set up some constants that I will use&amp;nbsp;later:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;RANDOM_SEED&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;
&lt;span class="n"&gt;TEST_SIZE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.30&lt;/span&gt;
&lt;span class="n"&gt;INDEX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;676&lt;/span&gt;
&lt;span class="n"&gt;CORR_THRESHOLD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;RANDOM_SEED&lt;/code&gt;: An integer used to initialize the pseudo-random number generator. It helps generate reproducible outputs in different&amp;nbsp;sessions/computers;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TEST_SIZE&lt;/code&gt;: A decimal (float) number indicating the size of the test dataset. Currently, it is 30% of the samples in the complete&amp;nbsp;dataset;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;INDEX&lt;/code&gt;: An integer indicating a slicing index. I will explain it&amp;nbsp;later.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CORR_THRESHOLD&lt;/code&gt;: A float number indicating a threshold for eliminating correlated variables in the dataset; it helps overfitting by reducing the &lt;a href="https://en.wikipedia.org/wiki/Multicollinearity"&gt;multicollinearity issue&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next, I will indicate the path of the dataset, which i saved as a &lt;span class="caps"&gt;CSV&lt;/span&gt;&amp;nbsp;file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resolve&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt;
&lt;span class="n"&gt;CSV_FILE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;interim&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;prostate_cancer_dataset.csv&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;See below the structure of the current working&amp;nbsp;directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ interim
â”‚   â”‚   â””â”€â”€ prostate_cancer_dataset.csv
â”‚   â””â”€â”€ processed
â”œâ”€â”€ models
â””â”€â”€ src
    â”œâ”€â”€ features
    â”‚   â””â”€â”€ make_features.py
    â””â”€â”€ models
        â””â”€â”€ make_model.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Load data into&amp;nbsp;Pandas&lt;/h2&gt;
&lt;p&gt;Now, I create a &lt;code&gt;pandas.DataFrame&lt;/code&gt; using the &lt;code&gt;read_csv&lt;/code&gt; function, and assign it to &lt;code&gt;df&lt;/code&gt; object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CSV_FILE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Check the column names with the &lt;code&gt;columns&lt;/code&gt; method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columms&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;status&lt;/code&gt; column contains the sample labels: &lt;code&gt;0&lt;/code&gt; means &amp;#8220;control&amp;#8221; and &lt;code&gt;1&lt;/code&gt; means &amp;#8220;case&amp;#8221;. The other columns are the variables (or features) of each&amp;nbsp;sample.&lt;/p&gt;
&lt;h2&gt;Dummy-encode categorical&amp;nbsp;variables&lt;/h2&gt;
&lt;p&gt;I can now transform the data, making it suitable for machine learning modeling. Luckily, the &lt;span class="caps"&gt;TCGA&lt;/span&gt; dataset has no missing values and no gross errors are present. Otherwise, I would have to impute missing values and correct the errors. Now I will recode the categorical variables in the dataset. To check which variables are categorical, use the &lt;code&gt;pandas&lt;/code&gt; &lt;code&gt;dtypes&lt;/code&gt; method. The variables labeled with &lt;code&gt;object&lt;/code&gt; are usually categorical. If you see &lt;code&gt;object&lt;/code&gt; beside the name of a variable you know it is not categorical, then it is possible that there are missing data or some other&amp;nbsp;error.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtypes&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;drop_first&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Split&amp;nbsp;datasets&lt;/h2&gt;
&lt;p&gt;With the transformed dataset, let&amp;#8217;s go ahead and split the dataset into training and testing&amp;nbsp;datasets.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;jn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_features_targets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;status&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_selection&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TEST_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stratify&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;RANDOM_SEED&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;get_features_targets&lt;/code&gt; from &lt;code&gt;janitor&lt;/code&gt; module will get the &lt;code&gt;df&lt;/code&gt; object and separate the sample labels from the variables (features). The features will then be assigned to object &lt;code&gt;X&lt;/code&gt;, which will be instanced as a &lt;code&gt;pandas.DataFrame&lt;/code&gt; and the &lt;code&gt;status&lt;/code&gt; column (see the &lt;code&gt;target_columns&lt;/code&gt; argument) will be put in the &lt;code&gt;y&lt;/code&gt; object, which will be instanced as a &lt;code&gt;pandas.Series&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The second function, &lt;code&gt;model_selection.train_test_split&lt;/code&gt; from &lt;code&gt;sklearn&lt;/code&gt; will split the &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; objects into its training and testing counterparts &amp;#8212; therefore, four objects: &lt;code&gt;X_train&lt;/code&gt;, &lt;code&gt;X_test&lt;/code&gt;, &lt;code&gt;y_train&lt;/code&gt; and &lt;code&gt;y_test&lt;/code&gt;. Check the &lt;code&gt;TEST_SIZE&lt;/code&gt; and &lt;code&gt;RANDOM_SEED&lt;/code&gt; constants I set up in the beginning of the script being used here. They indicate that 30% of the dataset must be included as a test dataset (therefore 70% in the training dataset) and setting a integer into the &lt;code&gt;random_state&lt;/code&gt; argument ensures that the splitting outputs can be&amp;nbsp;reproduced.&lt;/p&gt;
&lt;p&gt;Also note the &lt;code&gt;stratify&lt;/code&gt; argument. It ensures that the same proportion of cases and controls are drawn for training and testing datasets. For this, I indicate the object containing the labels, which is &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The commands below print the number of cases and controls for each&amp;nbsp;dataset:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Training Set has &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; Positive Labels (cases) and &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; Negative Labels (controls)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Test Set has &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; Positive Labels (cases) and &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; Negative Labels (controls)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The&amp;nbsp;output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Training Set has 38 Positive Labels (cases) and 127 Negative Labels (controls)
Test Set has 16 Positive Labels (cases) and 55 Negative Labels (controls)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Normalize&amp;nbsp;data&lt;/h2&gt;
&lt;p&gt;Now I will standardize (scale or &lt;a href="https://en.wikipedia.org/wiki/Standard_score"&gt;Z-score normalize&lt;/a&gt;) the numerical columns. For each numerical feature, I will calculate its mean and standard deviation. Then, for each observed value of the variable, I will subtract the mean and divide by the standard&amp;nbsp;deviation.&lt;/p&gt;
&lt;p&gt;After the dummy coding of the categorical variables, they were transposed to the end of the data frame. I manually checked the column names and identified the column index. I then sliced the list of column names and stored in the &lt;code&gt;num_cols&lt;/code&gt; object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;num_cols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)[:&lt;/span&gt;&lt;span class="n"&gt;INDEX&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That&amp;#8217;s why I conveniently saved the index number into the &lt;code&gt;INDEX&lt;/code&gt; variable at the beginning of the script. It helps code reusability with different data. It is one of the advantages of avoiding the so called &lt;a href="https://www.pluralsight.com/tech-blog/avoiding-magic-numbers/"&gt;&amp;#8220;magic numbers&amp;#8221;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With the numeric columns identified, I then used the &lt;code&gt;StandardScaler()&lt;/code&gt; function from &lt;code&gt;sklearn&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;preprocessing&lt;/code&gt; module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;sca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preprocessing&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;num_cols&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;num_cols&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;num_cols&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;num_cols&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I used the function &lt;code&gt;fit_transform()&lt;/code&gt; function in the &lt;code&gt;X_train&lt;/code&gt; dataset, passing the &lt;code&gt;num_cols&lt;/code&gt; list as the indication of which columns need to be normalized (using the brackets &lt;code&gt;[]&lt;/code&gt; notation) and saving the resulting transformation with the same names into the &lt;code&gt;X_train&lt;/code&gt; object. Thus, for all effects and purposes, I am replacing the old columns with the normalized&amp;nbsp;columns.&lt;/p&gt;
&lt;p&gt;Note that for &lt;code&gt;X_test&lt;/code&gt; dataset I used a different formula, &lt;code&gt;transform()&lt;/code&gt;. This is because I am using just the coefficients fitted by &lt;code&gt;fit_transform()&lt;/code&gt; in the train dataset to generate the normalization in the test dataset. This way I am sure that the scaling is not &amp;#8220;contaminated&amp;#8221; by the test data, that is supposed to not seem before the classification model&amp;nbsp;fitting.&lt;/p&gt;
&lt;h2&gt;Remove correlated&amp;nbsp;features&lt;/h2&gt;
&lt;p&gt;Now I will filter out correlated features. Please note that I would not normally do this with genomic data, but since here is just an exercise, I will show how to do it so you can apply to your projects when necessary. Check below the code (hat tip to &lt;a href="https://towardsdatascience.com/feature-selection-in-python-recursive-feature-elimination-19f1c39b8d15"&gt;Dario RadeÄiÄ‡&lt;/a&gt; for the&amp;nbsp;snippet):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;correlated_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;correlation_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;corr&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correlation_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correlation_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;CORR_THRESHOLD&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;colname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;correlation_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;correlated_features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;colname&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The commands above will calculate the pairwise correlation between all features in the dataset, creating a &lt;code&gt;correlation_matrix&lt;/code&gt;. If the correlation between two features is above &lt;code&gt;CORR_THRESHOLD&lt;/code&gt; (currently 0.80), the loop will store the name of one of them into the &lt;code&gt;correlated_features&lt;/code&gt; set, ensuring that no names are repeated. If you want to know how may pairs of correlated features were present in the dataset, run the command below to print to the&amp;nbsp;console:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Number of correlated feature pairs: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correlated_features&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I can convert the set as list and pass it to &lt;code&gt;pandas&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt; &lt;code&gt;drop()&lt;/code&gt; method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correlated_features&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correlated_features&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I dropped the columns and saved the data frames with the same name for convenience. Note the &lt;code&gt;axis=1&lt;/code&gt; argument, it tells &lt;code&gt;drop()&lt;/code&gt; to look the &lt;em&gt;columns&lt;/em&gt; for the list elements and then remove&amp;nbsp;them.&lt;/p&gt;
&lt;h2&gt;Serializing datasets for future&amp;nbsp;use&lt;/h2&gt;
&lt;p&gt;Finally, the train and test datasets are ready for use! To save them into disk for later use, I will use &lt;code&gt;pandas&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt; &lt;code&gt;to_pickle&lt;/code&gt; method. A &amp;#8220;pickled&amp;#8221; Python object is &lt;em&gt;serialized&lt;/em&gt;: it was converted into a series of bytes (a byte stream). This byte stream therefore can be &amp;#8220;unpickled&amp;#8221; to restore the object exactly as it was when was &amp;#8220;pickled&amp;#8221;. This is useful to backup data, share with others, and so on. Using &lt;code&gt;pathlib.Path&lt;/code&gt; notation, I saved the objects into the &amp;#8220;data/processed/&amp;#8221;&amp;nbsp;folder:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;processed&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X_train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;processed&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X_test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;processed&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y_train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;processed&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y_test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note: pickled objects are Python-specific only &amp;#8212; non-Python programs may not be able to reconstruct pickled Python objects. &lt;span class="caps"&gt;WARNING&lt;/span&gt;: never, &lt;span class="caps"&gt;NEVER&lt;/span&gt;, unpickle data you do not trust. As it says in the Python documentation: &lt;a href="https://docs.python.org/3/library/pickle.html"&gt;&amp;#8220;It is possible to construct malicious pickle data which will execute arbitrary code during unpickling&amp;#8221;&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, my folders are like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ interim
â”‚   â”‚   â””â”€â”€ prostate_cancer_dataset.csv
â”‚   â””â”€â”€ processed
â”‚       â”œâ”€â”€ X_train
â”‚       â”œâ”€â”€ X_test
â”‚       â”œâ”€â”€ y_train
â”‚       â””â”€â”€ y_test
â”œâ”€â”€ models
â””â”€â”€ src
    â”œâ”€â”€ features
    â”‚   â””â”€â”€ make_features.py
    â””â”€â”€ models
        â””â”€â”€ make_model.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I showed how&amp;nbsp;to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Import a &lt;span class="caps"&gt;CSV&lt;/span&gt; dataset into &lt;code&gt;pandas&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Dummy-encode categorical&amp;nbsp;data;&lt;/li&gt;
&lt;li&gt;Split the dataset into train/test&amp;nbsp;datasets;&lt;/li&gt;
&lt;li&gt;Normalize data&amp;nbsp;(Z-scores);&lt;/li&gt;
&lt;li&gt;Serialize (pickle) the datasets for future&amp;nbsp;use.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Go to the &lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-2-making-a-model.html"&gt;Part 2&lt;/a&gt;, where I show how to use the datasets to generate a classification model for predicting risk of prostate cancer disease progression with the &lt;code&gt;make_model.py&lt;/code&gt; script.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga"&gt;The Cancer Genome Atlas&amp;nbsp;Program&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics&amp;nbsp;Cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://scikit-learn.org/stable/"&gt;scikit-learn: machine learning in Python - scikit-learn 0.23.2&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Multicollinearity"&gt;Multicollinearity -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Standard_score"&gt;Standard score -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.pluralsight.com/tech-blog/avoiding-magic-numbers/"&gt;Avoiding Magic&amp;nbsp;Numbers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://towardsdatascience.com/feature-selection-in-python-recursive-feature-elimination-19f1c39b8d15"&gt;Feature Selection in Python â€” Recursive Feature&amp;nbsp;Elimination&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/pickle.html"&gt;pickle â€” Python object serialization | Python 3.9.0&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;</content><category term="Python"></category><category term="Bioinformatics"></category><category term="gene expression"></category><category term="machine learning"></category><category term="supervised classification"></category></entry><entry><title>Machine Learning with Python: Supervised Classification of TCGA Prostate Cancer Data (Part 2 - Making aÂ Model)</title><link href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-2-making-a-model.html" rel="alternate"></link><published>2020-11-05T14:50:00-03:00</published><updated>2020-11-05T14:50:00-03:00</updated><author><name>Antonio Victor Campos Coelho</name></author><id>tag:antoniocampos13.github.io,2020-11-05:/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-2-making-a-model.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;In a &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html"&gt;previous post&lt;/a&gt;, I showed how to retrieve &lt;a href="https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga"&gt;The Cancer Genome Atlas (&lt;span class="caps"&gt;TCGA&lt;/span&gt;)&lt;/a&gt; data from the &lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics Cloud (&lt;span class="caps"&gt;CGC&lt;/span&gt;) platform&lt;/a&gt;. I downloaded gene expression quantification data, created a relational database with PostgreSQL, and created a dataset uniting the raw quantification data for 675 differentially expressed genes &lt;a href="https://antoniocampos13.github.io/differential-expression-analysis-with-edger-in-r.html#differential-expression-analysis-with-edger-in-r"&gt;identified â€¦&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;In a &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html"&gt;previous post&lt;/a&gt;, I showed how to retrieve &lt;a href="https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga"&gt;The Cancer Genome Atlas (&lt;span class="caps"&gt;TCGA&lt;/span&gt;)&lt;/a&gt; data from the &lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics Cloud (&lt;span class="caps"&gt;CGC&lt;/span&gt;) platform&lt;/a&gt;. I downloaded gene expression quantification data, created a relational database with PostgreSQL, and created a dataset uniting the raw quantification data for 675 differentially expressed genes &lt;a href="https://antoniocampos13.github.io/differential-expression-analysis-with-edger-in-r.html#differential-expression-analysis-with-edger-in-r"&gt;identified by edgeR&lt;/a&gt;, race, age at diagnosis and tumor size at&amp;nbsp;diagnosis.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In &lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-1-making-features-datasets.html"&gt;Part 1&lt;/a&gt;, I used Python to prepare features datasets to use them to produce a classification model using machine learning tools, especially the &lt;code&gt;scikit-learn&lt;/code&gt; module. Check its documentation &lt;a href="https://scikit-learn.org/stable/"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here in Part 2, I develop an illustrative model. If it were a serious model, its objective would be to predict if a person is in risk of developing prostate cancer based on personal characteristics and the expression of differentially expressed&amp;nbsp;genes.&lt;/p&gt;
&lt;p&gt;The dataset and code presented here are available in my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2020_11_05_Supervised_Machine_Leaning_TCGA_Data"&gt;portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prepare&amp;nbsp;workspace&lt;/h2&gt;
&lt;p&gt;As in Part 1, I will import the modules needed to make the model (check the &lt;code&gt;make_model.py&lt;/code&gt; script located on &lt;code&gt;src/models&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pickle&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;xgboost&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f1_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;roc_auc_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;roc_curve&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;precision_recall_curve&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.dummy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DummyClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StratifiedKFold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;KFold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cross_val_score&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.naive_bayes&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GaussianNB&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.neighbors&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KNeighborsClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.svm&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SVC&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Install all non-standard library modules using your Python package manager, usually &lt;code&gt;pip&lt;/code&gt; or through anaconda, if you are using a conda environment with&amp;nbsp;Python.&lt;/p&gt;
&lt;p&gt;Again, I will set up some constants to use&amp;nbsp;later:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;RANDOM_SEED&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;
&lt;span class="n"&gt;SPLITS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Set up the file&amp;nbsp;paths:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resolve&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt;
&lt;span class="n"&gt;features_folder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;processed&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Importing&amp;nbsp;features&lt;/h2&gt;
&lt;p&gt;If you still have the features datasets loaded on memory, the commands below are not necessary. They serve to de-serialize the pickled datasets I saved in Part 1 using &lt;code&gt;pandas&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt; &lt;code&gt;read_pickle()&lt;/code&gt; method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;X_train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;X_test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;y_train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;y_test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Select classification&amp;nbsp;model&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;scikit-learn&lt;/code&gt; module contains code for several classification models. If you are in doubt, you can select from a list of models using a scoring metric, and the choose the best-performing model. See below a loop command that do just that (hat tip to &lt;a href="https://hairysun.com/"&gt;Matt Harrison&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;models&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
&lt;span class="n"&gt;DummyClassifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;KNeighborsClassifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;GaussianNB&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;xgboost&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;XGBClassifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="bp"&gt;cls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;kfold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KFold&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_splits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;SPLITS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;RANDOM_SEED&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;cls&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;f1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;kfold&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;22&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;  F1 Score: &amp;quot;&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; STD: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I will now explain what the loop does. First, I create a list named &lt;code&gt;models&lt;/code&gt; with the name of some &lt;code&gt;sklearn&lt;/code&gt; models. Note the way their names are written: they are the names of the corresponding &lt;code&gt;sklearn&lt;/code&gt; modules.&lt;/p&gt;
&lt;p&gt;I then loop this list creating a classifier &lt;code&gt;cls&lt;/code&gt; object by calling each model. Then, I create a K-Folds cross-validator. In other words, I randomly split the training dataset into &lt;code&gt;K&lt;/code&gt; datasets (folds). Each fold is then used once as a validation while the &lt;code&gt;K - 1&lt;/code&gt; remaining folds form the training set. The &lt;code&gt;n_splits&lt;/code&gt; argument indicates &lt;code&gt;K&lt;/code&gt;, which I set up using the &lt;code&gt;SPLITS&lt;/code&gt; constant (currently 10, so &lt;code&gt;K=10&lt;/code&gt; folds). Setting a integer (&lt;code&gt;RANDOM_SEED&lt;/code&gt; constant) into the &lt;code&gt;random_state&lt;/code&gt; argument ensures that the splitting outputs can be&amp;nbsp;reproduced.&lt;/p&gt;
&lt;p&gt;Next, I calculate the &lt;a href="https://en.wikipedia.org/wiki/F-score"&gt;F1 Score&lt;/a&gt; by comparing the predictions with their actual labels (&lt;code&gt;y_train&lt;/code&gt; series). As the text in the linked page states, &amp;#8220;it is calculated from the precision and recall of the test, where the precision is the number of correctly identified positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of correctly identified positive results divided by the number of all samples that should have been identified as positive&amp;#8221;. Recall is also named true positive rate (&lt;span class="caps"&gt;TPR&lt;/span&gt;) and sensitivity. The F1 Score is the harmonic mean of the precision and recall. This calculation is performed by &lt;code&gt;cross_val_score()&lt;/code&gt; method from &lt;code&gt;sklearn.model_selection&lt;/code&gt; module for each comparison among the folds. Then, the final result is an average of all measurements, which is stored at the &lt;code&gt;s&lt;/code&gt; object. Finally, the loop ends printing the mean F1 Score and its standard deviation (&lt;span class="caps"&gt;STD&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Why I chose the F1 Score? Because there is class imbalance in the data: there is much more control than cases in the dataset. Thus, scores such as accuracy and precision may be &lt;a href="https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28"&gt;misleading when considered alone&lt;/a&gt;. The &lt;a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity"&gt;Wikipedia article about sensitivity and specificity&lt;/a&gt; is a great summary of these&amp;nbsp;concepts.&lt;/p&gt;
&lt;p&gt;After a while, the output will be printed to your console. It will be something like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;DummyClassifier         F1 Score: 0.248 STD: 0.16
LogisticRegression      F1 Score: 0.578 STD: 0.28
DecisionTreeClassifier  F1 Score: 0.291 STD: 0.19
KNeighborsClassifier    F1 Score: 0.069 STD: 0.14
GaussianNB              F1 Score: 0.399 STD: 0.27
SVC                     F1 Score: 0.165 STD: 0.31
RandomForestClassifier  F1 Score: 0.337 STD: 0.26
XGBClassifier           F1 Score: 0.370 STD: 0.34
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The dataset folds will be identical, but you may have different values in some of these models due to stochasticity during calculation. Nevertheless, the values should be close enough. It seems that &lt;code&gt;LogisticRegression&lt;/code&gt; had the best performance among the models, with a rounded up F1 Score = 0.58. So I will continue with this model and try to improve the F1 Score by optimizing (tuning) the&amp;nbsp;model.&lt;/p&gt;
&lt;h2&gt;Model&amp;nbsp;optimization&lt;/h2&gt;
&lt;p&gt;I will now setup the model and produce a grid of &lt;strong&gt;hyperparameters&lt;/strong&gt;. A hyperparameter is a parameter of the model whose value affects the learning/classification process. A grid is therefore a collection of some of those hyperparameters that I will give to the model so it can choose the best candidates. I now set up the&amp;nbsp;model:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;estimator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And this is the&amp;nbsp;grid:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;penalty&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;l1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;l2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;elasticnet&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="s2"&gt;&amp;quot;dual&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="s2"&gt;&amp;quot;C&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that each model has different hyperparameters; those above are logistic regression-exclusive. Check the &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression"&gt;sklearn.linear_model.LogisticRegression module documentation&lt;/a&gt; to know more about these&amp;nbsp;hyperparameters.&lt;/p&gt;
&lt;p&gt;Now, I pass the model (assigned to &lt;code&gt;estimator&lt;/code&gt; object) to the &lt;code&gt;GridSearchCV&lt;/code&gt; function so that it returns the optimal parameters during fitting to the train data with 10-fold&amp;nbsp;cross-validation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;logreg_cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;SPLITS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;logreg_cv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let&amp;#8217;s see a summary of the&amp;nbsp;candidates:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tuned hyperparameters :(best parameters) &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;logreg_cv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_params_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output will&amp;nbsp;be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;tuned hyperparameters :(best parameters)  {&amp;#39;C&amp;#39;: 100.0, &amp;#39;dual&amp;#39;: False, &amp;#39;penalty&amp;#39;: &amp;#39;l2&amp;#39;}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I pass the best parameters as keywords to the optimized model, which I will assign to &lt;code&gt;logreg&lt;/code&gt; object, and then fit the training data once&amp;nbsp;again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;logreg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;logreg_cv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_params_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;logreg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With the fitted model, I can now generate predictions with the test&amp;nbsp;dataset:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logreg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;predicted_proba&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logreg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Evaluate model&amp;nbsp;performance&lt;/h2&gt;
&lt;p&gt;Let&amp;#8217;s compare the predict labels with actual classification using a confusion&amp;nbsp;matrix:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;conf_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Prediction: controls&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Prediction: cases&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Actual: controls&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Actual: cases&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf_matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output will&amp;nbsp;be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#                  Actual: controls  Actual: cases
Prediction: controls                47             11
Prediction: cases                    8              5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now remember the labels summary I got in &lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-1-making-features-datasets.html"&gt;Part 1&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Training Set has 38 Positive Labels (cases) and 127 Negative Labels (controls)
Test Set has 16 Positive Labels (cases) and 55 Negative Labels (controls)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Thus, the model erroneously classified 11 (of 16) cases as controls (false negatives) and correctly classified 5 (of 16) cases as cases (true positives). If this model were to be used in a real world scenario, this would be a problem, because the model would miss people at risk of disease&amp;nbsp;progression.&lt;/p&gt;
&lt;p&gt;Imagine that detecting prostate cancer disease progression risk will trigger further analysis (gather second opinion, ask the patients more examinations etc.) whereas if you don&amp;#8217;t detect this risk, the patient would be sent home with the disease actively progressing. In this situation, therefore, I would prefer false positives (type I error) over false negatives (type &lt;span class="caps"&gt;II&lt;/span&gt;&amp;nbsp;error).&lt;/p&gt;
&lt;p&gt;Please note that there is not a single response for every classification problem; the researcher must evaluate the consequences of the errors and make a decision, since reducing one type of error means increasing the other type of error. In every situation, one type of error is more preferable than the other one &amp;#8212; it is always a&amp;nbsp;trade-off.&lt;/p&gt;
&lt;p&gt;But why the model incorrectly classified some patients? One of the factors is because logistic regression models calculate probabilities to make these predictions. If the predicted probability from a sample is &amp;gt;= 0.50, it labels the sample as case, otherwise, it labels as a control. I can change this probability cutoff to try to reduce the number of false negatives, especially since there is class imbalance in the&amp;nbsp;dataset.&lt;/p&gt;
&lt;p&gt;Using the data in this confusion matrix, I can calculate and print some&amp;nbsp;metrics:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Accuracy: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Precision: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Recall: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;F1 Score: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;f1_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The&amp;nbsp;output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Accuracy: 0.732
Precision: 0.385
Recall: 0.312
F1 Score: 0.345
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I can also print a receiver operating characteristic (&lt;span class="caps"&gt;ROC&lt;/span&gt;) curve. As stated in the &lt;a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic"&gt;Wikipedia article&lt;/a&gt;, it &amp;#8220;illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied&amp;#8221;. The area under the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve (&lt;span class="caps"&gt;AUC&lt;/span&gt;) summarizes the predictive power of the&amp;nbsp;model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;false_pos_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;true_pos_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;proba&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;roc_curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicted_proba&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# plot random curve&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;false_pos_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;true_pos_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;marker&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;AUC = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;roc_auc_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicted_proba&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ROC Curve&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;True Positive Rate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;False Positive Rate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lower right&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The&amp;nbsp;output:&lt;/p&gt;
&lt;p&gt;&lt;img alt="ROC curve from logistic regression model classifying TCGA prostate cancer dataset" src="https://antoniocampos13.github.io/images/logreg_roc.png"&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class="caps"&gt;AUC&lt;/span&gt; is equal to the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative one. It ranges from 0 to 1 (perfect classifier). A random, thus useless, classifier, has a &lt;span class="caps"&gt;AUC&lt;/span&gt; = 0.5. Since the &lt;span class="caps"&gt;AUC&lt;/span&gt; of my model is about 0.65, means that it has some predictive power, although not perfect. I cannot improve the &lt;span class="caps"&gt;AUC&lt;/span&gt;, but I can change the classification probability threshold (as I mentioned above) to try to better utilize the potential of the&amp;nbsp;model.&lt;/p&gt;
&lt;h2&gt;Change classification&amp;nbsp;threshold&lt;/h2&gt;
&lt;p&gt;Looking at the metrics I printed above, I can see the model has fairly low sensitivity (recall) as well as low F1 score. Since there is class imbalance, I will change the classification threshold based on F1&amp;nbsp;score.&lt;/p&gt;
&lt;p&gt;I now calculate the range of F1 scores with several classification&amp;nbsp;thresholds:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;precision&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;recall&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;thresholds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;precision_recall_curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicted_proba&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;f1_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;recall&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;precision&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;recall&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;precision&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;precision_recall_curve()&lt;/code&gt; function will select a classification threshold (&lt;code&gt;thresholds&lt;/code&gt; object), label the samples according to their predicted probabilities given by the model (&lt;code&gt;predicted_proba[:, -1]&lt;/code&gt;) and compare with the actual labels (&lt;code&gt;y_test&lt;/code&gt;), calculating the precision and recall (&lt;code&gt;precision&lt;/code&gt; and &lt;code&gt;recall&lt;/code&gt; objects).&lt;/p&gt;
&lt;p&gt;All three objects are &lt;code&gt;NumPy&lt;/code&gt; arrays. Thus, I can obtain the probability cutoff value associated with maximum F1 score, and create a list of prediction labels and assign to &lt;code&gt;f1_predictions&lt;/code&gt; object using a list comprehension with a &lt;code&gt;if else&lt;/code&gt; statement:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;optimal_proba_cutoff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;thresholds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f1_scores&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="n"&gt;f1_predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;optimal_proba_cutoff&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;predicted_proba&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let&amp;#8217;s examine the confusion matrix using this new&amp;nbsp;cutoff:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;conf_matrix_th&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f1_predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Prediction: controls&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Prediction: cases&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Actual: controls&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Actual: cases&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf_matrix_th&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The&amp;nbsp;output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#                  Actual: controls  Actual: cases
Prediction: controls                34              4
Prediction: cases                   21             12
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I can see that I reduced the number of false negatives at the cost of having more false positives, as discussed above. Now the model erroneously classified just 4 cases as controls, compared to 11 before thresholding. Let&amp;#8217;s calculate the model&amp;#8217;s metrics&amp;nbsp;again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Accuracy Before: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; --&amp;gt; Now: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f1_predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Precision Before: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; --&amp;gt; Now: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f1_predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Recall Before: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; --&amp;gt; Now: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f1_predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;F1 Score Before: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;f1_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; --&amp;gt; Now: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;f1_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f1_predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The&amp;nbsp;output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Accuracy Before: 0.732 --&amp;gt; Now: 0.648
Precision Before: 0.385 --&amp;gt; Now: 0.364
Recall Before: 0.312 --&amp;gt; Now: 0.750
F1 Score Before: 0.345 --&amp;gt; Now: 0.490
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I see that the sensitivity (recall) improved greatly, with a consequential F1 score improvement. With the current data, this is the maximum improvement I can achieve. If this model would to be used in a real world scenario, I would have to gather more information to try to further improve the classification power of the model, always assessing the optimal probability cutoff to account for class&amp;nbsp;imbalance.&lt;/p&gt;
&lt;h2&gt;Save (Serialize)&amp;nbsp;model&lt;/h2&gt;
&lt;p&gt;I can save the model to disk as a pickled object to use in the future or share with someone. Remember that the same modules must be installed and loaded in the system that will receive the pickled model so it can be unpickled and work correctly. See below the commands to save the model and the chosen probability cutoff for&amp;nbsp;classification:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;pickle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logreg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;models&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;logreg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;wb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;pickle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimal_proba_cutoff&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;models&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;logreg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;wb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note: pickled objects are Python-specific only &amp;#8212; non-Python programs may not be able to reconstruct pickled Python objects. &lt;span class="caps"&gt;WARNING&lt;/span&gt;: never, &lt;span class="caps"&gt;NEVER&lt;/span&gt;, unpickle data you do not trust. As it says in the Python documentation: &lt;a href="https://docs.python.org/3/library/pickle.html"&gt;&amp;#8220;It is possible to construct malicious pickle data which will execute arbitrary code during unpickling&amp;#8221;&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Below is a schematics of my working directory. Check it on &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2020_11_05_Supervised_Machine_Leaning_TCGA_Data/data/interim"&gt;my porfolio&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ interim
â”‚   â”‚   â””â”€â”€ prostate_cancer_dataset.csv
â”‚   â””â”€â”€ processed
â”‚       â”œâ”€â”€ X_train
â”‚       â”œâ”€â”€ X_test
â”‚       â”œâ”€â”€ y_train
â”‚       â””â”€â”€ y_test
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ logreg
â”‚   â””â”€â”€ optimal_proba_cutoff
â””â”€â”€ src
    â”œâ”€â”€ features
    â”‚   â””â”€â”€ make_features.py
    â””â”€â”€ models
        â””â”€â”€ make_model.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this part I demonstrated how&amp;nbsp;to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Import pickled datasets to be used as training and test data for the&amp;nbsp;model;&lt;/li&gt;
&lt;li&gt;Check metrics among selected models to support model&amp;nbsp;choice;&lt;/li&gt;
&lt;li&gt;Optimize (tune) model hyperparameters via grid&amp;nbsp;search;&lt;/li&gt;
&lt;li&gt;Evaluate model performance via confusion matrix, metrics and &lt;span class="caps"&gt;ROC&lt;/span&gt; &lt;span class="caps"&gt;AUC&lt;/span&gt;&amp;nbsp;plotting;&lt;/li&gt;
&lt;li&gt;Select a classification&amp;nbsp;cutoff;&lt;/li&gt;
&lt;li&gt;Save the model to disk for backup and future&amp;nbsp;use.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Go back to &lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-1-making-features-datasets.html"&gt;Part 1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga"&gt;The Cancer Genome Atlas&amp;nbsp;Program&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics&amp;nbsp;Cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://scikit-learn.org/stable/"&gt;scikit-learn: machine learning in Python - scikit-learn 0.23.2&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hairysun.com/"&gt;Matt Harrison&amp;#8217;s&amp;nbsp;Blog&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/F-score"&gt;F-score -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28"&gt;Handling Imbalanced Datasets in Machine&amp;nbsp;Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity"&gt;Sensitivity and specificity -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression"&gt;sklearn.linear_model.LogisticRegression - scikit-learn 0.23.2&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic"&gt;Receiver operating characteristic -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/pickle.html"&gt;pickle â€” Python object serialization | Python 3.9.0&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;</content><category term="Python"></category><category term="Bioinformatics"></category><category term="gene expression"></category><category term="machine learning"></category><category term="supervised classification"></category></entry></feed>