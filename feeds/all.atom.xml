<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Antonio's Portfolio</title><link href="https://antoniocampos13.github.io/" rel="alternate"></link><link href="https://antoniocampos13.github.io/feeds/all.atom.xml" rel="self"></link><id>https://antoniocampos13.github.io/</id><updated>2020-10-12T12:42:00-03:00</updated><subtitle>PhD in Genetics</subtitle><entry><title>Working with Cancer Genomics Cloud datasets in a PostgreSQL database (Part 1)</title><link href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html" rel="alternate"></link><published>2020-10-12T12:42:00-03:00</published><updated>2020-10-12T12:42:00-03:00</updated><author><name>Antonio Victor Campos Coelho</name></author><id>tag:antoniocampos13.github.io,2020-10-12:/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently I have been looking for publicly-available genomics datasets to test machine learning models in Python. During my searches for such a &amp;#8220;toy dataset&amp;#8221;, I came upon the &lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics Cloud (&lt;span class="caps"&gt;CGC&lt;/span&gt;)&lt;/a&gt;&amp;nbsp;initiative.&lt;/p&gt;
&lt;p&gt;Anyone can register in &lt;span class="caps"&gt;CGC&lt;/span&gt; and have access to open access massive public datasets, like &lt;a href="http://cancergenome.nih.gov/"&gt;The …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently I have been looking for publicly-available genomics datasets to test machine learning models in Python. During my searches for such a &amp;#8220;toy dataset&amp;#8221;, I came upon the &lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics Cloud (&lt;span class="caps"&gt;CGC&lt;/span&gt;)&lt;/a&gt;&amp;nbsp;initiative.&lt;/p&gt;
&lt;p&gt;Anyone can register in &lt;span class="caps"&gt;CGC&lt;/span&gt; and have access to open access massive public datasets, like &lt;a href="http://cancergenome.nih.gov/"&gt;The Cancer Genomics Atlas (&lt;span class="caps"&gt;TCGA&lt;/span&gt;)&lt;/a&gt;. Most individual-level genomic data can only be accessed following approval of a Data Access Request through the &lt;a href="https://www.ncbi.nlm.nih.gov/gap/"&gt;Database of Genotypes and Phenotypes (dbGaP)&lt;/a&gt;. For now, I guess the open data tier will suffice for this&amp;nbsp;exercise.&lt;/p&gt;
&lt;p&gt;This demonstration will be separated into two parts. Here in the first part I will provide a brief run-down of how I queried the &lt;span class="caps"&gt;CGC&lt;/span&gt; to obtain genomic data from cancer patients and the first steps into preparing a local PostgreSQL relational database in my&amp;nbsp;computer.&lt;/p&gt;
&lt;h2&gt;Querying the &lt;span class="caps"&gt;CGC&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;I registered at &lt;span class="caps"&gt;CGC&lt;/span&gt;, then I created a project in the &lt;span class="caps"&gt;CGC&lt;/span&gt; dashboard and went into the data browser&amp;nbsp;tool:&lt;/p&gt;
&lt;p&gt;&lt;img alt="CGC dashboard options" src="https://antoniocampos13.github.io/images/cgc_1.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Then, I chose the &lt;span class="caps"&gt;TCGA&lt;/span&gt; GRCh38 dataset and clicked on the &lt;code&gt;Explore selected&lt;/code&gt; button.&lt;/p&gt;
&lt;p&gt;&lt;img alt="TCGA dataset" src="https://antoniocampos13.github.io/images/cgc_2.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Inside the data browser, I see that there are several information&amp;nbsp;entities:&lt;/p&gt;
&lt;p&gt;&lt;img alt="TCGA information entities" src="https://antoniocampos13.github.io/images/cgc_3.PNG"&gt;&lt;/p&gt;
&lt;p&gt;I clicked on the first one, &lt;code&gt;Cases&lt;/code&gt; and then created a query with the following entities and&amp;nbsp;filters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Entity&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Filters&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Case&lt;ul&gt;
&lt;li&gt;Primary site: Prostate&amp;nbsp;Gland&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Diagnosis&lt;ul&gt;
&lt;li&gt;Age at&amp;nbsp;diagnosis&lt;/li&gt;
&lt;li&gt;Clinical T (&lt;span class="caps"&gt;TNM&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Demographic&lt;ul&gt;
&lt;li&gt;Ethnicity&lt;/li&gt;
&lt;li&gt;Race&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Follow up&lt;ul&gt;
&lt;li&gt;Primary therapy&amp;nbsp;outcome&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;File&lt;ul&gt;
&lt;li&gt;Access level:&amp;nbsp;Open&lt;/li&gt;
&lt;li&gt;Data type: Gene Level Copy Number, Gene Expression Quantification, Gene Level Copy Number&amp;nbsp;Scores&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final query ended up like&amp;nbsp;this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Prostate cancer query" src="https://antoniocampos13.github.io/images/cgc_4.PNG"&gt;&lt;/p&gt;
&lt;p&gt;In other words, the query resulted in individuals diagnosed with prostate cancer (n=237), their age at diagnosis, their demographic characteristics, their therapeutic outcomes, and their genomic data (n=1,065 files overall: 276 with raw counts of gene expression quantification, 552 with &lt;a href="https://rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/"&gt;&lt;span class="caps"&gt;FPKM&lt;/span&gt;&lt;/a&gt; information, 236 from &lt;a href="https://www.nature.com/scitable/topicpage/copy-number-variation-445/"&gt;copy number variation&lt;/a&gt; genotyping, and a single file containing what I believe is a prostate cancer diagnosis score stratified by&amp;nbsp;gene).&lt;/p&gt;
&lt;p&gt;Then, I clicked on the &lt;code&gt;Copy files to project&lt;/code&gt; and on the &lt;code&gt;Export&lt;/code&gt; button and chose &lt;code&gt;Export as TSV&lt;/code&gt; option. I went back to my project dashboard, clicked on the &lt;code&gt;Files&lt;/code&gt; tab and downloaded&amp;nbsp;everything.&lt;/p&gt;
&lt;p&gt;I realized that the four &lt;code&gt;TSV&lt;/code&gt; and the genomic data could be organized as tables on a &lt;a href="https://en.wikipedia.org/wiki/Relational_database"&gt;relational database&lt;/a&gt;. So I used my &lt;a href="https://www.postgresql.org/"&gt;PostgreSQL server&lt;/a&gt; that I have installed on computer. For this demonstration, I will use my Windows 10 &lt;span class="caps"&gt;OS&lt;/span&gt;, but PostgreSQL can be installed on Unix systems as well. In my portfolio I provide a Windows script and a Unix script as well containing the steps I followed to load all the data into a PostgreSQL&amp;nbsp;database.&lt;/p&gt;
&lt;h2&gt;Creating the &amp;#8216;tcga&amp;#8217; database into the local PostgreSQL&amp;nbsp;server&lt;/h2&gt;
&lt;p&gt;The official PostgreSQL installation instructions are &lt;a href="https://www.postgresql.org/download/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I created a folder named &lt;code&gt;TCGA&lt;/code&gt; for this project, and put the downloaded files inside a &lt;code&gt;data&lt;/code&gt; subfolder. Here is a representation of my directory&amp;nbsp;structure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
└── TCGA
    ├── data
    │   ├── counts
    │   │   └── &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;200&lt;/span&gt;+ *.counts.gz files&lt;span class="o"&gt;]&lt;/span&gt;
    │   ├── focal_score_by_genes
    │   ├── fpkm
    │   ├── gene_level_copy_numbers
    │   ├── cases.tsv
    │   ├── demographic.tsv
    │   ├── files.tsv
    │   └── follow_up.tsv
    ├── main_tcga.ps1
    └── main_tcga.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;TSV&lt;/code&gt; files are the query results and inside the folders are the files containing the genomic data (these are not their original names &amp;#8212; I renamed them to make easier to identify the contents of each one). For now, I will use just the four &lt;code&gt;TSV&lt;/code&gt; files and the &lt;code&gt;counts&lt;/code&gt; folders.&lt;/p&gt;
&lt;p&gt;This structure is replicated in the folder corresponding to this post in my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/SQL/2020_10_12_Working_Data_CGC_PostgreSQL/TCGA"&gt;portfolio&lt;/a&gt;. The &lt;code&gt;main_tcga.ps1&lt;/code&gt; and &lt;code&gt;main_tcga.sh&lt;/code&gt; files contain the commands I used for this demonstration. The first is for Windows and the second for Unix&amp;nbsp;systems.&lt;/p&gt;
&lt;p&gt;Then, in the &lt;code&gt;TCGA&lt;/code&gt; folder I opened a Windows PowerShell terminal and using &lt;code&gt;psql&lt;/code&gt;, a terminal-based front-end to PostgreSQL, created a database named &lt;code&gt;tcga&lt;/code&gt; on my local&amp;nbsp;server:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;CREATE DATABASE tcga ENCODING &amp;#39;UTF-8&amp;#39; LC_COLLATE &amp;#39;English_United States&amp;#39; LC_CTYPE &amp;#39;English_United States&amp;#39; TEMPLATE template0&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;-U&lt;/code&gt; flag serves to indicate which user will connect to the local PostgreSQL server. &lt;code&gt;postgres&lt;/code&gt; is the default user created during PostgreSQL installation. The &lt;code&gt;-c&lt;/code&gt; flag means that we are sending a command to the server. Note that the command is inside double quotes and strings into the command are single-quoted.In summary, this command serves to connect the &lt;code&gt;postgres&lt;/code&gt; user into the server and pass a command to create the &lt;code&gt;tcga&lt;/code&gt; database with certain characteristics: use &lt;span class="caps"&gt;UTF&lt;/span&gt;-8 codification, with English locale using the &lt;code&gt;template0&lt;/code&gt; database as template, which is created by default during PostgreSQL server&amp;nbsp;installation.&lt;/p&gt;
&lt;p&gt;If during installation you provided a password to access the server, the terminal will ask for it after you press&amp;nbsp;Enter.&lt;/p&gt;
&lt;h2&gt;Creating tables in the &amp;#8216;tcga&amp;#8217;&amp;nbsp;database&lt;/h2&gt;
&lt;p&gt;Then, I created four tables, corresponding to each &lt;code&gt;TSV&lt;/code&gt; files with the following&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-a&lt;/span&gt; &lt;span class="o"&gt;-f&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;src/tcga_create_tables.sql&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The new friends here are &lt;code&gt;-d&lt;/code&gt; and &lt;code&gt;-a -f&lt;/code&gt;. &lt;code&gt;-d&lt;/code&gt; is the flag that indicates the &lt;em&gt;database&lt;/em&gt; I wished to connect; it is the &lt;code&gt;tcga&lt;/code&gt; I created above. The &lt;code&gt;-a&lt;/code&gt; serves to echo all information from the command to the terminal output so it is possible to check if the commands worked. The &lt;code&gt;-f&lt;/code&gt; flag mean &lt;em&gt;file&lt;/em&gt;: I am indicating that I want to pass the commands within the &lt;code&gt;tcga_create_tables.sql&lt;/code&gt; file inside the &lt;code&gt;src&lt;/code&gt; directory &amp;#8212; which I created as a subfolder of the &lt;code&gt;TCGA&lt;/code&gt; folder. If you are wondering how I created this file: wrote the commands in a text file and simply saved it with the &lt;code&gt;.sql&lt;/code&gt; extension.&lt;/p&gt;
&lt;p&gt;Below is one of the commands inside the &lt;code&gt;.sql&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt; &lt;span class="k"&gt;TABLE&lt;/span&gt; &lt;span class="n"&gt;allcases&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;case_id&lt;/span&gt; &lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;case_primarysite&lt;/span&gt; &lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;diagnosis&lt;/span&gt; &lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;diagnosis_ageatdiagnosis_1&lt;/span&gt; &lt;span class="nb"&gt;INT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;diagnosis_clinicalt_1&lt;/span&gt; &lt;span class="nb"&gt;TEXT&lt;/span&gt;
&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The command above creates the table &lt;code&gt;allcases&lt;/code&gt; with five columns: &lt;code&gt;case_id&lt;/code&gt;, &lt;code&gt;case_primarysite&lt;/code&gt;, &lt;code&gt;diagnosis&lt;/code&gt;, &lt;code&gt;diagnosis_ageatdiagnosis_1&lt;/code&gt;, and &lt;code&gt;diagnosis_clinicalt_1&lt;/code&gt;. Notice the words beside each one: they indicate the &lt;strong&gt;data type&lt;/strong&gt; of the data that the column will hold. In this case I have four columns that will get text data (&lt;code&gt;TEXT&lt;/code&gt;) and one that will get numbers &amp;#8212; integers (&lt;code&gt;INT&lt;/code&gt;)&amp;nbsp;specifically.&lt;/p&gt;
&lt;p&gt;Note the semicolon &lt;code&gt;;&lt;/code&gt; at the end &amp;#8212; it is a PostgreSQL requirement. It indicates the end of a command (however, if we are passing arguments through the &lt;code&gt;-c&lt;/code&gt; flag the semicolon is not needed though, it is implicit within the&amp;nbsp;flag).&lt;/p&gt;
&lt;p&gt;The file have three more commands similar to the one above. The output of the second to last command should be &lt;code&gt;CREATE TABLE&lt;/code&gt; messages, meaning all went well &amp;#8212; I created tour tables inside the &lt;code&gt;tcga&lt;/code&gt; database.&lt;/p&gt;
&lt;h2&gt;Populating the&amp;nbsp;tables&lt;/h2&gt;
&lt;p&gt;However, they are still empty. To populate the tables, I used the four commands below, one for each table (&lt;code&gt;allcases&lt;/code&gt;, &lt;code&gt;demographic&lt;/code&gt;, &lt;code&gt;follow_up&lt;/code&gt; and &lt;code&gt;allfiles&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;\COPY allcases FROM &amp;#39;data/cases.tsv&amp;#39; DELIMITER E&amp;#39;\t&amp;#39; CSV HEADER&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;\COPY demographic FROM &amp;#39;data/demographic.tsv&amp;#39; DELIMITER E&amp;#39;\t&amp;#39; CSV HEADER&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;\COPY follow_up FROM &amp;#39;data/follow_up.tsv&amp;#39; DELIMITER E&amp;#39;\t&amp;#39; CSV HEADER&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;\COPY allfiles FROM &amp;#39;data/files.tsv&amp;#39; DELIMITER E&amp;#39;\t&amp;#39; CSV HEADER&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(It is good practice to separate table-creating commands of table-populating ones). In summary, the commands tell the PostgreSQL server to copy the information contained in the &lt;code&gt;TSV&lt;/code&gt; files inside the &lt;code&gt;data&lt;/code&gt; directory into the specified&amp;nbsp;table.&lt;/p&gt;
&lt;p&gt;The argument &lt;code&gt;DELIMITER E'\t'&lt;/code&gt; means that the columns are tab-separated (delimited). This argument would be &lt;code&gt;DELIMITER ','&lt;/code&gt; if the file were comma-separated or omitted&amp;nbsp;altogether.  &lt;/p&gt;
&lt;p&gt;The &lt;code&gt;CSV&lt;/code&gt; indicates that we are importing a delimiter-separated file. &lt;code&gt;HEADER&lt;/code&gt; means that the copied file have a header &amp;#8212; the first line have the column titles, which &lt;strong&gt;must be equal&lt;/strong&gt; to the ones specified during table creation; an error will occur otherwise. This argument must be omitted if the file does not have a&amp;nbsp;header.&lt;/p&gt;
&lt;p&gt;The output &lt;code&gt;COPY&lt;/code&gt; followed by an integer (representing the number of rows copied) means that everything went well. Be careful: do not run the copy commands more than once, otherwise data duplication will&amp;nbsp;occur.&lt;/p&gt;
&lt;p&gt;With this I conclude the first part of this demonstration. In the next part I will use I will use a customized Python to help with the import of genomic data into the PostgreSQL&amp;nbsp;database.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-2.html"&gt;Go to Part 2&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion of Part&amp;nbsp;1&lt;/h2&gt;
&lt;p&gt;In this part&amp;nbsp;I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demonstrated how to query open access data in &lt;span class="caps"&gt;CGC&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;Showed basic commands for importing data into tables created in a local PostgreSQL&amp;nbsp;database.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics&amp;nbsp;Cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://cancergenome.nih.gov/"&gt;The Cancer Genome Atlas&amp;nbsp;Program&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/gap/"&gt;Home - dbGaP - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/"&gt;&lt;span class="caps"&gt;RPKM&lt;/span&gt;, &lt;span class="caps"&gt;FPKM&lt;/span&gt; and &lt;span class="caps"&gt;TPM&lt;/span&gt;, clearly&amp;nbsp;explained&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.nature.com/scitable/topicpage/copy-number-variation-445/"&gt;Copy Number Variation | Scitable by Nature&amp;nbsp;Education&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Relational_database"&gt;Relational database -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.postgresql.org/"&gt;PostgreSQL: The world&amp;#8217;s most advanced open source&amp;nbsp;database&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.postgresql.org/download/"&gt;PostgreSQL:&amp;nbsp;Downloads&lt;/a&gt;&lt;/p&gt;</content><category term="SQL"></category><category term="Bioinformatics"></category><category term="gene expression quantification"></category><category term="copy number variation"></category><category term="Windows"></category></entry><entry><title>Working with Cancer Genomics Cloud datasets in a PostgreSQL database (Part 2)</title><link href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-2.html" rel="alternate"></link><published>2020-10-12T12:42:00-03:00</published><updated>2020-10-12T12:42:00-03:00</updated><author><name>Antonio Victor Campos Coelho</name></author><id>tag:antoniocampos13.github.io,2020-10-12:/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-2.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Recently I have been looking for publicly-available genomics datasets to test machine learning models in Python. During my searches for such a &amp;#8220;toy dataset&amp;#8221;, I came upon the &lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics Cloud (&lt;span class="caps"&gt;CGC&lt;/span&gt;)&lt;/a&gt;&amp;nbsp;initiative.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Anyone can register in &lt;span class="caps"&gt;CGC&lt;/span&gt; and have access to open access massive public datasets, like &lt;a href="http://cancergenome.nih.gov/"&gt;The …&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Recently I have been looking for publicly-available genomics datasets to test machine learning models in Python. During my searches for such a &amp;#8220;toy dataset&amp;#8221;, I came upon the &lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics Cloud (&lt;span class="caps"&gt;CGC&lt;/span&gt;)&lt;/a&gt;&amp;nbsp;initiative.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Anyone can register in &lt;span class="caps"&gt;CGC&lt;/span&gt; and have access to open access massive public datasets, like &lt;a href="http://cancergenome.nih.gov/"&gt;The Cancer Genomics Atlas (&lt;span class="caps"&gt;TCGA&lt;/span&gt;)&lt;/a&gt;. Most individual-level genomic data can only be accessed following approval of a Data Access Request through the &lt;a href="https://www.ncbi.nlm.nih.gov/gap/"&gt;Database of Genotypes and Phenotypes (dbGaP)&lt;/a&gt;. For now, I guess the open data tier will suffice for this&amp;nbsp;exercise.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This demonstration will be separated into two parts. In the &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html"&gt;first part&lt;/a&gt; I provided a brief run-down of how I queried the &lt;span class="caps"&gt;CGC&lt;/span&gt; to obtain genomic data from cancer patients and the first steps into preparing a local PostgreSQL relational database in my&amp;nbsp;computer.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here in the second part I will use a customized Python to help with the import of genomic data into the PostgreSQL&amp;nbsp;database.&lt;/p&gt;
&lt;h2&gt;Why use Python to import the genomic data into the PostgreSQL&amp;nbsp;database&lt;/h2&gt;
&lt;p&gt;In the first part of this demonstration I mentioned that I got more than 200 files containing the raw counts of gene expression in the prostate cancer individuals, each corresponding to a individual with prostate gland cancer. Unfortunately, the counts files do not have the patient identification. This information is only available in the &lt;code&gt;files.tsv&lt;/code&gt; (and in my &lt;code&gt;allfiles&lt;/code&gt; table in the database consequently), which indicates which count file belongs to each patient. Therefore, I must include the count file name alongside the gene&amp;nbsp;counts.&lt;/p&gt;
&lt;p&gt;Below I have an illustration of the problem. I have two files, count_A and&amp;nbsp;count_B:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# count_A&lt;/span&gt;
ENSG00000000003.13 &lt;span class="m"&gt;4000&lt;/span&gt;
ENSG00000000005.5 &lt;span class="m"&gt;5&lt;/span&gt;
ENSG00000000419.11 &lt;span class="m"&gt;1800&lt;/span&gt;

&lt;span class="c1"&gt;# count_B&lt;/span&gt;
ENSG00000000003.13 &lt;span class="m"&gt;3000&lt;/span&gt;
ENSG00000000005.5 &lt;span class="m"&gt;25&lt;/span&gt;
ENSG00000000419.11 &lt;span class="m"&gt;500&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this state, I cannot know which patients provided the samples that generate count_A and count_B. But if I add a new column with the&amp;nbsp;filename:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# count_A&lt;/span&gt;
ENSG00000000003.13 &lt;span class="m"&gt;4000&lt;/span&gt;    count_A
ENSG00000000005.5 &lt;span class="m"&gt;5&lt;/span&gt;   count_A
ENSG00000000419.11 &lt;span class="m"&gt;1800&lt;/span&gt;    count_A

&lt;span class="c1"&gt;# count_B&lt;/span&gt;
ENSG00000000003.13 &lt;span class="m"&gt;3000&lt;/span&gt;    count_B
ENSG00000000005.5 &lt;span class="m"&gt;25&lt;/span&gt;  count_B
ENSG00000000419.11 &lt;span class="m"&gt;500&lt;/span&gt; count_B
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I can now cross-reference with the &lt;code&gt;allfiles&lt;/code&gt; table, and identify which file belong to each&amp;nbsp;patient:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;case_id file_name
case0001 count_A
case0002 count_B
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Thus, I created a &lt;strong&gt;relation&lt;/strong&gt; between the gene expression quantification and their patients of origin. Keep in mind that the gene counts file have &lt;strong&gt;thousands&lt;/strong&gt; of rows, each corresponding to one human gene/alternate transcript. Therefore, I&amp;nbsp;must:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Automate the creation of the third column containing the file name in all 200+ gene count&amp;nbsp;files;&lt;/li&gt;
&lt;li&gt;Join the modified files into a single, unified data&amp;nbsp;frame;&lt;/li&gt;
&lt;li&gt;Import the data frame into the &lt;code&gt;tcga&lt;/code&gt; database.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With only programming language &amp;#8212; Python &amp;#8212; I can do all three requirements above. So that&amp;#8217;s why I used Python: it is a very powerful, versatile&amp;nbsp;language!&lt;/p&gt;
&lt;h2&gt;Create Python virtual&amp;nbsp;environment&lt;/h2&gt;
&lt;p&gt;Follow instructions to install Python in Windows &lt;a href="https://www.python.org/downloads/"&gt;here&lt;/a&gt;. Ensure that Python &lt;a href="https://datatofish.com/add-python-to-windows-path/"&gt;is included in your Windows &lt;span class="caps"&gt;PATH&lt;/span&gt;&lt;/a&gt;. Python usually comes pre-installed in several Unix distros and already included in the &lt;span class="caps"&gt;PATH&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;First, I will create a virtual environment to hold the necessary Python modules for my customized Python script. This is good practice &amp;#8212; as I explained in my &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;previous post&lt;/a&gt; different environments isolate programs for different uses, ensuring compatibility. In the post I talked about miniconda, but the principle is the same for Python&amp;nbsp;here.&lt;/p&gt;
&lt;p&gt;Otherwise, you can create a miniconda environment with Python included, and install all Python packages via miniconda channels. Since I will not use any other software besides Python here, there is no need to use miniconda, in my opinion. I created a virtual environment using Python&amp;#8217;s &lt;code&gt;virtualenv&lt;/code&gt; tool. Currently, I am using Python version&amp;nbsp;3.8.&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;TCGA&lt;/code&gt; folder I open a PowerShell and issue the commands&amp;nbsp;below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="n"&gt;-m&lt;/span&gt; &lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="p"&gt;-&lt;/span&gt;&lt;span class="n"&gt;-user&lt;/span&gt; &lt;span class="n"&gt;virtualenv&lt;/span&gt;

&lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="n"&gt;-m&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The first command installs the program &lt;code&gt;virtualenv&lt;/code&gt; (&lt;code&gt;venv&lt;/code&gt;) via the Python package manager &lt;code&gt;pip&lt;/code&gt;. The second command uses &lt;code&gt;venv&lt;/code&gt; to create a virtual environment deposited in a folder named &lt;code&gt;venv&lt;/code&gt; in the current directory (&lt;code&gt;TCGA&lt;/code&gt; folder in this example). You can also provide a complete path like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="n"&gt;-m&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="err"&gt;:&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;Users&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;some_path&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;TCGA&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;venv&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Of course, you can call the virtual environment as you&amp;nbsp;wish.&lt;/p&gt;
&lt;h2&gt;Activate the virtual&amp;nbsp;environment&lt;/h2&gt;
&lt;p&gt;Still in the &lt;code&gt;TCGA&lt;/code&gt; folder, I type the&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;venv&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;Scripts&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;activate&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The virtual environment is ready to be used. I will install the necessary modules for the&amp;nbsp;work.&lt;/p&gt;
&lt;h2&gt;Install Python modules into the virtual&amp;nbsp;environment&lt;/h2&gt;
&lt;p&gt;The modules I will install&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.dask.org/en/latest/why.html"&gt;&lt;code&gt;dask&lt;/code&gt;&lt;/a&gt;: to create the unified data frame with the gene&amp;nbsp;expression;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;psycopg2-binary&lt;/code&gt; and &lt;code&gt;sqlalchemy&lt;/code&gt;: to connect with the PostgreSQL database and push the dataframe into&amp;nbsp;it.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;dask[complete]&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;psycopg2-binary&lt;/span&gt; &lt;span class="n"&gt;sqlalchemy&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The modules will be downloaded from the internet and installed at the &lt;code&gt;venv&lt;/code&gt; folder. Additional dependencies, such as &lt;a href="https://pandas.pydata.org/"&gt;&lt;code&gt;pandas&lt;/code&gt;&lt;/a&gt; (a widely-used data analysis and manipulation tool) and &lt;a href="https://numpy.org/"&gt;&lt;code&gt;NumPy&lt;/code&gt;&lt;/a&gt; (package for scientific computing), used by &lt;code&gt;dask&lt;/code&gt;, will be downloaded as&amp;nbsp;well.&lt;/p&gt;
&lt;h2&gt;Creating Python credentials to access PostgreSQL&amp;nbsp;database&lt;/h2&gt;
&lt;p&gt;To access the &lt;code&gt;tcga&lt;/code&gt; database through Python, we need to configure credentials for the&amp;nbsp;connection.&lt;/p&gt;
&lt;p&gt;In the terminal I&amp;nbsp;type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;CREATE USER &amp;lt;USER_NAME&amp;gt; with encrypted password &amp;#39;&amp;lt;PASSWORD&amp;gt;&amp;#39;&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;GRANT ALL PRIVILEGES ON DATABASE tcga TO &amp;lt;USER_NAME&amp;gt;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;&amp;lt;USER_NAME&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;PASSWORD&amp;gt;&lt;/code&gt; are placeholders for my username and password, respectively, since it is good practice to &lt;strong&gt;&lt;span class="caps"&gt;NEVER&lt;/span&gt; share sensitive information&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Then, I created a file named &lt;code&gt;settings.py&lt;/code&gt; and put it in a &lt;code&gt;src&lt;/code&gt; folder with the following&amp;nbsp;content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;DB_FLAVOR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;postgresql&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;DB_PYTHON_LIBRARY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;psycopg2&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;USER&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;lt;USER_NAME&amp;gt;&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;PASSWORD&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;lt;PASSWORD&amp;gt;&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;DB_HOST&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;localhost&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;PORT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;5432&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;DB_NAME&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;tcga&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Create one yourself with the user name and password you specified on the previous step. The other parameters can be left as they are. The 5432 port is usually the default port configured during installation to connection to PostgreSQL. Change it if needed, of course. &lt;code&gt;localhost&lt;/code&gt; means that the PostgreSQL is running locally in my&amp;nbsp;computer.&lt;/p&gt;
&lt;p&gt;Then, to keep the organization of my folder, I added my &lt;code&gt;tcga_processing_counts.py&lt;/code&gt; customized script to the &lt;code&gt;src&lt;/code&gt; folder. The folder structure is now like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
└── TCGA
    ├── data
    │   ├── counts
    │   │   └── &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;200&lt;/span&gt;+ *.counts.gz files&lt;span class="o"&gt;]&lt;/span&gt;
    │   ├── focal_score_by_genes
    │   ├── fpkm
    │   ├── gene_level_copy_numbers
    │   ├── cases.tsv
    │   ├── demographic.tsv
    │   ├── files.tsv
    │   └── follow_up.tsv
    ├── src
    │   ├── settings.py
    │   └── tcga_processing_counts.py
    ├── main_tcga.ps1
    └── main_tcga.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Running the&amp;nbsp;script&lt;/h2&gt;
&lt;p&gt;Back in the &lt;code&gt;TCGA&lt;/code&gt; folder, I type in the&amp;nbsp;PowerShell:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="n"&gt;src&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;tcga_processing_counts&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This will start the script, which has eight&amp;nbsp;steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Set up PostgreSQL connection object: &lt;code&gt;psycopg2&lt;/code&gt; and &lt;code&gt;sqlalchemy&lt;/code&gt; modules use the credential of the &lt;code&gt;settings.py&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Set up project paths: locate the data&amp;nbsp;folders;&lt;/li&gt;
&lt;li&gt;Decompress the &lt;code&gt;.counts.gz&lt;/code&gt; files;&lt;/li&gt;
&lt;li&gt;Make a list of all uncompressed&amp;nbsp;files;&lt;/li&gt;
&lt;li&gt;Create a function ready to return a pandas.DataFrame: this is when I add the third column with the filename in the counts&amp;nbsp;files;&lt;/li&gt;
&lt;li&gt;Create a list of commands to apply the read_and_label_csv function to all&amp;nbsp;files;&lt;/li&gt;
&lt;li&gt;Using &lt;code&gt;dask&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;delayed&lt;/code&gt; method, assemble the pandas.DataFrames into a &lt;code&gt;dask.DataFrame&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Send the &lt;code&gt;dask.DataFrame&lt;/code&gt; to the&amp;nbsp;database.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There is an optional step before step 8 to export the &lt;code&gt;dask.DataFrame&lt;/code&gt; as &lt;span class="caps"&gt;HUGE&lt;/span&gt; &lt;span class="caps"&gt;CSV&lt;/span&gt; file that I disabled by default. &lt;strong&gt;&lt;span class="caps"&gt;WARNING&lt;/span&gt;: &lt;span class="caps"&gt;IT&lt;/span&gt; &lt;span class="caps"&gt;USES&lt;/span&gt; A &lt;span class="caps"&gt;LOT&lt;/span&gt; &lt;span class="caps"&gt;OF&lt;/span&gt; &lt;span class="caps"&gt;RAM&lt;/span&gt; &lt;span class="caps"&gt;AND&lt;/span&gt; &lt;span class="caps"&gt;CPU&lt;/span&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The use of &lt;code&gt;dask&lt;/code&gt; for this job is crucial. &lt;code&gt;pandas&lt;/code&gt; works by loading all data into the &lt;span class="caps"&gt;RAM&lt;/span&gt;. However, since there are several files of considerable size, it would overload my available &lt;span class="caps"&gt;RAM&lt;/span&gt;. &lt;code&gt;dask&lt;/code&gt; is suited for larger-than-memory datasets, since it operates by lazy evaluation: it break operations into blocks and specifies task chains and execute them only on demand, saving computing&amp;nbsp;resources.&lt;/p&gt;
&lt;p&gt;Go check the contents of my &lt;a href="https://github.com/antoniocampos13/portfolio/blob/master/SQL/2020_10_12_Working_Data_CGC_PostgreSQL/TCGA/src/tcga_processing_counts.py"&gt;&lt;code&gt;tcga_processing_counts.py&lt;/code&gt; in my portfolio&lt;/a&gt;. By default, it will create a table named &lt;code&gt;gene_counts&lt;/code&gt; in the &lt;code&gt;tcga&lt;/code&gt; database. See an excerpt of the final&amp;nbsp;result:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gene counts table in tcga database" src="https://antoniocampos13.github.io/images/tcga_gene_counts.PNG"&gt;&lt;/p&gt;
&lt;h2&gt;Finishing&amp;nbsp;touches&lt;/h2&gt;
&lt;p&gt;With the gene expression counts dataset imported in the database, it is time to create the filename (gene counts)/patient relation as I explained in the beginning of the post. In the terminal again, I&amp;nbsp;type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;CREATE TABLE gene_counts_cases AS SELECT DISTINCT case_id, gene_id, gene_count FROM gene_counts LEFT JOIN allfiles ON gene_counts.filename = allfiles.file_uuid WHERE gene_id LIKE &amp;#39;%ENSG%&amp;#39;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The command above links the two tables by their information in common: the filename of the gene counts, which is named &lt;code&gt;filename&lt;/code&gt; in the gene_counts table and &lt;code&gt;file_uuid&lt;/code&gt; in &lt;code&gt;allfiles&lt;/code&gt; table that we created&amp;nbsp;before.&lt;/p&gt;
&lt;p&gt;See an excerpt of the final&amp;nbsp;result:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gene counts table in tcga database: counts/patient relation" src="https://antoniocampos13.github.io/images/tcga_gene_counts_cases.PNG"&gt;&lt;/p&gt;
&lt;p&gt;With this I conclude the second and last part of this demonstration. There is still missing the outcome information, which is located in the &lt;code&gt;follow_up&lt;/code&gt; table in the database. However, the &lt;code&gt;gene_counts_cases&lt;/code&gt; table is not yet ready to be linked. I need to pivot this table, but PostgreSQL has a limit of 1600 columns. Perhaps if I import this table into a session in &lt;code&gt;R&lt;/code&gt;, it will be possible to transform the table. Additionally, I will perform differential expression analysis for sequence count&amp;nbsp;data.&lt;/p&gt;
&lt;h2&gt;Conclusion of Part&amp;nbsp;2&lt;/h2&gt;
&lt;p&gt;In this part&amp;nbsp;I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demonstrated how Python can be used to create data frames larger-than-memory with &lt;code&gt;dask&lt;/code&gt; module;&lt;/li&gt;
&lt;li&gt;Demonstrated how to connect Python to PostgreSQL databases with &lt;code&gt;psycopg2&lt;/code&gt; and &lt;code&gt;sqlalchemy&lt;/code&gt; modules;&lt;/li&gt;
&lt;li&gt;Demonstrated simple &lt;code&gt;LEFT JOIN&lt;/code&gt; operation to link gene counts to individual cases of prostate&amp;nbsp;cancer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html"&gt;Go back to Part 1&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics&amp;nbsp;Cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://cancergenome.nih.gov/"&gt;The Cancer Genome Atlas&amp;nbsp;Program&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/gap/"&gt;Home - dbGaP - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.python.org/downloads/"&gt;Download&amp;nbsp;Python&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://datatofish.com/add-python-to-windows-path/"&gt;How to add Python to Windows &lt;span class="caps"&gt;PATH&lt;/span&gt; - Data to&amp;nbsp;Fish&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;Setting Up Your Unix Computer for Bioinformatics&amp;nbsp;Analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.dask.org/en/latest/why.html"&gt;Dask  documentation - Why&amp;nbsp;Dask?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pandas.pydata.org/"&gt;pandas - Python Data Analysis&amp;nbsp;Library&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://numpy.org/"&gt;NumPy&lt;/a&gt;&lt;/p&gt;</content><category term="SQL"></category><category term="Bioinformatics"></category><category term="gene expression quantification"></category><category term="copy number variation"></category><category term="Windows"></category></entry><entry><title>FASTQ to Annotation (Part 4)</title><link href="https://antoniocampos13.github.io/fastq-to-annotation-part-4.html" rel="alternate"></link><published>2020-10-06T18:00:00-03:00</published><updated>2020-10-06T18:00:00-03:00</updated><author><name>Antonio Victor Campos Coelho</name></author><id>tag:antoniocampos13.github.io,2020-10-06:/fastq-to-annotation-part-4.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;In a &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;previous post&lt;/a&gt;, I showed how to configure an Ubuntu system to install Bioinformatics&amp;nbsp;programs.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Now, using the environment I created, I will demonstrate a bash script, &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; that takes next generation sequencing (&lt;span class="caps"&gt;NGS&lt;/span&gt;) raw reads from human whole genome sequencing as input and produces variant annotation …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;In a &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;previous post&lt;/a&gt;, I showed how to configure an Ubuntu system to install Bioinformatics&amp;nbsp;programs.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Now, using the environment I created, I will demonstrate a bash script, &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; that takes next generation sequencing (&lt;span class="caps"&gt;NGS&lt;/span&gt;) raw reads from human whole genome sequencing as input and produces variant annotation as output. Variant annotation is the process of identifying genetic variants in some genomic &lt;span class="caps"&gt;DNA&lt;/span&gt; sample, and assess, for example, if any of the found variants have any effect on phenotype, such as increased susceptibility to certain&amp;nbsp;diseases.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In the &lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-1"&gt;first part&lt;/a&gt;, I showed how to search for &lt;span class="caps"&gt;NGS&lt;/span&gt; projects deposited in &lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology Information (&lt;span class="caps"&gt;NCBI&lt;/span&gt;) databases&lt;/a&gt; from which I can download sequencing reads later to use with the&amp;nbsp;script.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In the &lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-2"&gt;second part&lt;/a&gt;, I showed how to retrieve raw genome sequencing reads in the form of &lt;code&gt;FASTQ&lt;/code&gt; files, which are deposited in &lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;&lt;span class="caps"&gt;SRA&lt;/span&gt;&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In the &lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-3"&gt;third part&lt;/a&gt;, I made the final preparations for the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script demonstration using the &lt;code&gt;FASTQ&lt;/code&gt; files obtained in the second&amp;nbsp;part.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here in the fourth and final part, I finally can summarize the inner workings of the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script.&lt;/p&gt;
&lt;h2&gt;FastQ_to_Annotation.sh&amp;nbsp;parameters&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Activate&lt;/strong&gt; your miniconda environment if needed and go to your &lt;code&gt;demo&lt;/code&gt; folder. Make sure you have the &lt;code&gt;FASTQ&lt;/code&gt; files and a &lt;code&gt;refs&lt;/code&gt; folder with the human genome &lt;code&gt;FASTA&lt;/code&gt; files and the other various supporting&amp;nbsp;files.&lt;/p&gt;
&lt;p&gt;The script needs 10 command line parameters to work correctly. They&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mate-pair &lt;span class="caps"&gt;FASTQ&lt;/span&gt; files name root (without extension) (absolute file&amp;nbsp;path)&lt;/li&gt;
&lt;li&gt;Reference genome &lt;span class="caps"&gt;FASTA&lt;/span&gt; (absolute file&amp;nbsp;path)&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;BED&lt;/span&gt; or &lt;span class="caps"&gt;GFF&lt;/span&gt; file (absolute file&amp;nbsp;path)&lt;/li&gt;
&lt;li&gt;Minimum quality for bases at read ends, below which bases will be cut (integer - default:&amp;nbsp;20)&lt;/li&gt;
&lt;li&gt;Minimum allowed read length (integer - default:&amp;nbsp;20)&lt;/li&gt;
&lt;li&gt;Adaptor for trimming off read ends (&amp;#8216;illumina&amp;#8217; / &amp;#8216;nextera&amp;#8217; /&amp;nbsp;&amp;#8216;small_rna&amp;#8217;)&lt;/li&gt;
&lt;li&gt;Minimum read depth for calling a variant (integer - default:&amp;nbsp;3)&lt;/li&gt;
&lt;li&gt;Minimum allowed mapping quality (integer - default:&amp;nbsp;0)&lt;/li&gt;
&lt;li&gt;Stringency for calling variants (&amp;#8216;relaxed&amp;#8217; / &amp;#8216;normal&amp;#8217;) (relaxed uses &amp;#8212;pval-threshold 1.0 with BCFtools&amp;nbsp;call)&lt;/li&gt;
&lt;li&gt;User identification for logging&amp;nbsp;(alphanumeric)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this example, use the following&amp;nbsp;values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;SRR6784104&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;refs/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;refs/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.bed&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;20&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;20&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;illumina&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;normal&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Your name (do not use&amp;nbsp;spaces)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since the names of the compressed human genome &lt;code&gt;FASTA&lt;/code&gt; file is big, you can rename it, or create an alias in the command line to simplify the&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nv"&gt;REF&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;refs/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz
&lt;span class="nv"&gt;BED&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;refs/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.bed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then, joining everything&amp;nbsp;together:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;./FastQ_to_Annotation.sh SRR6784104 &lt;span class="nv"&gt;$REF&lt;/span&gt; &lt;span class="nv"&gt;$BED&lt;/span&gt; &lt;span class="m"&gt;20&lt;/span&gt; &lt;span class="m"&gt;20&lt;/span&gt; illumina &lt;span class="m"&gt;3&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; normal antonio
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Pipeline&amp;nbsp;steps&lt;/h2&gt;
&lt;p&gt;The script will check if all parameters are adequate and then run the core pipeline, which proceeds in an 8-step&amp;nbsp;process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Adaptor and read quality trimming: uses &lt;a href="https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/"&gt;Trim Galore!&lt;/a&gt;, &lt;a href="http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"&gt;FastQC&lt;/a&gt; and &lt;a href="https://github.com/marcelm/cutadapt/"&gt;Cutadapt&lt;/a&gt; programs. They remove adaptor sequence from reads and discards low-quality reads so they do not interfere with the second step, alignment. Outputs the trimmed &lt;code&gt;FASTQ&lt;/code&gt; files, text and &lt;code&gt;HTML&lt;/code&gt; reports of the trimming&amp;nbsp;results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Alignment: uses &lt;code&gt;bwa mem&lt;/code&gt; command (&lt;a href="https://academic.oup.com/bioinformatics/article/25/14/1754/225615"&gt;Li &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Durbin, 2009&lt;/a&gt;). &lt;code&gt;bwa&lt;/code&gt; is a widely-used program to align short reads into genomes, so we can pinpoint where in the genome the identified variants are located. Takes the trimmed &lt;code&gt;FASTQ&lt;/code&gt; files, the reference &lt;code&gt;FASTA&lt;/code&gt; file and produces an aligned &lt;span class="caps"&gt;SAM&lt;/span&gt;&amp;nbsp;file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Marking and removing &lt;span class="caps"&gt;PCR&lt;/span&gt; duplicates: uses Picard (Broad Institute of &lt;span class="caps"&gt;MIT&lt;/span&gt; and Harvard) and SAMtools &lt;a href="https://academic.oup.com/bioinformatics/article/25/16/2078/204688"&gt;(Li et al., 2009)&lt;/a&gt;. This is another cleanup step. It takes the aligned &lt;span class="caps"&gt;SAM&lt;/span&gt; file and produces an aligned sorted &lt;span class="caps"&gt;BAM&lt;/span&gt; file with duplicated reads removed. &lt;a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1097-3"&gt;Ebbert et al.&lt;/a&gt; define &lt;span class="caps"&gt;PCR&lt;/span&gt; duplicates as: &amp;#8220;&amp;#8230;sequence reads that result from sequencing two or more copies of the exact same &lt;span class="caps"&gt;DNA&lt;/span&gt; fragment, which, at worst, may contain erroneous mutations introduced during &lt;span class="caps"&gt;PCR&lt;/span&gt; amplification, or, at the very least, make the occurrence of the allele(s) sequenced in duplicates appear proportionately more often than it should compared to the other allele (assuming a non-haploid&amp;nbsp;organism)&amp;#8221;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remove low mapping quality reads: uses SAMtools (Li et al., 2009). Reads falling in repetitive regions usually get very low mapping quality, so we remove it to reduce noise during variant call. Takes the aligned sorted &lt;span class="caps"&gt;BAM&lt;/span&gt; file with duplicated reads removed and removes low mapping quality&amp;nbsp;reads.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quality control (&lt;span class="caps"&gt;QC&lt;/span&gt;): uses SAMtools (Li et al., 2009), BEDTools &lt;a href="https://academic.oup.com/bioinformatics/article/26/6/841/244688"&gt;(Quinlan &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Hall, 2010)&lt;/a&gt;. Quantifies the removed off-target reads, the sequencing reads that do not align to the target genome and calculates the mean depth of read coverage in the genome. Takes in the &lt;span class="caps"&gt;BAM&lt;/span&gt; file generated in the previous&amp;nbsp;step.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Downsampling/random read sampling: uses &lt;a href="https://broadinstitute.github.io/picard/"&gt;Picard&lt;/a&gt; (Broad Institute of &lt;span class="caps"&gt;MIT&lt;/span&gt; and Harvard). This step takes the cleaned-up aligned sorted &lt;span class="caps"&gt;BAM&lt;/span&gt; file generated by the previous steps and splits into 3 &amp;#8216;sub-BAMs&amp;#8217; of random reads sorted with probabilities of 75%, 50%, and&amp;nbsp;25%.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Variant calling: uses SAMtools/BCFtools (Li et al., 2009). This step identifies genetic variation present in the sample reads. It takes on all 4 &lt;span class="caps"&gt;BAM&lt;/span&gt; files, after which a consensus &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3137218/"&gt;Variant Call Format (&lt;span class="caps"&gt;VCF&lt;/span&gt;)&lt;/a&gt; file is&amp;nbsp;produced.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Annotation: uses Variant Effect Predictor &lt;a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0974-4"&gt;(McLaren et al., 2016)&lt;/a&gt;. It takes the list of variants compiled in the consensus &lt;span class="caps"&gt;VCF&lt;/span&gt; file and annotates them, identifying possible phenotypic effects. Outputs text and html summary files with the results. Check &lt;a href="https://www.ensembl.org/info/docs/tools/vep/script/vep_other.html"&gt;&lt;span class="caps"&gt;VEP&lt;/span&gt;&amp;#8217;s documentation&lt;/a&gt; if you want to customize the annotation options in the&amp;nbsp;script.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Output&lt;/h2&gt;
&lt;p&gt;Once the script is running you will see several files being generated. Once the script finishes, the files will be neatly organized in a folder &lt;code&gt;prefix_results&lt;/code&gt;, where &lt;code&gt;prefix&lt;/code&gt; is the name root of the &lt;code&gt;FASTQ&lt;/code&gt; files:&lt;/p&gt;
&lt;p&gt;&lt;img alt="demo results 1" src="https://antoniocampos13.github.io/images/demo_results_1.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Open the folder and check that there are some files and three subfolders. These subfolders hold all intermediate files generated by the script (&lt;code&gt;.sam&lt;/code&gt;, &lt;code&gt;.bam&lt;/code&gt; and many others). &lt;code&gt;trimmed_files&lt;/code&gt; folder hold the trimmed &lt;code&gt;FASTQ&lt;/code&gt; files alongside Trim Galore!&amp;#8217;s reports (step 1). &lt;code&gt;alignment_files&lt;/code&gt; hold intermediate files generated by steps 2 trough 5. &lt;code&gt;variant_call_files&lt;/code&gt; hold intermediate files generated by steps 7 through&amp;nbsp;8.&lt;/p&gt;
&lt;p&gt;&lt;img alt="demo results 2" src="https://antoniocampos13.github.io/images/demo_results_2.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s focus the attention on the other five&amp;nbsp;files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Master_Log.txt and Pipeline_Log.txt files: logs from the script operations. The first one has a copy of all commands issued by the script. The second one is more concise; it summarizes input parameters alongside date and time each step in the scripted started. Check these files if any errors occur to identify what went&amp;nbsp;wrong.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Final.vcf: a &lt;span class="caps"&gt;VCF&lt;/span&gt; file containing all variants identified in the sample. It contains chromosome position of the variants, alleles and other&amp;nbsp;information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AnnotationVEP.txt and AnnotationVEP.html: outputs of annotation by Ensembl&amp;#8217;s &lt;span class="caps"&gt;VEP&lt;/span&gt;. The text file is tab-separated file listing the called variants and their characteristics (more on that later). The &lt;code&gt;HTML&lt;/code&gt; file contains a summarized quantification of the variants&amp;nbsp;characteristics.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Open the &lt;code&gt;SRR6784104_AnnotationVEP.txt&lt;/code&gt; file into a spreadsheet to make the visualization easier. You will see there is a header with several definitions/abbreviations for the information contained in the file. Scroll down until you found a table-like&amp;nbsp;part.&lt;/p&gt;
&lt;p&gt;In this table part, there is several important information that is interesting to check. Some of the columns I like to&amp;nbsp;assess:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;#Uploaded_variation&lt;/code&gt;: an identifier of each&amp;nbsp;variation;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Location&lt;/code&gt;: chromosome and position of the&amp;nbsp;variation;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Allele&lt;/code&gt;: particular nucleotide configuration found in determined position in the&amp;nbsp;sample;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Gene&lt;/code&gt;: if the variant is located within a gene, its unique RefSeq gene &lt;span class="caps"&gt;ID&lt;/span&gt; (an integer) will be&amp;nbsp;there;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Feature&lt;/code&gt;: if the variant is located within a gene, a unique RefSeq accession code of the gene sequence will be&amp;nbsp;there;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Consequence&lt;/code&gt;: I found this column weirdly-named, because it reflects more the overall location of the variant than a molecular consequence as the name implies. For example, it will indicate that the variant is a &lt;code&gt;missense_variant&lt;/code&gt;, an &lt;code&gt;intron_variant&lt;/code&gt;, &lt;code&gt;regulatory_region_variant&lt;/code&gt; and so&amp;nbsp;on;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Protein_position&lt;/code&gt;, &lt;code&gt;Amino_acids&lt;/code&gt;, &lt;code&gt;Codons&lt;/code&gt;: if missense or synonym, information about amino acids changes and position on the protein will be in these&amp;nbsp;columns;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Existing_variation&lt;/code&gt;: if variation was already previously identified in other samples, the RefSeq (starting with &lt;code&gt;rs&lt;/code&gt;) or other identifier will be there. RefSeq-identified variants can be found in &lt;a href="https://www.ncbi.nlm.nih.gov/snp/"&gt;&lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;#8217;s dbSNP&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IMPACT&lt;/code&gt;: the variant&amp;#8217;s impact on phenotype (&lt;span class="caps"&gt;LOW&lt;/span&gt;, &lt;span class="caps"&gt;MODIFIER&lt;/span&gt;, &lt;span class="caps"&gt;HIGH&lt;/span&gt;);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VARIANT_CLASS&lt;/code&gt;: the class of the variant. &lt;span class="caps"&gt;SNV&lt;/span&gt; (single nucleotide variation, the same as single nucleotied polymorphism &amp;#8212; &lt;span class="caps"&gt;SNP&lt;/span&gt;), insertions and deletions are the most&amp;nbsp;common;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SYMBOL&lt;/code&gt;: the official symbol (abbreviation) of the gene&amp;nbsp;name;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BIOTYPE&lt;/code&gt;: if the variant is located within a gene, the gene function. For example: protein_coding, lncRNA, miRNA, and so&amp;nbsp;on;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SIFT&lt;/code&gt; and &lt;code&gt;PolyPhen&lt;/code&gt;: named after the tools that predict whether an amino acid substitution affects protein function and structure of a human&amp;nbsp;protein;&lt;/li&gt;
&lt;li&gt;Columns prefixed with &lt;code&gt;AF&lt;/code&gt;: contain the allelic frequency of a given variant in some &lt;a href="https://www.internationalgenome.org/category/population/"&gt;global populations&lt;/a&gt;. For example, &lt;code&gt;AFR&lt;/code&gt;: African,&lt;code&gt;AMR&lt;/code&gt;: Ad Mixed American, &lt;code&gt;EAS&lt;/code&gt;: East Asian, &lt;code&gt;SAS&lt;/code&gt;: South&amp;nbsp;Asian;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CLIN_SIG&lt;/code&gt;: a short sentence stating the clinical significance (if available) of the&amp;nbsp;variant;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PUBMED&lt;/code&gt;: a list of PubMed IDs of references citing the variation (if&amp;nbsp;available).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;AnnotationVEP.html&lt;/code&gt; file contains a collection of graphical representations of several characteristics of the detected variants. See below some of them. Notice that your results will be different from these figures, since I used a different set of &lt;code&gt;FASTQ&lt;/code&gt; files and reference&amp;nbsp;files.&lt;/p&gt;
&lt;p&gt;&lt;img alt="demo results 3" src="https://antoniocampos13.github.io/images/demo_results_3.PNG"&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion of Part&amp;nbsp;4&lt;/h2&gt;
&lt;p&gt;In this part I&amp;nbsp;showed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to use the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script;&lt;/li&gt;
&lt;li&gt;Summarized the steps performed by the&amp;nbsp;script;&lt;/li&gt;
&lt;li&gt;Summarized the principal results output by the&amp;nbsp;script.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, I finished all the steps I followed to prepare the system for Bioinformatics analysis, gather the necessary files and apply them to obtain annotations from human genome &lt;span class="caps"&gt;NGS&lt;/span&gt; reads&amp;nbsp;samples.&lt;/p&gt;
&lt;p&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;rss feed&lt;/a&gt; or &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; to keep updated whenever I post new&amp;nbsp;protocols.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-1"&gt;Go back to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;1)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-2"&gt;Go back to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;2)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-3"&gt;Go back to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;3)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;Setting Up Your Unix Computer for Bioinformatics&amp;nbsp;Analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology&amp;nbsp;Information&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;Home - &lt;span class="caps"&gt;SRA&lt;/span&gt; - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/"&gt;Babraham Bioinformatics - Trim&amp;nbsp;Galore!&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"&gt;Babraham Bioinformatics - FastQC A Quality Control tool for High Throughput Sequence&amp;nbsp;Data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/marcelm/cutadapt/"&gt;marcelm/cutadapt&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://academic.oup.com/bioinformatics/article/25/14/1754/225615"&gt;Fast and accurate short read alignment with Burrows–Wheeler&amp;nbsp;transform&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://academic.oup.com/bioinformatics/article/25/16/2078/204688"&gt;Sequence Alignment/Map format and&amp;nbsp;SAMtools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1097-3"&gt;Evaluating the necessity of &lt;span class="caps"&gt;PCR&lt;/span&gt; duplicate removal from next-generation sequencing data and a comparison of&amp;nbsp;approaches&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://academic.oup.com/bioinformatics/article/26/6/841/244688"&gt;BEDTools: a flexible suite of utilities for comparing genomic&amp;nbsp;features&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://broadinstitute.github.io/picard/"&gt;Picard Tools - By Broad&amp;nbsp;Institute&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3137218/"&gt;The variant call format and&amp;nbsp;VCFtools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0974-4"&gt;The Ensembl Variant Effect&amp;nbsp;Predictor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ensembl.org/info/docs/tools/vep/script/vep_other.html"&gt;Other&amp;nbsp;information&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/snp/"&gt;Home - &lt;span class="caps"&gt;SNP&lt;/span&gt; - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.internationalgenome.org/category/population/"&gt;Population | 1000&amp;nbsp;Genomes&lt;/a&gt;&lt;/p&gt;</content><category term="Unix"></category><category term="Bioinformatics"></category><category term="genomic variation"></category><category term="entrez-direct"></category><category term="EDirect"></category></entry><entry><title>FASTQ to Annotation (Part 3)</title><link href="https://antoniocampos13.github.io/fastq-to-annotation-part-3.html" rel="alternate"></link><published>2020-10-05T18:00:00-03:00</published><updated>2020-10-05T18:00:00-03:00</updated><author><name>Antonio Victor Campos Coelho</name></author><id>tag:antoniocampos13.github.io,2020-10-05:/fastq-to-annotation-part-3.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;In a &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;previous post&lt;/a&gt;, I showed how to configure an Ubuntu system to install Bioinformatics&amp;nbsp;programs.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Now, using the environment I created, I will demonstrate a bash script, &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; that takes next generation sequencing (&lt;span class="caps"&gt;NGS&lt;/span&gt;) raw reads from human whole genome sequencing as input and produces variant annotation …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;In a &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;previous post&lt;/a&gt;, I showed how to configure an Ubuntu system to install Bioinformatics&amp;nbsp;programs.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Now, using the environment I created, I will demonstrate a bash script, &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; that takes next generation sequencing (&lt;span class="caps"&gt;NGS&lt;/span&gt;) raw reads from human whole genome sequencing as input and produces variant annotation as output. Variant annotation is the process of identifying genetic variants in some genomic &lt;span class="caps"&gt;DNA&lt;/span&gt; sample, and assess, for example, if any of the found variants have any effect on phenotype, such as increased susceptibility to certain&amp;nbsp;diseases.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In the &lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-1"&gt;first part&lt;/a&gt;, I showed how to search for &lt;span class="caps"&gt;NGS&lt;/span&gt; projects deposited in &lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology Information (&lt;span class="caps"&gt;NCBI&lt;/span&gt;) databases&lt;/a&gt; from which I can download sequencing reads later to use with the&amp;nbsp;script.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In the &lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-2"&gt;second part&lt;/a&gt;, I showed how to retrieve raw genome sequencing reads in the form of &lt;code&gt;FASTQ&lt;/code&gt; files, which are deposited in &lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;&lt;span class="caps"&gt;SRA&lt;/span&gt;&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here in the third part, I make the final preparations for the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script demonstration using the &lt;code&gt;FASTQ&lt;/code&gt; files obtained in the previous&amp;nbsp;part.&lt;/p&gt;
&lt;h2&gt;Final&amp;nbsp;preparations&lt;/h2&gt;
&lt;h3&gt;Installing local cache of Ensembl Variant Effect Predictor (&lt;span class="caps"&gt;VEP&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://www.ensembl.org/info/docs/tools/vep/index.html"&gt;Ensembl Variant Effect Predictor (&lt;span class="caps"&gt;VEP&lt;/span&gt;)&lt;/a&gt; is the core tool used by the script for the annotation of the effects of any variants present in the sample. It may be used online, but Ensembl strongly recommends users to download and install a local cache of all data deposited in the tool to avoid server overload. I open a terminal, activate the &lt;code&gt;bioenv&lt;/code&gt; miniconda environment and execute the commands&amp;nbsp;below:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;WARNING&lt;/span&gt;: Several gigabytes of data will be downloaded from the internet and installed on your computer. Be sure that you have plenty or unlimited data allowances from your &lt;span class="caps"&gt;ISP&lt;/span&gt; and sufficient free space on your hard drive before continuing. It will take a while (several minutes to hours) until all the needed processes&amp;nbsp;finish.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;vep_install -a cf -s homo_sapiens_refseq -y GRCh38 -c . –CONVERT
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Unfortunately, the command above is prone to network and other esoteric errors. If you have any problem, you can try an alternative manner. See&amp;nbsp;below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Alternative: manually download the compressed cache&lt;/span&gt;
wget ftp://ftp.ensembl.org/pub/release-101/variation/indexed_vep_cache/homo_sapiens_refseq_vep_101_GRCh38.tar.gz -P &lt;span class="nv"&gt;$HOME&lt;/span&gt;/.vep

&lt;span class="c1"&gt;# Uncompress the cache&lt;/span&gt;
tar -zxf &lt;span class="nv"&gt;$HOME&lt;/span&gt;/.vep/homo_sapiens_refseq_vep_101_GRCh38.tar.gz -C &lt;span class="nv"&gt;$HOME&lt;/span&gt;/.vep
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After some time (several minutes to some hours depending on the network and the capabilities of your computer) the &lt;span class="caps"&gt;VEP&lt;/span&gt; cache will be downloaded and installed into a hidden folder in your home folder (&lt;code&gt;$HOME/.vep&lt;/code&gt;). Therefore, notice that it is independent of miniconda environments. Thus, it is expected that once installed, this cache will work with any miniconda environment on your computer. You may backup the &lt;code&gt;.vep/&lt;/code&gt; folder to avoid downloading the whole thing again (but consider to download newer versions of the cache as they become available&amp;nbsp;though).&lt;/p&gt;
&lt;h3&gt;Obtaining human genome reference&amp;nbsp;files&lt;/h3&gt;
&lt;p&gt;The annotation process require a collection of reference files. These files will assist us to generate a &amp;#8220;list&amp;#8221; of genetic variants, alongside their possible effects on the phenotype (the &lt;strong&gt;annotation&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;These files&amp;nbsp;are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A &lt;a href="https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;amp;PAGE_TYPE=BlastDocs&amp;amp;DOC_TYPE=BlastHelp"&gt;&lt;code&gt;FASTA&lt;/code&gt;&lt;/a&gt; file (extensions &lt;code&gt;.fasta&lt;/code&gt;, &lt;code&gt;.fa&lt;/code&gt; or &lt;code&gt;.fna&lt;/code&gt;). It must contain the complete nucleotide sequence of the human genome. We will compare our &lt;code&gt;FASTQ&lt;/code&gt; files against&amp;nbsp;it;&lt;/li&gt;
&lt;li&gt;A &lt;a href="http://www.htslib.org/doc/faidx.html"&gt;&lt;code&gt;FASTA index&lt;/code&gt;&lt;/a&gt; (&lt;code&gt;.fai&lt;/code&gt;). It stores genomic regions as coordinates. We will use it to generate a Browser Extensible Data (&lt;code&gt;.bed&lt;/code&gt;) file (see&amp;nbsp;below);&lt;/li&gt;
&lt;li&gt;A &lt;a href="https://en.wikipedia.org/wiki/BED_(file_format)"&gt;Browser Extensible Data (&lt;code&gt;.bed&lt;/code&gt;)&lt;/a&gt; file. It stores genomic regions as coordinates, indicating the start and end of chromosomes. It is most useful when its information is chromosome-ordered and&amp;nbsp;position-sorted;&lt;/li&gt;
&lt;li&gt;Alternatively, the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script can accept a &lt;a href="https://www.ensembl.org/info/website/upload/gff.html"&gt;General Feature Format (&lt;code&gt;.gff&lt;/code&gt;)&lt;/a&gt; instead of the &lt;code&gt;.bed&lt;/code&gt; file. It is used for describing genes and other features of &lt;span class="caps"&gt;DNA&lt;/span&gt;, &lt;span class="caps"&gt;RNA&lt;/span&gt; and protein&amp;nbsp;sequences;&lt;/li&gt;
&lt;li&gt;Burrows-Wheelers Aligner index files (&lt;code&gt;.amb&lt;/code&gt;, &lt;code&gt;.ann&lt;/code&gt;, &lt;code&gt;.bwt&lt;/code&gt;, &lt;code&gt;.pac&lt;/code&gt; and &lt;code&gt;.sa&lt;/code&gt;). The &lt;a href="http://bio-bwa.sourceforge.net/"&gt;&lt;code&gt;bwa&lt;/code&gt; program&lt;/a&gt; is a short read alignment tool. In other words, it identifies the location of the reads inside the &lt;code&gt;FASTQ&lt;/code&gt; files. The &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script uses &lt;code&gt;bwa&lt;/code&gt; at the second step of the pipeline. It works by efficiently using this collection of five files as a&amp;nbsp;index.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Why we need these files?&lt;/em&gt; Briefly, They serve to map the genomic location of any variant we identify in our samples, as well as the genetic mutation that occurred there, which allows us to predict the possible effect(s) over the phenotype in question (in our case, MAPKi-resistance in melanoma samples). If we compare the genetic variation profile of MAPKi-susceptible samples with MAPKi-resistant samples, we could identify genetic variants associated with the resistances, and perhaps point to new directions of prognosis and new&amp;nbsp;treatments.&lt;/p&gt;
&lt;p&gt;I will now show how to obtain all these files. Remember to &lt;strong&gt;activate&lt;/strong&gt; the &lt;code&gt;miniconda&lt;/code&gt; that you created before if&amp;nbsp;necessary.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;WARNING&lt;/span&gt;: Several gigabytes of data will be downloaded from the internet and installed on your computer. Be sure that you have plenty or unlimited data allowances from your &lt;span class="caps"&gt;ISP&lt;/span&gt; and sufficient free space on your hard drive before continuing. It will take a while (several minutes to hours) until all the needed processes&amp;nbsp;finish.&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;1. Human genome &lt;span class="caps"&gt;FASTA&lt;/span&gt;&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# First, create a subfolder into the demo folder to better organize our reference files&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt; demo
mkdir refs
&lt;span class="nb"&gt;cd&lt;/span&gt; refs

&lt;span class="c1"&gt;# Download GRCh38 major release without ALT contigs and with decoy genomes (EBV and hs38d1 contig) from NCBI&amp;#39;s FTP server&lt;/span&gt;
curl -O ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.28_GRCh38.p13/GRCh38_major_release_seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;2. Human genome &lt;span class="caps"&gt;FASTA&lt;/span&gt;&amp;nbsp;index&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Download from NCBI&amp;#39;s FTP server&lt;/span&gt;
curl -O ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.28_GRCh38.p13/GRCh38_major_release_seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.fai
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;3. Human genome &lt;span class="caps"&gt;BED&lt;/span&gt;&amp;nbsp;file&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Produce sorted BED file from reference genome index file obtained above&lt;/span&gt;
awk &lt;span class="s1"&gt;&amp;#39;{print $1 &amp;quot;\t0\t&amp;quot; $2}&amp;#39;&lt;/span&gt; GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.fai &lt;span class="p"&gt;|&lt;/span&gt; sort -k1,1V -k2,2n &amp;gt; GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.bed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;4. Human genome &lt;span class="caps"&gt;GFF&lt;/span&gt; file (optional alternative to &lt;span class="caps"&gt;BED&lt;/span&gt;&amp;nbsp;file)&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Download from NCBI&amp;#39;s FTP server&lt;/span&gt;
curl -O ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.28_GRCh38.p13/GRCh38_major_release_seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gff.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h4&gt;5. bwa index&amp;nbsp;files&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;bwa index GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When you complete all of the steps in Part 1, Part 2 and in this part, your &lt;code&gt;demo&lt;/code&gt; folder should have the files showed&amp;nbsp;below&lt;/p&gt;
&lt;p&gt;&lt;img alt="demo folder contents until now" src="https://antoniocampos13.github.io/images/demo_folder.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Now, go to the folder &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Unix/2020-10-01_Fastq%20to%20Annotation"&gt;&lt;code&gt;FastQ_to_Annotation&lt;/code&gt; folder in my portfolio&lt;/a&gt;, take heed of the &lt;span class="caps"&gt;GPL&lt;/span&gt; License and Copyright Notice, download and copy the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script into your &lt;code&gt;demo&lt;/code&gt; folder.&lt;/p&gt;
&lt;p&gt;Thus, the only mandatory files are the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt;, the &lt;code&gt;FASTQ&lt;/code&gt; pair and the ones in the &lt;code&gt;refs&lt;/code&gt; folder. If you are missing any other file, do not&amp;nbsp;worry.&lt;/p&gt;
&lt;h2&gt;&lt;span class="caps"&gt;GPL&lt;/span&gt; License and Copyright&amp;nbsp;Notice&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script is a modified version from &lt;a href="https://github.com/kevinblighe/ClinicalGradeDNAseq"&gt;Dr. Kevin Blighe&amp;#8217;s original scripts&lt;/a&gt;. Both works are licensed under &lt;a href="https://www.gnu.org/licenses/licenses.en.html"&gt;&lt;span class="caps"&gt;GNU&lt;/span&gt; General Public License v3.0&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Conclusion of Part&amp;nbsp;3&lt;/h2&gt;
&lt;p&gt;In this part I showed how&amp;nbsp;to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set up &lt;span class="caps"&gt;VEP&lt;/span&gt; local&amp;nbsp;cache;&lt;/li&gt;
&lt;li&gt;Obtain human genome reference&amp;nbsp;files;&lt;/li&gt;
&lt;li&gt;Obtain auxiliary files needed for short read&amp;nbsp;alignment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, everything is in place for the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-4"&gt;Go to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;4)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-1"&gt;Go back to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;1)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-2"&gt;Go back to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;2)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology&amp;nbsp;Information&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;Home - &lt;span class="caps"&gt;SRA&lt;/span&gt; - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ensembl.org/info/docs/tools/vep/index.html"&gt;Ensembl Variant Effect Predictor (&lt;span class="caps"&gt;VEP&lt;/span&gt;)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;amp;PAGE_TYPE=BlastDocs&amp;amp;DOC_TYPE=BlastHelp"&gt;&lt;span class="caps"&gt;BLAST&lt;/span&gt; Topics | Query Input and database&amp;nbsp;selection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.htslib.org/doc/faidx.html"&gt;faidx(5) manual&amp;nbsp;page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/BED_(file_format)"&gt;&lt;span class="caps"&gt;BED&lt;/span&gt; (file format) -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ensembl.org/info/website/upload/gff.html"&gt;&lt;span class="caps"&gt;GFF&lt;/span&gt;/&lt;span class="caps"&gt;GTF&lt;/span&gt; File&amp;nbsp;Format&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bio-bwa.sourceforge.net/"&gt;Burrows-Wheeler&amp;nbsp;Aligner&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/kevinblighe/ClinicalGradeDNAseq"&gt;kevinblighe/ClinicalGradeDNAseq&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.gnu.org/licenses/licenses.en.html"&gt;&lt;span class="caps"&gt;GNU&lt;/span&gt; General Public&amp;nbsp;License&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.28_GRCh38.p13/GRCh38_major_release_seqs_for_alignment_pipelines/"&gt;&lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;#8217;s &lt;span class="caps"&gt;FTP&lt;/span&gt; Server | GRCh38 Major release sequences for alignment&amp;nbsp;pipelines&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://ftp://ftp.ensembl.org/pub/release-101/fasta/homo_sapiens/dna/"&gt;Ensembl&amp;#8217;s &lt;span class="caps"&gt;FTP&lt;/span&gt; Server | Homo sapiens &lt;span class="caps"&gt;DNA&lt;/span&gt; sequences release&amp;nbsp;101&lt;/a&gt;&lt;/p&gt;</content><category term="Unix"></category><category term="Bioinformatics"></category><category term="genomic variation"></category><category term="entrez-direct"></category><category term="EDirect"></category></entry><entry><title>FASTQ to Annotation (Part 2)</title><link href="https://antoniocampos13.github.io/fastq-to-annotation-part-2.html" rel="alternate"></link><published>2020-10-02T18:00:00-03:00</published><updated>2020-10-02T18:00:00-03:00</updated><author><name>Antonio Victor Campos Coelho</name></author><id>tag:antoniocampos13.github.io,2020-10-02:/fastq-to-annotation-part-2.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;In a &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis"&gt;previous post&lt;/a&gt;, I showed how to configure an Ubuntu system to install Bioinformatics&amp;nbsp;programs.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Now, using the environment I created, I will demonstrate a bash script, &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; that takes next generation sequencing (&lt;span class="caps"&gt;NGS&lt;/span&gt;) raw reads from human whole genome sequencing as input and produces variant annotation …&lt;/em&gt;&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;In a &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis"&gt;previous post&lt;/a&gt;, I showed how to configure an Ubuntu system to install Bioinformatics&amp;nbsp;programs.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Now, using the environment I created, I will demonstrate a bash script, &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; that takes next generation sequencing (&lt;span class="caps"&gt;NGS&lt;/span&gt;) raw reads from human whole genome sequencing as input and produces variant annotation as output. Variant annotation is the process of identifying genetic variants in some genomic &lt;span class="caps"&gt;DNA&lt;/span&gt; sample, and assess, for example, if any of the found variants have any effect on phenotype, such as increased susceptibility to certain&amp;nbsp;diseases.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In the &lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-1"&gt;first part&lt;/a&gt;, I showed how to search for &lt;span class="caps"&gt;NGS&lt;/span&gt; projects deposited in &lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology Information (&lt;span class="caps"&gt;NCBI&lt;/span&gt;) databases&lt;/a&gt; from which I can download sequencing reads later to use with the&amp;nbsp;script.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here in the second part, I will show how to retrieve raw genome sequencing reads in the form of &lt;code&gt;FASTQ&lt;/code&gt; files, which are deposited in &lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;&lt;span class="caps"&gt;SRA&lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But first, let&amp;#8217;s review what &lt;code&gt;FASTQ&lt;/code&gt; files&amp;nbsp;are.&lt;/p&gt;
&lt;h2&gt;What is the the &lt;span class="caps"&gt;FASTQ&lt;/span&gt;&amp;nbsp;format&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://support.illumina.com/bulletins/2016/04/fastq-files-explained.html"&gt;&lt;code&gt;FASTQ&lt;/code&gt;&lt;/a&gt; file format is how we store the output of whole genome or transcriptomic sequencing (sequences of nucleotides). It inherits its name from the &lt;code&gt;FASTA&lt;/code&gt; format that stores and the word &lt;code&gt;Qualities&lt;/code&gt;, because a &lt;code&gt;FASTQ&lt;/code&gt; file not only contains the nucleotide sequence, but also contains the quality of the sequencing&amp;nbsp;procedure.&lt;/p&gt;
&lt;p&gt;The qualities are represented by Phred scores (&lt;code&gt;Q&lt;/code&gt;), which is used to calculate the probability of a nucleotide being incorrectly identified during sequencing using a formula (I will not go into details here). So, for example, if we check a &lt;code&gt;FASTQ&lt;/code&gt; file and found a nucleotide with &lt;code&gt;Q = 30&lt;/code&gt;, it means that there is a probability of 1 in 1000 that it was incorrectly assigned during sequencing &amp;#8212; in other words an accuracy of 99.9%. Therefore, &lt;code&gt;Q&lt;/code&gt; values around 30 and above are generally seem as very good&amp;nbsp;quality.&lt;/p&gt;
&lt;h3&gt;The reason &lt;code&gt;FASTQ&lt;/code&gt; files contain information about&amp;nbsp;quality&lt;/h3&gt;
&lt;p&gt;Because during use of these kind of files, it is important that we have confidence on the sequence assignment. During processing in Bioinformatics analysis pipelines, we can remove low-quality nucleotides to ensure that he have the &amp;#8220;cleanest&amp;#8221; information&amp;nbsp;possible.&lt;/p&gt;
&lt;h3&gt;Obtaining &lt;span class="caps"&gt;FASTQ&lt;/span&gt;&amp;nbsp;files&lt;/h3&gt;
&lt;p&gt;We obtain &lt;code&gt;FASTQ&lt;/code&gt; after sequencing of genomic samples in platforms such as &lt;a href="https://www.illumina.com"&gt;Illumina&lt;/a&gt;, which practically dominates the &lt;span class="caps"&gt;NGS&lt;/span&gt; market nowadays. Check the fundamentals of Illumina&amp;#8217;s &lt;span class="caps"&gt;NGS&lt;/span&gt; platform &lt;a href="https://www.illumina.com/science/technology/next-generation-sequencing/beginners.html"&gt;here&lt;/a&gt;. Normally, researchers deposit raw &lt;code&gt;FASTQ&lt;/code&gt; files on public databases to share their discoveries with other scientists. This is why I took note of the &lt;code&gt;BioProject&lt;/code&gt; accession &lt;span class="caps"&gt;ID&lt;/span&gt; during the demonstration of Part 1. With this &lt;span class="caps"&gt;ID&lt;/span&gt;, I can retrieve sequencing reads associated with the&amp;nbsp;project.&lt;/p&gt;
&lt;h3&gt;A&amp;nbsp;Warning&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;FASTQ&lt;/code&gt; files, especially from human samples, have very big sizes, in the gigabytes range. Therefore, considerable computing power and storage are needed to process these kind of&amp;nbsp;files.&lt;/p&gt;
&lt;h2&gt;Retrieving reads from a&amp;nbsp;BioProject&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Activate&lt;/strong&gt; the environment, if needed, and connect to &lt;code&gt;SRA&lt;/code&gt; database via &lt;code&gt;EDirect esearch&lt;/code&gt; command using the &lt;code&gt;PRJNA436005&lt;/code&gt; as keyword for query. Then, we pipe the results to the &lt;code&gt;efetch&lt;/code&gt; command. With the &lt;code&gt;-format&lt;/code&gt; flag, it will format the results into the &lt;code&gt;runinfo&lt;/code&gt; format (more on that later). Finally, will save it into the &lt;code&gt;PRJNA436005_runinfo.csv&lt;/code&gt; file. You can choose other name if you&amp;nbsp;wish.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Continuing into the folder I created in the previous part&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt; demo
conda activate bioenv

esearch -db sra -query &lt;span class="s1"&gt;&amp;#39;PRJNA436005&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; efetch -format runinfo &amp;gt; PRJNA436005_runinfo.csv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;runinfo&lt;/code&gt; format displays metadata of read sets. Reads are inferred sequences of base pairs corresponding to &lt;span class="caps"&gt;DNA&lt;/span&gt; fragments produced during procedures for &lt;span class="caps"&gt;NGS&lt;/span&gt;. The collection of &lt;span class="caps"&gt;DNA&lt;/span&gt; fragments from a given sample is called a &lt;strong&gt;library&lt;/strong&gt;, which are sequenced to produce the set of &lt;strong&gt;reads&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Checking the &lt;code&gt;CSV&lt;/code&gt; file, I see that there are seven read sets generated by the project, each displayed on a row, and are identified by the &lt;code&gt;SRR&lt;/code&gt; prefix followed by some numbers. With this &lt;span class="caps"&gt;ID&lt;/span&gt; is possible to retrieve &lt;code&gt;FASTQ&lt;/code&gt; files for each read set. Now I check the &lt;code&gt;LibraryLayout&lt;/code&gt; column to confirm they are all &lt;strong&gt;&lt;span class="caps"&gt;PAIRED&lt;/span&gt;&lt;/strong&gt; reads, meaning that the researchers sequenced both ends of a fragment. Thus, each read set will produce two &lt;code&gt;FASTQ&lt;/code&gt; files, containing the sequences and qualities from all reads obtained from the library of the original sample. This is important to check because the script requires paired&amp;nbsp;reads.&lt;/p&gt;
&lt;p&gt;Other interesting columns that I like to check&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;spots&lt;/code&gt;, which are the number of physical locations in the sequencing flow cells where the sequencing adaptors are fixed. A spot contains several nucleotide bases from several, possibly millions, of&amp;nbsp;reads;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;avgLength&lt;/code&gt;, which as the name implies, is the average length, in nucleotides, of reads in the&amp;nbsp;set;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;size_MB&lt;/code&gt;, the size in megabytes of the read&amp;nbsp;set;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LibrarySource&lt;/code&gt;, which indicates if the sample source is &lt;span class="caps"&gt;GENOMIC&lt;/span&gt;, &lt;span class="caps"&gt;TRANSCRIPTOMIC&lt;/span&gt; and so&amp;nbsp;on;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Platform&lt;/code&gt;, the vendor of &lt;span class="caps"&gt;NGS&lt;/span&gt;&amp;nbsp;procedure;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Model&lt;/code&gt;, the model of the &lt;code&gt;Platform&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Sex&lt;/code&gt;, &lt;code&gt;Disease&lt;/code&gt; and &lt;code&gt;Tumor&lt;/code&gt;: descriptors of sample&amp;nbsp;phenotype.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For now, I will use only the first read, which has the &lt;code&gt;SRR6784104&lt;/code&gt; &lt;span class="caps"&gt;ID&lt;/span&gt;, since I will just demonstrate the script use. Finally, let&amp;#8217;s download the read set with the &lt;code&gt;EDirect fastq-dump&lt;/code&gt; command and split it into two files (&lt;code&gt;--split-files&lt;/code&gt; flag), one with reads from each end of &lt;span class="caps"&gt;DNA&lt;/span&gt; fragments in the original library, and compress them with &lt;code&gt;--gzip&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;fastq-dump --split-files SRR6784104 --gzip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;After a moment, two &lt;code&gt;fastq.gz&lt;/code&gt; files will be downloaded to the current working directory and are ready to be used as the input for the &lt;code&gt;FastQ_to_VariantCall.sh&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Conclusion (Part&amp;nbsp;2)&lt;/h2&gt;
&lt;p&gt;In this part I showed how&amp;nbsp;to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;obtain and inspect metadata from projects via their &lt;code&gt;BioProjects&lt;/code&gt; accession &lt;span class="caps"&gt;ID&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;download read sets via their &lt;code&gt;SRR&lt;/code&gt; accession &lt;span class="caps"&gt;ID&lt;/span&gt; via &lt;code&gt;fastq-dump&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I need to do some final preparations before using the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-3"&gt;Go to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;3)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-1"&gt;Go back to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;1)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis"&gt;Setting Up Your Unix Computer for Bioinformatics&amp;nbsp;Analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-1"&gt;&lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;1)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://support.illumina.com/bulletins/2016/04/fastq-files-explained.html"&gt;&lt;span class="caps"&gt;FASTQ&lt;/span&gt; files&amp;nbsp;explained&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology&amp;nbsp;Information&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;Home - &lt;span class="caps"&gt;SRA&lt;/span&gt; - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.illumina.com"&gt;Illumina | Sequencing and array-based solutions for genetic&amp;nbsp;research&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.illumina.com/science/technology/next-generation-sequencing/beginners.html"&gt;Next-Generation Sequencing for Beginners | &lt;span class="caps"&gt;NGS&lt;/span&gt; basics for&amp;nbsp;researchers&lt;/a&gt;&lt;/p&gt;</content><category term="Unix"></category><category term="Bioinformatics"></category><category term="genomic variation"></category><category term="entrez-direct"></category><category term="EDirect"></category></entry><entry><title>FASTQ to Annotation (Part 1)</title><link href="https://antoniocampos13.github.io/fastq-to-annotation-part-1.html" rel="alternate"></link><published>2020-10-01T18:00:00-03:00</published><updated>2020-10-01T18:00:00-03:00</updated><author><name>Antonio Victor Campos Coelho</name></author><id>tag:antoniocampos13.github.io,2020-10-01:/fastq-to-annotation-part-1.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In my &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;previous post&lt;/a&gt;, I showed how to configure an Ubuntu system to install Bioinformatics&amp;nbsp;programs.&lt;/p&gt;
&lt;p&gt;Now, using the environment I created, I will demonstrate a bash script, &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; that takes next generation sequencing (&lt;span class="caps"&gt;NGS&lt;/span&gt;) raw reads from human whole genome sequencing as input and produces variant annotation …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In my &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;previous post&lt;/a&gt;, I showed how to configure an Ubuntu system to install Bioinformatics&amp;nbsp;programs.&lt;/p&gt;
&lt;p&gt;Now, using the environment I created, I will demonstrate a bash script, &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; that takes next generation sequencing (&lt;span class="caps"&gt;NGS&lt;/span&gt;) raw reads from human whole genome sequencing as input and produces variant annotation as output. Variant annotation is the process of identifying genetic variants in some genomic &lt;span class="caps"&gt;DNA&lt;/span&gt; sample, and assess, for example, if any of the found variants have any effect on phenotype, such as increased susceptibility to certain&amp;nbsp;diseases.&lt;/p&gt;
&lt;p&gt;This demonstration will be separated in four parts. Here in the first part, I will show how to search for &lt;span class="caps"&gt;NGS&lt;/span&gt; projects deposited in &lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology Information (&lt;span class="caps"&gt;NCBI&lt;/span&gt;) databases&lt;/a&gt; from which I can download sequencing reads later to use with the&amp;nbsp;script.&lt;/p&gt;
&lt;h2&gt;Using &lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;#8217;s entrez-direct (EDirect) to retrieve &lt;span class="caps"&gt;FASTQ&lt;/span&gt;&amp;nbsp;files&lt;/h2&gt;
&lt;p&gt;I open my Unix terminal and activate the &lt;code&gt;bioenv&lt;/code&gt; environment:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda activate bioenv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now I use the &lt;code&gt;EDirect&lt;/code&gt; &lt;code&gt;esearch&lt;/code&gt; command to search &lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;#8217;s databases. I must provide a database using the flag &lt;code&gt;-db&lt;/code&gt;. Check the available databases &lt;a href="https://www.ncbi.nlm.nih.gov/books/NBK25497/table/chapter2.T._entrez_unique_identifiers_ui/?report=objectonly"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I will search the &lt;code&gt;biproject&lt;/code&gt; database because it contains metadata from projects dealing with high-throughput genome sequencing, transcriptome expression analysis and so on. I must use the &lt;code&gt;-query&lt;/code&gt; flag to provide keywords for search. In this example, I will search for studies dealing with &lt;strong&gt;vorinostat&lt;/strong&gt;, a medicine that is have been used in experimental &lt;span class="caps"&gt;HIV&lt;/span&gt;-1 latency reversal, or &amp;#8220;shock-and-kill&amp;#8221;&amp;nbsp;treatments.&lt;/p&gt;
&lt;p&gt;Remember to use single quotes (&amp;#8221;) enclosing the query, especially if it has several&amp;nbsp;words.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# It is just the beginning... (1/4)&lt;/span&gt;

&lt;span class="c1"&gt;# Let&amp;#39;s create a folder to organize our files inside&lt;/span&gt;
mkdir demo
&lt;span class="nb"&gt;cd&lt;/span&gt; demo

esearch -db bioproject -query &lt;span class="s1"&gt;&amp;#39;vorinostat&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The output is just a &lt;code&gt;XML&lt;/code&gt; summary including, among other things, the number of results&amp;nbsp;retrieved:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;&amp;lt;ENTREZ_DIRECT&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;Db&amp;gt;&lt;/span&gt;bioproject&lt;span class="nt"&gt;&amp;lt;/Db&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;WebEnv&amp;gt;&lt;/span&gt;MCID_5f7726730525f301023dc947&lt;span class="nt"&gt;&amp;lt;/WebEnv&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;QueryKey&amp;gt;&lt;/span&gt;1&lt;span class="nt"&gt;&amp;lt;/QueryKey&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;Count&amp;gt;&lt;/span&gt;61&lt;span class="nt"&gt;&amp;lt;/Count&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;Step&amp;gt;&lt;/span&gt;1&lt;span class="nt"&gt;&amp;lt;/Step&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/ENTREZ_DIRECT&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In this case, the query resulted in 61 results (check the &lt;code&gt;&amp;lt;count&amp;gt;&lt;/code&gt; tag). Thus, I will add more commands to retrieve the actual query results. I will pipe, i.e. transfer, the results of the query to the another command &amp;#8212; &lt;code&gt;efetch&lt;/code&gt; &amp;#8212; that will do this work for me. This is the pipe symbol: &lt;code&gt;|&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# ... not there yet ... (2/4)&lt;/span&gt;
esearch -db bioproject -query &lt;span class="s1"&gt;&amp;#39;vorinostat&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; efetch -format native -mode xml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The output is in &lt;code&gt;XML&lt;/code&gt; format, and it is unfortunately not very much human-readable. Here is a print screen showing the first result. Notice how the record is contained within a &lt;code&gt;&amp;lt;/DocumentSummary&amp;gt;&lt;/code&gt; node:&lt;/p&gt;
&lt;p&gt;&lt;img alt="esearch vorinostat XML result" src="https://antoniocampos13.github.io/images/esearch_vorinostat_xml_results.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Thus, I will once again pipe the results, this time to &lt;code&gt;xtract&lt;/code&gt; command. As its name implies, it extracts information from the &lt;code&gt;XML&lt;/code&gt; and formats into a tab-separated format that is easier to understand. I must input the flag &lt;code&gt;-pattern&lt;/code&gt; with the part of the &lt;code&gt;XML&lt;/code&gt; files that contains the desired information, which are &lt;code&gt;elements&lt;/code&gt;. In this example, I will search inside the &lt;code&gt;DocumentSummary&lt;/code&gt; for &lt;code&gt;ArchiveID@accession&lt;/code&gt; (project unique accession number), &lt;code&gt;ID&lt;/code&gt; (an auxiliary &lt;span class="caps"&gt;ID&lt;/span&gt; code to search for samples of said project), &lt;code&gt;Title&lt;/code&gt;(the title of the project),  &lt;code&gt;Description&lt;/code&gt; (normally an abstract of the project) and &lt;code&gt;Reference&lt;/code&gt; (a list of project-related papers in PubMed ids &amp;#8212; PMIDs, if available). Note that I am separating each argument with spaces, no quotes are necessary in this part of the&amp;nbsp;command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# ...almost there ... (3/4)&lt;/span&gt;
esearch -db bioproject -query &lt;span class="s1"&gt;&amp;#39;vorinostat&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; efetch -format native -mode xml &lt;span class="p"&gt;|&lt;/span&gt; xtract -pattern DocumentSummary -element ArchiveID@accession ID Title Description Reference
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here is the tab-separated output of the same record displayed on my&amp;nbsp;terminal:&lt;/p&gt;
&lt;p&gt;&lt;img alt="esearch vorinostat xtract result" src="https://antoniocampos13.github.io/images/esearch_vorinostat_xtract_results.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Lastly, I will add a final command to transfer to a local text file &lt;code&gt;vorinostat_projects.txt&lt;/code&gt; that will be saved in the current working directory. Note that if you have a identically-named file in the working directory, it will be overwritten, so be&amp;nbsp;careful.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Finally there! (4/4)&lt;/span&gt;
esearch -db bioproject -query &lt;span class="s1"&gt;&amp;#39;vorinostat&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; efetch -format native -mode xml &lt;span class="p"&gt;|&lt;/span&gt; xtract -pattern DocumentSummary -element ArchiveID@accession ID Reference Title Description &amp;gt; vorinostat_projects.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;vorinostat_projects.txt&lt;/code&gt; file can then be imported into spreadsheets to make easier to organize and read the&amp;nbsp;results.&lt;/p&gt;
&lt;h2&gt;Refining the&amp;nbsp;search&lt;/h2&gt;
&lt;p&gt;The command above is a very basic one to search &lt;span class="caps"&gt;NCBI&lt;/span&gt; databases via &lt;code&gt;EDirect&lt;/code&gt;. I can create more elaborate queries by adding other keywords and filtering results. &lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;#8217;s search engines have several parameters. I advise you go to any advanced search page on the &lt;span class="caps"&gt;NCBI&lt;/span&gt; website to look for the available&amp;nbsp;parameters.&lt;/p&gt;
&lt;p&gt;Using &lt;a href="https://www.ncbi.nlm.nih.gov/bioproject/"&gt;&lt;code&gt;BioProject&lt;/code&gt; database&lt;/a&gt; as example again, click on &lt;em&gt;Advanced&lt;/em&gt; to go the query&amp;nbsp;constructor:&lt;/p&gt;
&lt;p&gt;&lt;img alt="BioProject search box" src="https://antoniocampos13.github.io/images/bioproject_start.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Using the &lt;strong&gt;BioProject Advanced Search Builder&lt;/strong&gt;, I will refine our search. I wish to include only projects that with samples deposited on &lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;Sequence Read Archive (&lt;span class="caps"&gt;SRA&lt;/span&gt;)&lt;/a&gt;, from human samples and that investigated genetic variation. I input all of this into the search&amp;nbsp;boxes:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Refining our search" src="https://antoniocampos13.github.io/images/vorinostat_refined.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Note that clicking in &lt;code&gt;Show index list&lt;/code&gt; will provide a list of curated terms. I used them to filter for &amp;#8220;bioproject sra&amp;#8221; and &amp;#8220;variation&amp;#8221; projects. To filter for organism, it is easier: I simply selected the Organism on the drop-down list on the left of the search box. Finally, I connected all keywords with the &lt;code&gt;AND&lt;/code&gt; Boolean constructor, resulting on the&amp;nbsp;query:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;(((vorinostat) AND "bioproject sra"[Filter]) AND Homo sapiens[Organism]) AND "variation"[Filter]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You could continue the search on the website, of course, but let&amp;#8217;s go back to the terminal and continue from&amp;nbsp;there:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;esearch -db bioproject -query &lt;span class="s1"&gt;&amp;#39;(((vorinostat) AND &amp;quot;bioproject sra&amp;quot;[Filter]) AND Homo sapiens[Organism]) AND &amp;quot;variation&amp;quot;[Filter]&amp;#39;&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; efetch -format native -mode xml &lt;span class="p"&gt;|&lt;/span&gt; xtract -pattern DocumentSummary -element ArchiveID@accession ID Reference Title Description  &amp;gt; vorinostat_refined.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Remember: single quotes enclosing the query. Turns out that this refined search was rather restrictive: it resulted in a single record. Checking the &lt;code&gt;vorinostat_refined.txt&lt;/code&gt; I see in the abstract that the project dealt with samples from patients with melanoma. One of the last sentences says: &amp;#8220;&lt;span class="caps"&gt;DNA&lt;/span&gt; Seq data: biopsy samples from patients pre- and post- treated with Vorinostat; check mutations related to MAPKi-resistance&amp;#8221; (MAPKi: Mitogen Activated Protein Kinase inhibitors). Although I had &lt;span class="caps"&gt;HIV&lt;/span&gt;-1-related projects in mind, that&amp;#8217;s fine for now, since it is suitable to &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script: identify and annotate genetic&amp;nbsp;variation.&lt;/p&gt;
&lt;p&gt;Then, I take note of the project &lt;span class="caps"&gt;ID&lt;/span&gt;: &lt;code&gt;PRJNA436005&lt;/code&gt;. I will use it to retrieve reads from this project by searching the &lt;span class="caps"&gt;SRA&lt;/span&gt; with&amp;nbsp;it.&lt;/p&gt;
&lt;h2&gt;Conclusion of Part&amp;nbsp;1&lt;/h2&gt;
&lt;p&gt;In this part I showed how&amp;nbsp;to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;search &lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;#8217;s databases, (especially&amp;nbsp;BioProject);&lt;/li&gt;
&lt;li&gt;refine&amp;nbsp;searches;&lt;/li&gt;
&lt;li&gt;save search results into local, human-readable text&amp;nbsp;files.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now I need to use the information gathered here to download read sets in &lt;code&gt;FASTQ&lt;/code&gt; format.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-2"&gt;Go to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;2)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;Setting Up Your Unix Computer for Bioinformatics&amp;nbsp;Analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology&amp;nbsp;Information&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/books/NBK25497/table/chapter2.T._entrez_unique_identifiers_ui/?report=objectonly"&gt;Entrez Unique Identifiers (UIDs) for selected&amp;nbsp;databases&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/bioproject/"&gt;Home - BioProject - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;Home - &lt;span class="caps"&gt;SRA&lt;/span&gt; - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</content><category term="Unix"></category><category term="Bioinformatics"></category><category term="genomic variation"></category><category term="entrez-direct"></category><category term="EDirect"></category></entry><entry><title>Setting Up Your Unix Computer for Bioinformatics Analysis</title><link href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html" rel="alternate"></link><published>2020-09-30T18:00:00-03:00</published><updated>2020-09-30T18:00:00-03:00</updated><author><name>Antonio Victor Campos Coelho</name></author><id>tag:antoniocampos13.github.io,2020-09-30:/setting-up-your-unix-computer-for-bioinformatics-analysis.html</id><summary type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post I will show how I set up my Unix machine to use Bioinformatics programs and tools. I am currently using Ubuntu 20.04 &lt;span class="caps"&gt;LTS&lt;/span&gt; (Focal Fossa) 64-bit on a &lt;a href="https://www.digitalocean.com/community/posts/trying-the-new-wsl-2-its-fast-windows-subsystem-for-linux"&gt;Windows Subsystem for Linux (&lt;span class="caps"&gt;WSL2&lt;/span&gt;)&lt;/a&gt; on Windows 10, so no &lt;span class="caps"&gt;GUI&lt;/span&gt;&amp;nbsp;today!&lt;/p&gt;
&lt;p&gt;The code and files used …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post I will show how I set up my Unix machine to use Bioinformatics programs and tools. I am currently using Ubuntu 20.04 &lt;span class="caps"&gt;LTS&lt;/span&gt; (Focal Fossa) 64-bit on a &lt;a href="https://www.digitalocean.com/community/posts/trying-the-new-wsl-2-its-fast-windows-subsystem-for-linux"&gt;Windows Subsystem for Linux (&lt;span class="caps"&gt;WSL2&lt;/span&gt;)&lt;/a&gt; on Windows 10, so no &lt;span class="caps"&gt;GUI&lt;/span&gt;&amp;nbsp;today!&lt;/p&gt;
&lt;p&gt;The code and files used here can be retrieved from &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Unix/2020-09-30_Setting%20Up%20Your%20Unix%20Computer%20for%20Bioinformatics%20Analysis"&gt;this post corresponding folder on my portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Preparing the&amp;nbsp;system&lt;/h2&gt;
&lt;p&gt;First, it is recommended that we upgrade the system. Open the command line terminal in your machine and copy and paste or type the following commands, pressing Enter after each one (make sure you type your password correctly whenever&amp;nbsp;asked):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sudo apt-get update
sudo apt-get upgrade
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then I must install some useful libraries, especially to be sure that all future libraries I need will be installed and work properly. Some of these (e.g. default-jdk, the Java libraries), may already be installed in your system, but just to&amp;nbsp;ensure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sudo apt-get install -y curl unzip build-essential ncurses-dev
sudo apt-get install -y byacc zlib1g-dev python-dev git cmake
sudo apt-get install -y default-jdk ant
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Installing&amp;nbsp;(mini)conda&lt;/h2&gt;
&lt;p&gt;Now I will install &lt;a href="https://conda.io/miniconda.html"&gt;miniconda&lt;/a&gt;. What is miniconda? Miniconda is a simplified version of Conda, an environment management system. Every program we install on our computers depend on other programs to work. So if a program X needs a program Y to work, it may stop working if Y gets an update that for some reason is incompatible with the original X&amp;nbsp;program.&lt;/p&gt;
&lt;p&gt;Thus, environments were developed to solve this kind of problem, because they serve to isolate groups of programs, ensuring only compatible versions of software are working together. Therefore, miniconda serves to create and manage environments. The best practice is that one should create one environment for one specific use. In my case, I installed miniconda to create a environment and populate it with tools used for several Bioinformatics analysis. Other people can create environments for other uses with specific programs needed and so on. Other advantage of miniconda is that the configuration files for environments can be shared with others, ensuring &lt;strong&gt;backup&lt;/strong&gt; and &lt;strong&gt;reproducibility&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Without further ado, let&amp;#8217;s finally install miniconda. Since I am using a Unix with Python 3.7.7 pre-installed, the version of the installer &lt;a href="https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"&gt;is this one&lt;/a&gt;. Check the &lt;a href="https://docs.conda.io/en/latest/miniconda.html#linux-installers"&gt;installation page&lt;/a&gt; if you have a different Python&amp;nbsp;version.&lt;/p&gt;
&lt;p&gt;You can download the installer from your browser or via command&amp;nbsp;line:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then, go to the folder where the installer was downloaded and run the&amp;nbsp;script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;bash Miniconda3-latest-Linux-x86_64.sh

./Miniconda3-latest-Linux-x86_64.sh &lt;span class="c1"&gt;# same effect&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When the installation finishes, I must initialize&amp;nbsp;conda:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;miniconda3/condabin/conda init
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Close the terminal and open it again. Now miniconda must be ready to use. Check by typing and pressing&amp;nbsp;Enter:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then, I added two &lt;strong&gt;channels&lt;/strong&gt;. Channels are &lt;a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-channels.html"&gt;&amp;#8220;the locations where packages are stored&amp;#8221;&lt;/a&gt;. Miniconda has the &lt;code&gt;defaults&lt;/code&gt; channel pre-configured. The two channels in question are dedicated to Bioinformatics and Data analysis programs, which may not be present in the default channels, so I must add&amp;nbsp;them.&lt;/p&gt;
&lt;h2&gt;Configuring miniconda&amp;nbsp;channels&lt;/h2&gt;
&lt;p&gt;Once again in the terminal enter the following&amp;nbsp;commands:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda config --add channels bioconda
conda config --add channels conda-forge
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Miniconda sets up priorities in the list of channels it receives. When we need to install some program, miniconda will search in the higher-priority channels first, then in the channels with lower-priority. &amp;#8220;Different channels can have the same package&amp;#8221; and you can &amp;#8220;safely put channels at the bottom of your channel list to provide additional packages that are not in the default channels&amp;#8221; as stated in the &lt;a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-channels.html"&gt;official website&lt;/a&gt;. The flag &lt;code&gt;--add&lt;/code&gt; adds the respective channels (&lt;code&gt;bioconda&lt;/code&gt; and &lt;code&gt;conda-forge&lt;/code&gt;) to the &lt;strong&gt;top&lt;/strong&gt; of the priorities list. If you want to give lower priority, putting them in the &lt;strong&gt;bottom&lt;/strong&gt; of the list, use the &lt;code&gt;--append&lt;/code&gt; command instead. Thus, according to the command above, the order of channel priorities in our new miniconda installation will be: &lt;code&gt;conda-forge&lt;/code&gt;, &lt;code&gt;bioconda&lt;/code&gt; and lastly, &lt;code&gt;defaults&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Create an environment for Bioinformatics&amp;nbsp;programs&lt;/h2&gt;
&lt;p&gt;Now that miniconda is configured, I will create the environment that will receive them. I will name it &lt;code&gt;bioenv&lt;/code&gt;. You can choose whatever name you&amp;nbsp;like! &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda create -y --name bioenv &lt;span class="nv"&gt;python&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;.6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Activating and deactivating an&amp;nbsp;environment&lt;/h2&gt;
&lt;p&gt;With the &lt;code&gt;bioenv&lt;/code&gt; created, I must &lt;strong&gt;activate&lt;/strong&gt;&amp;nbsp;it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda activate bioenv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I need to perform this step every time I want to use the programs that I will install in this environment. If you do not need to use the environment for the moment, simply &lt;strong&gt;deactivate&lt;/strong&gt;&amp;nbsp;it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda deactivate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Simply &lt;strong&gt;activate&lt;/strong&gt; it again when&amp;nbsp;needed.&lt;/p&gt;
&lt;h2&gt;Installing&amp;nbsp;programs&lt;/h2&gt;
&lt;p&gt;Now we can finally install our programs. Activate the environment again (only if you have deactivated it). Download the &lt;a href="https://raw.githubusercontent.com/antoniocampos13/portfolio/master/Unix/2020-09-30_Setting%20Up%20Your%20Unix%20Computer%20for%20Bioinformatics%20Analysis/bioenv.txt"&gt;&lt;code&gt;bioenv.txt&lt;/code&gt; file&lt;/a&gt; in my GitHub repository. This file contains a selection of most used Bioinformatics programs (hat tip to &lt;a href="https://www.biostarhandbook.com/index.html"&gt;Dr. István Albert&lt;/a&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;cat bioenv.txt &lt;span class="p"&gt;|&lt;/span&gt; xargs conda install -y
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Backing up and restoring your environment&amp;nbsp;configuration&lt;/h2&gt;
&lt;p&gt;Miniconda has a special command to backup your environment configuration. &lt;strong&gt;Activate&lt;/strong&gt; (if needed) the environment you want to backup and enter the&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda env &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep -v &lt;span class="s2"&gt;&amp;quot;prefix&amp;quot;&lt;/span&gt; &amp;gt; bioenv.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It will result in a &lt;code&gt;YAML&lt;/code&gt; file in the current working folder containing all configurations in your environment. Again, I named the file &lt;code&gt;bioenv.yml&lt;/code&gt; but you can choose whatever you like. Note that if you already have a &lt;code&gt;bioenv.yml&lt;/code&gt; in your directory, it will be overwritten, so be&amp;nbsp;careful.&lt;/p&gt;
&lt;p&gt;To restore this environment in your computer, or on other computer, first install miniconda again, and then use the&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda env create -f bioenv.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;-f&lt;/code&gt; flag means you are creating an environment using the configurations in the &lt;code&gt;bioenv.yml&lt;/code&gt; file. The first line of the yml file sets the new environment&amp;#8217;s name, so you can change it in the file if you like. It will also restore the channels configured in the previous installation of&amp;nbsp;miniconda.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This is how I configured my system so I could use the major Bioinformatics tools out there. In summary,&amp;nbsp;I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prepared an Unix (Ubuntu)&amp;nbsp;system;&lt;/li&gt;
&lt;li&gt;Installed miniconda, an environment&amp;nbsp;manager;&lt;/li&gt;
&lt;li&gt;Configured channels so I could retrieve desired&amp;nbsp;software;&lt;/li&gt;
&lt;li&gt;Created an environment, showed how to activate and deactivate it, and finally installed software in&amp;nbsp;it;&lt;/li&gt;
&lt;li&gt;Showed how to backup your environment for safekeeping or sharing with&amp;nbsp;others.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In future posts I will demo some uses of the installed programs I  in the new&amp;nbsp;environment.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/antoniocampos13/portfolio"&gt;My&amp;nbsp;Portfolio&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.digitalocean.com/community/posts/trying-the-new-wsl-2-its-fast-windows-subsystem-for-linux"&gt;Trying the New &lt;span class="caps"&gt;WSL&lt;/span&gt; 2. It&amp;#8217;s Fast! (Windows Subsystem for Linux) |&amp;nbsp;DigitalOcean&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://conda.io/miniconda.html"&gt;Miniconda&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"&gt;Miniconda&amp;nbsp;installer&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.conda.io/en/latest/miniconda.html#linux-installers"&gt;Miniconda &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Conda&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-channels.html"&gt;Managing channels; conda 4.8.4.post65+1a0ab046&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.biostarhandbook.com/index.html"&gt;The Biostar Handbook: 2nd&amp;nbsp;Edition&lt;/a&gt;&lt;/p&gt;</content><category term="Unix"></category><category term="Bioinformatics"></category></entry></feed>