<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>Antonio's Portfolio</title><link>https://antoniocampos13.github.io/</link><description>PhD in Genetics</description><lastBuildDate>Sat, 29 Jul 2023 17:43:00 -0300</lastBuildDate><item><title>Parallelization withÂ R</title><link>https://antoniocampos13.github.io/parallelization-with-r.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Sometimes, some computations can be carried out in parallel. Certain large tasks can be divided into independent ones, allowing them to be solved at the same time, rather than waiting for each task to be solved&amp;nbsp;sequentially.&lt;/p&gt;
&lt;p&gt;I find the native R parallel functions such as &lt;code&gt;mclapply()&lt;/code&gt;, or those from highly used packages such as &lt;code&gt;snow&lt;/code&gt; to be cumbersome. Frankly, I assume I may never get those approaches to&amp;nbsp;work.&lt;/p&gt;
&lt;p&gt;This is why I got very happy when I recently discovered the &lt;code&gt;furrr&lt;/code&gt; package. This package is from the &lt;code&gt;tidyverse&lt;/code&gt; family, and as such, it is easy to use. With &lt;code&gt;furrr&lt;/code&gt;, all the mental gymnastics I used to have when I tried parallelization in R is&amp;nbsp;over.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;furrr&lt;/code&gt; package is the &amp;#8220;marriage&amp;#8221; between &lt;code&gt;purrr&lt;/code&gt; and &lt;code&gt;future&lt;/code&gt; packages. It has versions of the main mapping functions from &lt;code&gt;purrr&lt;/code&gt;, but using &lt;code&gt;future&lt;/code&gt; backend to execute computations in parallel and&amp;nbsp;asynchronously.&lt;/p&gt;
&lt;p&gt;In this post, I will demonstrate a simple way to achieve parallelization with R and &lt;code&gt;furrr&lt;/code&gt;. The code of this demo is in my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/R/2023_07_31_Parallelization_with_R"&gt;portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I will parallelize a series of differential expression analyses (&lt;span class="caps"&gt;DEA&lt;/span&gt;) of a dataset by Li et al. (2022). Check the Gene Expression Omnibus (&lt;span class="caps"&gt;GEO&lt;/span&gt;) summary page &lt;a href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE219278"&gt;here&lt;/a&gt;. Briefly, they investigated the cellular alterations associated with the &lt;em&gt;C9orf72&lt;/em&gt; gene pathogenic repeat expansions. They performed single nucleus transcriptomics (snRNA-seq) in postmortem samples of motor and frontal cortices from amyotrophic lateral sclerosis (&lt;span class="caps"&gt;ALS&lt;/span&gt;) and frontotemporal dementia (&lt;span class="caps"&gt;FTD&lt;/span&gt;) donors as well as unaffected controls. They sampled three major brain cell populations (neurons, oligodendrocytes, and other glia) from both of the&amp;nbsp;cortices.&lt;/p&gt;
&lt;h2&gt;Installing&amp;nbsp;packages&lt;/h2&gt;
&lt;p&gt;Install the following packages into your R library with the &lt;code&gt;src/installPackages.R&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### src/installPackages.R ###&lt;/span&gt;
&lt;span class="n"&gt;packages&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;
  &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;here&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;tidyverse&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;glue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;openxlsx&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;future&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;future.callr&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;devtools&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;tictoc&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;BiocManager&amp;quot;&lt;/span&gt;
  &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;lapply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;packages&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pkg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nf"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="n"&gt;pkg&lt;/span&gt; &lt;span class="o"&gt;%in%&lt;/span&gt; &lt;span class="nf"&gt;rownames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;installed.packages&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;

    &lt;span class="nf"&gt;install.packages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pkg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="p"&gt;})&lt;/span&gt;

&lt;span class="n"&gt;biocPackages&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;
  &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;edgeR&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;AnnotationDbi&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;annotate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;org.Hs.eg.db&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;EnsDb.Hsapiens.v79&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;ensembldb&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;lapply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;biocPackages&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pkg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nf"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="n"&gt;pkg&lt;/span&gt; &lt;span class="o"&gt;%in%&lt;/span&gt; &lt;span class="nf"&gt;rownames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;installed.packages&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;

    &lt;span class="n"&gt;BiocManager&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;install&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pkg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I create an &lt;code&gt;outputs&lt;/code&gt; folder to hold the &lt;span class="caps"&gt;DEA&lt;/span&gt;&amp;nbsp;results:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### main.R ###&lt;/span&gt;
&lt;span class="c1"&gt;# Paths ----&lt;/span&gt;
&lt;span class="nf"&gt;if &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="nf"&gt;dir.exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;outputs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nf"&gt;dir.create&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;outputs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I load all the functions I created for this&amp;nbsp;demo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### main.R ###&lt;/span&gt;
&lt;span class="c1"&gt;# Scripts ----&lt;/span&gt;
&lt;span class="nf"&gt;source&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;src&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;functions.R&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This script will load the following scripts in the &lt;code&gt;functions&lt;/code&gt; folder: &lt;code&gt;makeCountsDf.R&lt;/code&gt;, &lt;code&gt;sourceFromGitHub.R&lt;/code&gt;, and &lt;code&gt;runParallelDEA.R&lt;/code&gt;. I will explain each one at the appropriate&amp;nbsp;moment.&lt;/p&gt;
&lt;h2&gt;Obtaining and preparing the&amp;nbsp;data&lt;/h2&gt;
&lt;p&gt;The next step is executing the &lt;code&gt;prepareData.R&lt;/code&gt; script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### main.R ###&lt;/span&gt;
&lt;span class="nf"&gt;source&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;src&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;prepareData.R&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The script will start by loading the gene expression count data directly from the &lt;span class="caps"&gt;GEO&lt;/span&gt; &lt;span class="caps"&gt;FTP&lt;/span&gt; server into a data frame, and rounding up any number to the nearest integer just to be safe, since &lt;code&gt;edgeR&lt;/code&gt; requires raw, integer&amp;nbsp;counts:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### src/prepareData.R ###&lt;/span&gt;
&lt;span class="c1"&gt;# Load data ----&lt;/span&gt;
&lt;span class="n"&gt;countDataOriginal&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;read_tsv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;https://ftp.ncbi.nlm.nih.gov/geo/series/GSE219nnn/GSE219278/suppl/GSE219278_allSamples_rsem_genes_results_counts_annotated.tsv.gz&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Round up gene counts ----&lt;/span&gt;
&lt;span class="n"&gt;countData&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;countDataOriginal&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;across&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is.numeric&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;ceiling&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This data frame has 60656 rows (genes/transcripts) and 117 columns. The first three columns are gene information: &lt;code&gt;gene_ID&lt;/code&gt;, &lt;code&gt;gene_name&lt;/code&gt;, and &lt;code&gt;gene_type&lt;/code&gt;, leaving 114 columns to represent the samples (column indexes between 4 and&amp;nbsp;117).&lt;/p&gt;
&lt;p&gt;&lt;img alt="countData data frame view" src="https://antoniocampos13.github.io/images/count_data_C9orf72.png"&gt;&lt;/p&gt;
&lt;p&gt;Thankfully, the project authors gave very descriptive names to the columns, so I could identify right away their study design. I created a data frame named &lt;code&gt;sampleInfo&lt;/code&gt; using the column names to extract information from each&amp;nbsp;sample:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### src/prepareData.R ###&lt;/span&gt;
&lt;span class="c1"&gt;# Identify samples and outcomes ----&lt;/span&gt;
&lt;span class="n"&gt;sampleInfo&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;tibble&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;countData&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="nf"&gt;length&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;countData&lt;/span&gt;&lt;span class="p"&gt;))])&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;separate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;into&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;outcome&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;patientId&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;location&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;cellType&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columnIndex&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;row_number&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="sampleInfo data frame view" src="https://antoniocampos13.github.io/images/sampleInfo_C9orf72.png?raw=true"&gt;&lt;/p&gt;
&lt;p&gt;With the &lt;code&gt;columnIndex&lt;/code&gt; column I created on the &lt;code&gt;sampleInfo&lt;/code&gt;, I can extract the necessary cases/control combinations to generate 12 distinct data frames, by splitting the &lt;code&gt;countData&lt;/code&gt; data&amp;nbsp;frame.&lt;/p&gt;
&lt;p&gt;By observing this table, I surmised they recruited three outcome groups: &amp;#8220;&lt;span class="caps"&gt;C9ALS&lt;/span&gt;&amp;#8221;, &amp;#8220;&lt;span class="caps"&gt;C9FTD&lt;/span&gt;&amp;#8221; and &amp;#8220;Control&amp;#8221;, with six, seven, and six individuals in each group, respectively. Since they collect three cell types from two cerebral cortices, we have six cortex/cell type combinations. Coupling with two distinct diseases, we can perform two &lt;strong&gt;case vs. control comparisons&lt;/strong&gt; per combination. Therefore, we can perform $3 \times 2 \times 2 = 12$ &lt;strong&gt;distinct DEAs&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;To keep track of the samples in each of the 12 analyses, I created two lists of data frames, containing cases and controls, stratified by cortex and cell&amp;nbsp;type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### src/prepareData.R ###&lt;/span&gt;
&lt;span class="n"&gt;alsDfsList&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;sampleInfo&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span class="n"&gt;dplyr&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outcome&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;C9ALS&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;outcome&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Control&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cellType&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;group_split&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;ftdDfsList&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;sampleInfo&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span class="n"&gt;dplyr&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outcome&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;C9FTD&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="n"&gt;outcome&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Control&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cellType&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;group_split&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I concatenate the two lists&amp;nbsp;together:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### src/prepareData.R ###&lt;/span&gt;
&lt;span class="n"&gt;allAnalysesList&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alsDfsList&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ftdDfsList&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, I can use the &lt;code&gt;makeCountsDf()&lt;/code&gt; function to generate the 12 distinct gene count data frames. The function inputs are a data frame and a vector of column indexes, so it can convert the &lt;code&gt;gene_ID&lt;/code&gt; into row names of the data frame and select the desired sample&amp;nbsp;columns:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### src/functions/makeCountsDf.R ###&lt;/span&gt;
&lt;span class="n"&gt;makeCountsDf&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataFrame&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columnIndexes&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

  &lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;dataFrame&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span class="n"&gt;dplyr&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;all_of&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columnIndexes&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span class="nf"&gt;rename_with&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;str_replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;.x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;C9ALS|C9FTD&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;case&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span class="nf"&gt;rename_with&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;str_replace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;.x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Control&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;control&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
    &lt;span class="nf"&gt;as.data.frame&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; 

  &lt;span class="nf"&gt;row.names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;gene_ID&lt;/span&gt;

  &lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;subset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;select&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene_ID&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

  &lt;span class="nf"&gt;return&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I use a simple &lt;code&gt;lapply&lt;/code&gt; loop to finally generate the &lt;code&gt;countsDfsList&lt;/code&gt; object, which is a list of 12 data frames, each with the necessary samples for each distinct &lt;span class="caps"&gt;DEA&lt;/span&gt; as mentioned&amp;nbsp;above.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### src/prepareData.R ###&lt;/span&gt;
&lt;span class="c1"&gt;# Split count data into 12 dataframes for differential expression analysis with edgeR ----&lt;/span&gt;
&lt;span class="n"&gt;countsDfsList&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;lapply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;seq_along&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;allAnalysesList&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

  &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;allAnalysesList&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;

  &lt;span class="n"&gt;cIdx&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;columnIndex&lt;/span&gt;

  &lt;span class="nf"&gt;makeCountsDf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataFrame&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;countData&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;columnIndexes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cIdx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To keep track of each analysis, I named each element with a string contaning the disease, cortex and cell type by creating a string column combining the corresponding &lt;code&gt;sampleInfo&lt;/code&gt; columns and &lt;code&gt;pull&lt;/code&gt;ing it to create a simple vector. With the vector, I set the &lt;code&gt;countsDfsList&lt;/code&gt; element&amp;nbsp;names:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### src/prepareData.R ###&lt;/span&gt;
&lt;span class="n"&gt;allAnalysesNames&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;sampleInfo&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="n"&gt;dplyr&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outcome&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Control&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="n"&gt;dplyr&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outcome&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cellType&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;distinct&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;glue&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;{outcome}_{location}_{cellType}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;pull&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;countsDfsList&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;allAnalysesNames&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;In summary&lt;/em&gt;: I got the gene counts &lt;code&gt;countData&lt;/code&gt;, and split it into 12 distinct data frames, stratifying by disease, cortex and cell type. Each one will allow a &lt;span class="caps"&gt;DEA&lt;/span&gt; with the &lt;code&gt;edgeR&lt;/code&gt; package.&lt;/p&gt;
&lt;h2&gt;The &lt;code&gt;runParallelDEA()&lt;/code&gt; function&lt;/h2&gt;
&lt;p&gt;By executing the &lt;code&gt;sourceFromGitHub.R&lt;/code&gt; script, I sourced my &lt;code&gt;edgeR_setup&lt;/code&gt; function directly from my GitHub&amp;nbsp;portfolio:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### src/functions/sourceFromGitHub.R ###&lt;/span&gt;
&lt;span class="n"&gt;URLs&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;https://raw.githubusercontent.com/antoniocampos13/portfolio/master/R/2020_10_22_DEA_with_edgeR/src/edgeR_setup.R&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;lapply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;URLs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;devtools&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;source_url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In a &lt;a href="https://antoniocampos13.github.io/differential-expression-analysis-with-edger-in-r.html"&gt;previous post&lt;/a&gt; I demonstrated the &lt;code&gt;edger_setup&lt;/code&gt; custom function that uses &lt;code&gt;edgeR&lt;/code&gt; package to perform differential expression&amp;nbsp;analysis.&lt;/p&gt;
&lt;p&gt;Since I have 12 &lt;strong&gt;independent&lt;/strong&gt; DEAs to perform, I surmised I could parallelize the computation, so I created the &lt;code&gt;run&lt;/code&gt; function to demonstrate that sometimes parallel computation allows us to complete some tasks faster than if we executed them sequentially. Check the function&amp;nbsp;code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### src/functions/runParallelDEA.R ###&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;glue&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;furrr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;future.callr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tictoc&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;runParallelDEA&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nWorkers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

  &lt;span class="nf"&gt;plan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;callr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;workers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;nWorkers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="n"&gt;opts&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;furrr_options&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;scheduling&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;seed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="n"&gt;parallelFileNames&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;glue&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;{names(countsDfsList)}_parallel_{nWorkers}_workers.xlsx&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="n"&gt;logMessage&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;ifelse&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nWorkers&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Sequential execution&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;glue&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Parallel execution with {nWorkers} workers&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

  &lt;span class="nf"&gt;tic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logMessage&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nf"&gt;future_walk2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;.x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;parallelFileNames&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;.y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;countsDfsList&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;edger_setup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;which&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;parallelFileNames&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;.x&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
      &lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;.y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="n"&gt;gene_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ENSEMBL&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="n"&gt;output_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;outputs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;.x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;.options&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;opts&lt;/span&gt;
  &lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nf"&gt;toc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;parallelTime&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;unlist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;tic.log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="nf"&gt;tic.clearlog&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

  &lt;span class="n"&gt;parallelTime&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="nf"&gt;write_lines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;outputs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;tictoc_log.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;append&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;file.exists&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;outputs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;tictoc_log.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It is a convenient function around the execution of &lt;code&gt;furrr&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;future_walk2&lt;/code&gt; function based on the number of parallel workers (&lt;span class="caps"&gt;CPU&lt;/span&gt; cores). First, let me explain &lt;code&gt;future_walk2&lt;/code&gt;: it applies a function (in this case, &lt;code&gt;edger_setup&lt;/code&gt;) to each element of two vectors of the same length in parallel using the &lt;code&gt;futures&lt;/code&gt; package backend. The number of &lt;strong&gt;simultaneous, parallel executions&lt;/strong&gt; of &lt;code&gt;edger_setup&lt;/code&gt; is controlled by the &lt;code&gt;nWorkers&lt;/code&gt; parameter, which is passed over to the &lt;code&gt;plan()&lt;/code&gt; function, which will use the &lt;code&gt;future.callr&lt;/code&gt; &lt;span class="caps"&gt;API&lt;/span&gt; to finetune the &lt;code&gt;futures&lt;/code&gt; package&amp;nbsp;backend.&lt;/p&gt;
&lt;p&gt;I chose &lt;code&gt;future_walk2&lt;/code&gt; because &lt;code&gt;edger_setup&lt;/code&gt; does not return any value to the console; it just writes a spreadsheet with results directly to disk. Thus, I am interested in the &amp;#8220;side-effect&amp;#8221; of the function, which is exactly the purpose of &lt;code&gt;walk&lt;/code&gt;-like&amp;nbsp;functions.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;edger_setup&lt;/code&gt; has three required inputs: &lt;code&gt;name&lt;/code&gt;, &lt;code&gt;counts&lt;/code&gt; and &lt;code&gt;output_path&lt;/code&gt;. The &lt;code&gt;counts&lt;/code&gt; parameter will receive each data frame stored in the &lt;code&gt;countsDfsList&lt;/code&gt; list. With the &lt;code&gt;parallelFileNames&lt;/code&gt; vector inside the function, I could satisfy both the &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;output_path&lt;/code&gt; parameters.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In summary:&lt;/em&gt; the &lt;code&gt;future_walk2&lt;/code&gt; will map over two vectors (&lt;code&gt;countsDfsList&lt;/code&gt; and &lt;code&gt;parallelFileNames&lt;/code&gt;) and pass each element of both vectors simultaneously over to &lt;code&gt;edger_setup&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Performing differential expression analysis (&lt;span class="caps"&gt;DEA&lt;/span&gt;) sequentially and in&amp;nbsp;parallel&lt;/h2&gt;
&lt;p&gt;Now that I explained the logic behind the &lt;code&gt;runParallelDEA()&lt;/code&gt; function, I can finally demonstrate how parallelization may accelerate the completion time of certain tasks. To this end, I created a numerical&amp;nbsp;vector:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;### main.R ###&lt;/span&gt;
&lt;span class="c1"&gt;# Run DEA  ----&lt;/span&gt;
&lt;span class="n"&gt;nWorkers&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;lapply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nWorkers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;runParallelDEA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Each number represents the number of workers (&lt;span class="caps"&gt;CPU&lt;/span&gt; cores) that I will loop over with &lt;code&gt;lapply&lt;/code&gt; to pass over to &lt;code&gt;runParallelDEA()&lt;/code&gt;. Six is the maximum number in the vector because my &lt;span class="caps"&gt;PC&lt;/span&gt; has this many&amp;nbsp;cores.&lt;/p&gt;
&lt;p&gt;Observe that the first element is &lt;code&gt;1&lt;/code&gt;: it will boil over to a sequential execution, since &lt;code&gt;future_walk2&lt;/code&gt; will use a single worker to process all 12 DEAs with &lt;code&gt;edger_setup&lt;/code&gt;. The &lt;code&gt;runParallelDEA()&lt;/code&gt; will count the number of seconds elapsed to complete all 12 DEAs and save it to the &lt;code&gt;outputs/tictoc_log.txt&lt;/code&gt; file, displayed&amp;nbsp;below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;### outputs/tictoc_log.txt ###
Sequential execution: 52.57 sec elapsed
Parallel execution with 2 workers: 35.78 sec elapsed
Parallel execution with 4 workers: 28.98 sec elapsed
Parallel execution with 6 workers: 26.89 sec elapsed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Notice how with six cores, the elapsed time to produce all 12 result spreadsheets is cut short by half! Now imagine the time gained in the calculation of more complex tasks, provided plenty of &lt;span class="caps"&gt;CPU&lt;/span&gt; cores and &lt;span class="caps"&gt;RAM&lt;/span&gt; size. Tasks that would run for several days or weeks if done sequentially can be performed in much shorter times if they are amenable to&amp;nbsp;parallelization.&lt;/p&gt;
&lt;p&gt;You can check the 12 spreadsheets and the &lt;code&gt;tictoc&lt;/code&gt; log at the &lt;code&gt;outputs&lt;/code&gt; folder (I included just only the set of spreadsheets produced by the 6 workers to avoid duplicate files on my GitHub&amp;nbsp;portfolio).&lt;/p&gt;
&lt;p&gt;Now that you know about the &lt;code&gt;furrr&lt;/code&gt; package through my post, you can use it as a inspiration to try parallel computing with your use&amp;nbsp;cases.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I demonstrated the &lt;code&gt;furrr&lt;/code&gt; package for easy parallelization of tasks within&amp;nbsp;R.&lt;/li&gt;
&lt;li&gt;The &lt;code&gt;future.callr&lt;/code&gt; package finetunes the &lt;code&gt;future&lt;/code&gt; backend used by &lt;code&gt;furrr&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Some tasks are completed quicker if they are amenable to&amp;nbsp;parallelization&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE219278"&gt;&lt;span class="caps"&gt;GEO&lt;/span&gt; Accession viewer | &lt;span class="caps"&gt;GSE219278&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/differential-expression-analysis-with-edger-in-r.html"&gt;Differential Expression Analysis with edgeR in&amp;nbsp;R&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Sat, 29 Jul 2023 17:43:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2023-07-29:/parallelization-with-r.html</guid><category>R</category><category>Bioinformatics</category><category>gene expression</category><category>edgeR</category><category>furrr</category></item><item><title>Genomic plots withÂ circlize</title><link>https://antoniocampos13.github.io/genomic-plots-with-circlize.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Genomics is undoubtedly a complex science. The human genome is huge, with more than 3 billion base pairs, about 20,000 protein-coding genes, several millions of variants, and many more interesting characteristics. The visualization of genomic/omics data is challenging due to the sheer volume of information. Circular plots are a popular way to extract information at a glance from omics-level&amp;nbsp;information.&lt;/p&gt;
&lt;p&gt;In this post, I will demonstrate the &lt;a href="https://jokergoo.github.io/circlize_book/book/"&gt;&lt;code&gt;circlize&lt;/code&gt; package&lt;/a&gt; from R software, a versatile tool with applications in&amp;nbsp;Genomics.&lt;/p&gt;
&lt;p&gt;As always, I will post the code of this demo in my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/R/2023_04_21_Genomic_plots_with_circlize"&gt;portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Necessary&amp;nbsp;packages&lt;/h2&gt;
&lt;p&gt;I will use the following R&amp;nbsp;packages:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tidyverse&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;glue&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;circlize&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Install any of them with the function &lt;code&gt;install.packages()&lt;/code&gt;, for&amp;nbsp;example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;install.packages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;tidyverse&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;version&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;gt;= 1.5.0&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# we need this version or later&lt;/span&gt;
&lt;span class="nf"&gt;install.packages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;glue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;install.packages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;circlize&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Preparing the&amp;nbsp;data&lt;/h2&gt;
&lt;p&gt;In this demo, I will create a simple circular plot showing the mean number of pathogenic variants per 100,000 base pairs windows (intervals) in the human genome, as well as showing regions involved with segmental&amp;nbsp;duplication.&lt;/p&gt;
&lt;p&gt;Thus, I create a variable named &lt;code&gt;intervalWidth&lt;/code&gt; representing the 100,000 bp intervals to use later in the&amp;nbsp;script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Magic numbers ----&lt;/span&gt;
&lt;span class="n"&gt;intervalWidth&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;1e5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is a fancy way to write the number 100,000. Then, I create two variables to hold the web links containing the data. The first one is the ClinVar variant summary. I will extract the pathogenic variants&amp;#8217; coordinates from it and calculate the mean number of them in each interval of the human&amp;nbsp;genome.&lt;/p&gt;
&lt;p&gt;The second one is a dataset from &lt;span class="caps"&gt;UCSC&lt;/span&gt; storing the coordinates of segmental duplication regions in the human&amp;nbsp;genome.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Dataset links ----&lt;/span&gt;
&lt;span class="n"&gt;variantSummaryPath&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;genomicSuperDupsPath&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;https://hgdownload.soe.ucsc.edu/goldenPath/hg38/database/genomicSuperDups.txt.gz&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Preparing the ClinVar&amp;nbsp;data&lt;/h2&gt;
&lt;p&gt;Let&amp;#8217;s import the ClinVar dataset and keep only germline, pathogenic, single nucleotide variants mapped in the GRCh38 assembly, excluding those in the mitochondrial&amp;nbsp;chromosome:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;pathogenicSNVs&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;variantSummary&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;OriginSimple&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;germline&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Assembly&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;GRCh38&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ClinicalSignificance&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Pathogenic&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Type&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;single nucleotide variant&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Chromosome&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;MT&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="c1"&gt;# continues in the next codeblock&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I create new columns to help me link with the segmental duplication dataset. I create a new column named &lt;code&gt;chr&lt;/code&gt; appending the string &amp;#8220;chr&amp;#8221; to each chromosome name, and another column, &lt;code&gt;posInterval&lt;/code&gt; to put each variant inside a 100,000 base pair window as I explained earlier. To this end, I group the data by chromosome, so each chromosome has their&amp;nbsp;windows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;glue&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;chr{Chromosome}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;posInterval&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;cut_width&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Start&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;width&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;intervalWidth&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;boundary&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;ungroup&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="c1"&gt;# continues in the next codeblock&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I count how many variants there are per&amp;nbsp;interval:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;posInterval&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;summarise&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;nVariants&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;n&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;ungroup&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="c1"&gt;# continues in the next codeblock&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I create one more column (&lt;code&gt;newStart&lt;/code&gt;) to mark the starting coordinate of each interval, to help me plot in the circular layout&amp;nbsp;later:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;  &lt;span class="nf"&gt;rowwise&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;newStart&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;as.integer&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;str_remove&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="nf"&gt;str_split_1&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;posInterval&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;\\[|\\(&amp;quot;&lt;/span&gt;
  &lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;ungroup&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This last step involves a bit of string manipulation. The &lt;code&gt;posInterval&lt;/code&gt; is created with &lt;code&gt;factor&lt;/code&gt; type. I must convert it to &lt;code&gt;character&lt;/code&gt; (string) so I can extract the first coordinate of the interval by splitting it at the comma delimiter with the help of the &lt;code&gt;str_split_1()&lt;/code&gt; function. Then, I remove &lt;code&gt;[&lt;/code&gt; or &lt;code&gt;(&lt;/code&gt; characters from the string with the &lt;code&gt;str_remove()&lt;/code&gt; function. Finally, I convert the string into &lt;code&gt;integer&lt;/code&gt; type with the &lt;code&gt;as.integer()&lt;/code&gt; function.&lt;/p&gt;
&lt;h2&gt;Preparing the segmental duplication&amp;nbsp;data&lt;/h2&gt;
&lt;p&gt;I import the segmental duplication dataset into the object &lt;code&gt;genomicSuperDups&lt;/code&gt; and explicitly name the columns with the &lt;a href="http://genome.ucsc.edu/cgi-bin/hgTables?db=hg38&amp;amp;hgta_group=rep&amp;amp;hgta_track=genomicSuperDups&amp;amp;hgta_table=genomicSuperDups&amp;amp;hgta_doSchema=describe+table+schema"&gt;original names from &lt;span class="caps"&gt;UCSC&lt;/span&gt;&lt;/a&gt; since the &lt;span class="caps"&gt;BED&lt;/span&gt; format does not have header&amp;nbsp;names:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Import segmental duplication dataset ----&lt;/span&gt;
&lt;span class="n"&gt;genomicSuperDups&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;read_tsv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;genomicSuperDupsPath&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;col_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;bin&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;chrom&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;chromStart&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;chromEnd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;score&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;strand&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;otherChr&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;otherStart&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;otherEnd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;otherSize&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;uid&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;posBasesHit&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;testResult&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;verdict&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;chits&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;ccov&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;alignfile&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;alignL&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;indelN&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;indelS&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;alignB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;matchB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;mismatchB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;transitionsB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;transversionsB&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;fracMatch&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;fracMatchIndel&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;jcK&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s"&gt;&amp;quot;k2K&amp;quot;&lt;/span&gt;
  &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, some filtering. I remove any duplication involving alternative/decoys&amp;nbsp;chromosomes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;## Extract coordinates ----&lt;/span&gt;
&lt;span class="n"&gt;genomicSuperDupsFiltered&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;genomicSuperDups&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="nf"&gt;str_detect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chrom&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;random|chrUn&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="nf"&gt;str_detect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;otherChr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;random|chrUn&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, I create two data frames representing the pairs of coordinates (origin/target) involved in the&amp;nbsp;duplications:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;bed1&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;genomicSuperDups&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chrom&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;chromStart&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;chromEnd&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;setNames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;chr&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;start&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;end&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;bed2&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;genomicSuperDups&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;otherChr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;otherStart&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;otherEnd&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;setNames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;chr&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;start&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;end&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I rename the column names so &lt;code&gt;circlize&lt;/code&gt; can recognize them&amp;nbsp;later.&lt;/p&gt;
&lt;h2&gt;Preparing color helper&amp;nbsp;function&lt;/h2&gt;
&lt;p&gt;I will now create a helper function to color the plot according to the number of pathogenic variants in each interval. Thus, each interval will be colored according to the mean number of pathogenic variants in the whole genome. The mean number will be colored white, and regions with fewer variants than the mean will be colored with shades of blue, whereas regions with more variants will be colored with shades of&amp;nbsp;red.&lt;/p&gt;
&lt;p&gt;To this end, I will scale the limits of the number of variants with their&amp;nbsp;logarithm:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Create color function ----&lt;/span&gt;
&lt;span class="n"&gt;minVariants&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;log10&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;floor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pathogenicSNVs&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;nVariants&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="n"&gt;meanVariants&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;log10&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;floor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pathogenicSNVs&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;nVariants&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="n"&gt;maxVariants&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;log10&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;ceiling&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pathogenicSNVs&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;nVariants&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;

&lt;span class="n"&gt;colorFunction&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;
  &lt;span class="nf"&gt;colorRamp2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;minVariants&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;meanVariants&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;maxVariants&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
             &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;blue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;white&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;red&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Making the &lt;code&gt;circlize&lt;/code&gt; plot&lt;/h2&gt;
&lt;p&gt;Everything is ready for making the &lt;code&gt;circlize&lt;/code&gt; plot. I will save the output to disk in a &lt;span class="caps"&gt;TIFF&lt;/span&gt; format with the specified dimensions in centimeters and with a resolution of 300&amp;nbsp;dpi:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;tiff&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="s"&gt;&amp;quot;circlize_demo.tiff&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;units&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;cm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;width&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;17.35&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;23.35&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;pointsize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;18&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;300&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;compression&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;lzw&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After declaring the output format, I can run the commands that will construct the circular&amp;nbsp;plot:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;circos.par&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;start.degree&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;90&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first command is purely cosmetic: it tells &lt;code&gt;circlize&lt;/code&gt; to rotate the layout 90 degrees, so chromosome 1 will appear approximately at the top of the plot, with the remaining chromosomes following in a clockwise&amp;nbsp;manner.&lt;/p&gt;
&lt;p&gt;The next command creates the first track of the plot, with ideograms representing the cytobands of each chromosome. Observe the &lt;code&gt;species&lt;/code&gt; argument value - the &lt;code&gt;circlize&lt;/code&gt; default is the GRCh37 assembly, therefore we must configure it to use the&amp;nbsp;GRCh38:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;circos.initializeWithIdeogram&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;species&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;hg38&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The third command creates the second track of the plot. This track will be a simple color plot. The &lt;code&gt;ylim&lt;/code&gt; represents a Y-axis with limits going from 0 to 100. The X-axis will correspond to chromosome coordinates. Each chromosome is a sector in this&amp;nbsp;track.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;circos.track&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ylim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, I draw the plot with colored lines. Each line will represent one 100,00 bp interval and the color represent the density of pathogenic variants within the interval. Regions with fewer variants than the mean will have blue shades, whereas variant-rich regions will have red&amp;nbsp;shades.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;circos.trackLines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;pathogenicSNVs&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;chr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pathogenicSNVs&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;newStart&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;rep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;nrow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pathogenicSNVs&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
  &lt;span class="n"&gt;type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;h&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;colorFunction&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;log10&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pathogenicSNVs&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;nVariants&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first argument represents the chromosomes (sectors), &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; the line coordinates. Since I wanted each line spanning the complete Y-axis length, I simply repeat the number 100 &lt;em&gt;N&lt;/em&gt; times (with the function &lt;code&gt;rep()&lt;/code&gt;), in which &lt;em&gt;N&lt;/em&gt; represents the number of rows in the &lt;code&gt;pathogenicSNVs&lt;/code&gt; dataset, obtained by using the &lt;code&gt;nrow()&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;Finally, I invoke the final command for the plot: it will create links in the center of the plot, connecting the regions involved with segmental&amp;nbsp;duplications:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;circos.genomicLink&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bed1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bed2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;coral&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;border&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;NA&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As soon as I execute the command above, I execute the following two commands so that R can save the &lt;span class="caps"&gt;TIFF&lt;/span&gt; image to disk and reset &lt;code&gt;circlize&lt;/code&gt;, readying it for generating other&amp;nbsp;plots.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;dev.off&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nf"&gt;circos.clear&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The plotting process may take a while since we are dealing with quite a number of datapoints. This is the&amp;nbsp;result:&lt;/p&gt;
&lt;p&gt;&lt;img alt="circlize plot: pathogenic variants and segmental duplications" src="https://antoniocampos13.github.io/images/circlize_demo.png"&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I demonstrated the &lt;code&gt;circlize&lt;/code&gt; package for producing interesting circular plots, which are specially used for visualizing complex genomic&amp;nbsp;information.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://jokergoo.github.io/circlize_book/book/"&gt;Circular Visualization in&amp;nbsp;R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://genome.ucsc.edu/cgi-bin/hgTables?db=hg38&amp;amp;hgta_group=rep&amp;amp;hgta_track=genomicSuperDups&amp;amp;hgta_table=genomicSuperDups&amp;amp;hgta_doSchema=describe+table+schema"&gt;&lt;span class="caps"&gt;UCSC&lt;/span&gt; Genome Browser | Schema for Segmental&amp;nbsp;Dups&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Sat, 29 Apr 2023 10:00:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2023-04-29:/genomic-plots-with-circlize.html</guid><category>R</category><category>circlize</category><category>genomics</category><category>data visualization</category></item><item><title>Parsing the ClinVar XML file withÂ pandas</title><link>https://antoniocampos13.github.io/parsing-the-clinvar-xml-file-with-pandas.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/intro/"&gt;ClinVar&lt;/a&gt; is one of the &lt;span class="caps"&gt;USA&lt;/span&gt;&amp;#8217;s National Center for Biotechnology Information (&lt;span class="caps"&gt;NCBI&lt;/span&gt;) databases. ClinVar archives reports of relationships among human genetic variants and phenotypes (usually genetic disorders). Any organization, such as a laboratory, hospital, clinic etc can submit data to ClinVar. The core idea of ClinVar is aggregate evidence for the clinical significance of any genetic variant concerning any disorder. Over 2,400 organizations contributed more than 2 million 600 thousand &lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/submitters/"&gt;records to ClinVar&lt;/a&gt;, representing more than 1 million 600 thousand unique&amp;nbsp;variants.&lt;/p&gt;
&lt;p&gt;Anyone can freely search ClinVar through their &lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/"&gt;website&lt;/a&gt;, using gene symbols, genomic coordinates, &lt;a href="https://varnomen.hgvs.org/"&gt;&lt;span class="caps"&gt;HGVS&lt;/span&gt; expressions&lt;/a&gt;, phenotypes, and more. If you want to perform a few queries, the online search tool does a good job. However, if you are pursuing more complex scientific questions, or are intending to download batches of data, the search tool will not suffice. Other &lt;span class="caps"&gt;NCBI&lt;/span&gt; databases can be queried via the command line with the &lt;a href="https://www.ncbi.nlm.nih.gov/books/NBK179288/"&gt;Entrez Direct (EDirect) utilities&lt;/a&gt; (in a &lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-2.html"&gt;previous post&lt;/a&gt; I mention how to work with the EDirect utilities). Unfortunately, ClinVar does not currently support a batch query interface via EDirect&amp;nbsp;utilities.&lt;/p&gt;
&lt;p&gt;However, ClinVar provides &lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/docs/maintenance_use/"&gt;other approaches&lt;/a&gt; for the access and use of their data. One of these approaches is the provisioning of the complete public data set in the form of an &lt;a href="https://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/"&gt;&lt;span class="caps"&gt;XML&lt;/span&gt; file stored at the ClinVar &lt;span class="caps"&gt;FTP&lt;/span&gt; server&lt;/a&gt;. The &lt;code&gt;ClinVarFullRelease&lt;/code&gt; &lt;span class="caps"&gt;XML&lt;/span&gt; file is updated weekly, and every release happening on the first Thursday of the month is&amp;nbsp;archived.&lt;/p&gt;
&lt;h2&gt;Parsing the ClinVar &lt;span class="caps"&gt;XML&lt;/span&gt;&amp;nbsp;file&lt;/h2&gt;
&lt;p&gt;Recently, I started assisting my team in uploading variant/phenotype interpretations to ClinVar. I wanted to find a way to gather all our submissions into a spreadsheet so every team member could easily check whenever necessary. Thus, I downloaded the full ClinVar release &lt;span class="caps"&gt;XML&lt;/span&gt; file and tried to parse it with the &lt;span class="caps"&gt;XML&lt;/span&gt;-handling &lt;a href="https://docs.python.org/3/library/xml.etree.elementtree.html"&gt;&lt;code&gt;ElementTree&lt;/code&gt; module&lt;/a&gt;. However, I had limited success. I could extract some information, but the output did not turn out exactly the way I was intending, so I set out to find working&amp;nbsp;alternatives.&lt;/p&gt;
&lt;p&gt;Eventually, I found out that the &lt;a href="https://pandas.pydata.org/"&gt;&lt;code&gt;pandas&lt;/code&gt; module&lt;/a&gt; has a method to convert &lt;span class="caps"&gt;XML&lt;/span&gt;-stored data into traditional data frames. Moreover, since September 2022, their &lt;code&gt;read_xml()&lt;/code&gt; function supports large &lt;span class="caps"&gt;XML&lt;/span&gt; files via the &lt;code&gt;iterparse&lt;/code&gt; argument (read an excerpt of the release note &lt;a href="https://pandas.pydata.org/docs/whatsnew/v1.5.0.html#read-xml-now-supports-large-xml-using-iterparse"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The function documentation states that the &lt;code&gt;iterparse&lt;/code&gt; argument is a memory-efficient method for handling big &lt;span class="caps"&gt;XML&lt;/span&gt; files without storing all data elements within memory. This was exactly my case, so I tried the &lt;code&gt;read_xml()&lt;/code&gt; function &amp;mdash; it worked quite&amp;nbsp;well!&lt;/p&gt;
&lt;p&gt;I wrote a small script that you can use to parse the ClinVar &lt;span class="caps"&gt;XML&lt;/span&gt; file. Of course, when you get acquainted with the &lt;code&gt;read_xml()&lt;/code&gt;, you may use it for parsing any other &lt;span class="caps"&gt;XML&lt;/span&gt; you wish. I used an &lt;a href="https://aws.amazon.com/ec2/?nc1=h_ls"&gt;&lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;EC2&lt;/span&gt; instance&lt;/a&gt; with 90 &lt;span class="caps"&gt;GB&lt;/span&gt; &lt;span class="caps"&gt;RAM&lt;/span&gt; while working on this tutorial. I did not try to process the ClinVar file in less powerful systems. &lt;em&gt;Try at your own risk&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I uploaded the script (named &lt;code&gt;clinvar_pandas_xml_parser.py&lt;/code&gt;) to &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2023_02_04_Parsing_ClinVar_XML_with_pandas"&gt;the corresponding folder on my portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Downloading the full ClinVar release &lt;span class="caps"&gt;XML&lt;/span&gt;&amp;nbsp;file&lt;/h3&gt;
&lt;p&gt;You can download the latest &lt;code&gt;.gz&lt;/code&gt;-compressed &lt;span class="caps"&gt;XML&lt;/span&gt; files via the following links (&lt;strong&gt;&lt;span class="caps"&gt;WARNING&lt;/span&gt;:&lt;/strong&gt; the release files are &lt;span class="caps"&gt;HUGE&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/weekly_release/ClinVarFullRelease_00-latest_weekly.xml.gz"&gt;Weekly&amp;nbsp;release&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/ClinVarFullRelease_00-latest.xml.gz"&gt;Monthly&amp;nbsp;release&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Go to a convenient directory on your system and download one of the files above. I downloaded the most recent monthly release and decompressed it soon&amp;nbsp;after:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;wget https://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/ClinVarFullRelease_00-latest.xml.gz

gunzip ClinVarFullRelease_00-latest.xml.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you want to check the file integrity, compare your checksum against the corresponding ClinVar-provided checksum&amp;nbsp;file:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/weekly_release/ClinVarFullRelease_00-latest_weekly.xml.gz.md5"&gt;Weekly release (&lt;span class="caps"&gt;MD5&lt;/span&gt; checksum&amp;nbsp;file)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/ClinVarFullRelease_00-latest.xml.gz.md5"&gt;Monthly release (&lt;span class="caps"&gt;MD5&lt;/span&gt; checksum&amp;nbsp;file)&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Installing&amp;nbsp;modules&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;iterparse&lt;/code&gt; argument in the &lt;code&gt;read_xml()&lt;/code&gt; function was introduced in &lt;code&gt;pandas&lt;/code&gt; version 1.5.0 and requires the &lt;code&gt;lxml&lt;/code&gt; or &lt;code&gt;ElementTree&lt;/code&gt; modules to work. In this tutorial, I will use &lt;code&gt;lxml&lt;/code&gt; (the default). Therefore, install the necessary modules via &lt;code&gt;pip&lt;/code&gt; or &lt;code&gt;conda&lt;/code&gt; (see my &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;previous post&lt;/a&gt; on how to configure &lt;code&gt;conda&lt;/code&gt; virtual environments in a Unix system). For&amp;nbsp;example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda activate env_name
conda install -c conda-forge &lt;span class="nv"&gt;pandas&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;.5.0 lxml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Running the &lt;code&gt;clinvar_pandas_xml_parser.py&lt;/code&gt; script&lt;/h3&gt;
&lt;p&gt;Finally, let&amp;#8217;s walk through the script. First, I import the &lt;code&gt;pandas&lt;/code&gt; module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I saved the &lt;span class="caps"&gt;XML&lt;/span&gt; file path into the &lt;code&gt;xml_file_path&lt;/code&gt; object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;xml_file_path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ClinVarFullRelease_00-latest.xml&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, I investigated the &lt;span class="caps"&gt;XML&lt;/span&gt; using &lt;code&gt;grep&lt;/code&gt; commands to match specific strings of interest to get a feel of how the &lt;span class="caps"&gt;XML&lt;/span&gt; file was structured. I am sure that are better ways to assess the &lt;span class="caps"&gt;XML&lt;/span&gt; elements structure, but I am not an expert in &lt;span class="caps"&gt;XML&lt;/span&gt;&amp;nbsp;files.&lt;/p&gt;
&lt;p&gt;Through my investigation of the file, I concluded that the &lt;code&gt;ClinVarAssertion&lt;/code&gt; elements within the &lt;span class="caps"&gt;XML&lt;/span&gt; structure contained all information I was needing at the moment. Thus, I created a Python dictionary object named &lt;code&gt;iterparse_dict&lt;/code&gt; with the string &amp;#8220;&lt;code&gt;ClinVarAssertion&lt;/code&gt;&amp;#8221; as a &lt;em&gt;key&lt;/em&gt; and a Python list as its corresponding &lt;em&gt;value&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;iterparse_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ClinVarAssertion&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[]}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;em&gt;key&lt;/em&gt; represents the parent &lt;span class="caps"&gt;XML&lt;/span&gt; node tag. The &lt;em&gt;value&lt;/em&gt; is a list containing all child or grandchild nodes, tags, or attributes at any node level inside the main &lt;span class="caps"&gt;XML&lt;/span&gt; node &amp;mdash; simple as that. I chose the&amp;nbsp;following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;iterparse_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;ClinVarAssertion&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;ID&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;SubmissionName&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;localKey&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;submittedAssembly&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;submitter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;submitterDate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;Acc&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;RecordStatus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;OrgID&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;DateCreated&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;DateUpdated&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;Version&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;DateLastEvaluated&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;Description&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;ReviewStatus&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;Comment&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, I passed the &lt;code&gt;iterparse_dict&lt;/code&gt; as the value for the &lt;code&gt;iterparse&lt;/code&gt; argument of the &lt;code&gt;read_xml()&lt;/code&gt; function and stored the output as the &lt;code&gt;df&lt;/code&gt; object &amp;mdash; a &lt;code&gt;pandas.DataFrame&lt;/code&gt;. The columns of the data frame will correspond to the information stored at each &lt;code&gt;ClinVarAssertion&lt;/code&gt; tag,&amp;nbsp;attributes&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_xml&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;xml_file_path&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;iterparse&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;iterparse_dict&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After some time, the function returns a data frame that you can further filter to search for information. For now, I saved the data frame as a pickled&amp;nbsp;object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pandas_parsed.pkl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;At any moment, I can restore the data frame through &lt;code&gt;pandas&lt;/code&gt; as well (&lt;strong&gt;&lt;span class="caps"&gt;REMEMBER&lt;/span&gt;:&lt;/strong&gt; Loading pickled data received from untrusted sources can be&amp;nbsp;unsafe):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pandas_parsed.pkl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I demonstrated one way of exploring the full release of the ClinVar database, through an up-to-date &lt;code&gt;pandas&lt;/code&gt; method that can deal with big &lt;span class="caps"&gt;XML&lt;/span&gt;&amp;nbsp;files.&lt;/p&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;p&gt;A brief explanation of what each &lt;em&gt;value&lt;/em&gt; in the &lt;code&gt;iterparse_dict&lt;/code&gt; dictionary object&amp;nbsp;represents:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ID&lt;/code&gt;: A unique numeric id representing a submission (a single submission usually contains many variant/phenotype&amp;nbsp;interpretations).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SubmissionName&lt;/code&gt;: A unique string representing a&amp;nbsp;submission.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;localKey&lt;/code&gt;: The &lt;span class="caps"&gt;HGVS&lt;/span&gt; expression representing each variant within a single&amp;nbsp;submission.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;submittedAssembly&lt;/code&gt;: The assembly (genome reference build) that was used for variant calling, annotation and localization. Usually is &amp;#8220;GRCh37&amp;#8221; or&amp;nbsp;&amp;#8220;GRCh38&amp;#8221;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;submitter&lt;/code&gt;: The organization that was responsible for the&amp;nbsp;submission.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;submitterDate&lt;/code&gt;: The date when the submission was uploaded to&amp;nbsp;ClinVar.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Acc&lt;/code&gt;: A ClinVar identifier string. As stated in the &lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/docs/identifiers/"&gt;ClinVar identifiers documentation&lt;/a&gt;: &amp;#8220;Accession numbers in ClinVar have the pattern of 3 letters and 9 numerals. The letters are either &lt;span class="caps"&gt;SCV&lt;/span&gt; (think of it as Submitted record in ClinVar), &lt;span class="caps"&gt;RCV&lt;/span&gt; (Reference ClinVar record) or &lt;span class="caps"&gt;VCV&lt;/span&gt; (Variation ClinVar&amp;nbsp;record).&amp;#8221;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;RecordStatus&lt;/code&gt;: The status of the record, whether current, deleted or secondary&amp;nbsp;(merged).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;OrgID&lt;/code&gt;: A unique numeric identifier for each organization that was responsible for the&amp;nbsp;submission.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DateCreated&lt;/code&gt;: The date when the submission was accepted and integrated into the&amp;nbsp;database.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DateUpdated&lt;/code&gt;: The date when the submitter updated the&amp;nbsp;record.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Version&lt;/code&gt;: The version assigned to a record. As stated in the &lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/docs/identifiers/"&gt;ClinVar identifiers documentation&lt;/a&gt;: &amp;#8220;The version number is incremented when a submitter updates a record or when the contents of a reference or variation record change because of addition to, updates of, or deletion of the &lt;span class="caps"&gt;SCV&lt;/span&gt; accessions on which it is&amp;nbsp;based.&amp;#8221;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DateLastEvaluated&lt;/code&gt;: The date when the organization evaluated the clinical significance of any given variant in the context of any given&amp;nbsp;phenotype.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Description&lt;/code&gt;: A description of the clinical significance of any given variant in the context of any given phenotype, such as &amp;#8220;Pathogenic&amp;#8221;, &amp;#8220;Likely pathogenic&amp;#8221;, &amp;#8220;Benign&amp;#8221;,&amp;nbsp;etc.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ReviewStatus&lt;/code&gt;: As stated in the &lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/docs/review_status/"&gt;ClinVar review status documentation&lt;/a&gt;: &amp;#8220;The level of review supporting the assertion of clinical significance for the&amp;nbsp;variation.&amp;#8221;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Comment&lt;/code&gt;: Any additional (free-text) comments the organization that was responsible for the submission provided regarding the interpretation of any given variant in the context of any given&amp;nbsp;phenotype.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/intro/"&gt;ClinVar | Documentation | What is&amp;nbsp;ClinVar?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/submitters/"&gt;ClinVar | Documentation |&amp;nbsp;Submitters&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/"&gt;ClinVar | Search&amp;nbsp;Tool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://varnomen.hgvs.org/"&gt;Sequence Variant&amp;nbsp;Nomenclature&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/books/NBK179288/"&gt;Entrez Direct: E-utilities on the Unix Command Line - Entrez Programming Utilities Help - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;nbsp;Bookshelf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-2.html"&gt;&lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;2)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/docs/maintenance_use/"&gt;Accessing and using data in&amp;nbsp;ClinVar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ftp.ncbi.nlm.nih.gov/pub/clinvar/xml/"&gt;ClinVar | &lt;span class="caps"&gt;FTP&lt;/span&gt; server | Index of&amp;nbsp;/pub/clinvar/xml&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/xml.etree.elementtree.html"&gt;xml.etree.ElementTree â The ElementTree &lt;span class="caps"&gt;XML&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pandas.pydata.org/"&gt;pandas - Python Data Analysis&amp;nbsp;Library&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pandas.pydata.org/docs/whatsnew/v1.5.0.html#read-xml-now-supports-large-xml-using-iterparse"&gt;pandas | Documentation | Whatâs new in 1.5.0 (September 19,&amp;nbsp;2022)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://aws.amazon.com/ec2/?nc1=h_ls"&gt;Secure and resizable cloud compute â Amazon &lt;span class="caps"&gt;EC2&lt;/span&gt; â Amazon Web&amp;nbsp;Services&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;Setting Up Your Unix Computer for Bioinformatics&amp;nbsp;Analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/docs/identifiers/"&gt;ClinVar | Documentation | Identifiers in&amp;nbsp;ClinVar&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/clinvar/docs/review_status/"&gt;ClinVar | Documentation | Review&amp;nbsp;status&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Sat, 04 Feb 2023 16:25:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2023-02-04:/parsing-the-clinvar-xml-file-with-pandas.html</guid><category>Python</category><category>pandas</category><category>ClinVar</category><category>genomics</category><category>variants</category></item><item><title>Opening files of size larger than RAM withÂ pandas</title><link>https://antoniocampos13.github.io/opening-files-of-size-larger-than-ram-with-pandas.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Dealing with big files is a routine for everyone working in genomics. &lt;span class="caps"&gt;FASTQ&lt;/span&gt;, &lt;span class="caps"&gt;VCF&lt;/span&gt;, &lt;span class="caps"&gt;BAM&lt;/span&gt;, and &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; files, to name a few, can range from some hundreds of megabytes to several gigabytes in size. Usually, we can use cloud services to configure computing instances with a lot of &lt;span class="caps"&gt;RAM&lt;/span&gt;, but we may use some ways to read and manipulate large-than-&lt;span class="caps"&gt;RAM&lt;/span&gt; files in our personal/work&amp;nbsp;machines.&lt;/p&gt;
&lt;p&gt;This post will demonstrate how to work with big tabular data using the &lt;code&gt;chunksize&lt;/code&gt; option with &lt;code&gt;pandas&lt;/code&gt;. You can find the code in my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2022_06_27_Opening_files_of_size_larger_than_RAM_with_pandas"&gt;portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Chunking: divide and&amp;nbsp;conquer&lt;/h3&gt;
&lt;p&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Chunking&amp;#8221; means splitting the big file into chunks (partitions) so the Python session can work with each part separately, meaning it would not need to hold the big data in memory all at once. Keep in mind that not every problem can be solved by chunking. Therefore, if your goal does &lt;span class="caps"&gt;NOT&lt;/span&gt; involve coordination between chunks, such as some filtering and little edition, chunking could help. However, if your task is more complicated than this, other modules such as &lt;a href="https://www.dask.org/"&gt;&lt;code&gt;dask&lt;/code&gt;&lt;/a&gt; are the better option. The panda&amp;#8217;s documentation has an excellent &lt;a href="https://pandas.pydata.org/docs/user_guide/scale.html#"&gt;chapter&lt;/a&gt; explaining ways to go when scaling to large&amp;nbsp;datasets.&lt;/p&gt;
&lt;h2&gt;The input: the human reference genome &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt;&amp;nbsp;file&lt;/h2&gt;
&lt;p&gt;I downloaded the human reference genome &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; file &lt;code&gt;GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gff.gz&lt;/code&gt; file at the &lt;a href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/"&gt;&lt;span class="caps"&gt;NCBI&lt;/span&gt; &lt;span class="caps"&gt;FTP&lt;/span&gt; server&lt;/a&gt; containing files preformatted for use in Bioinformatic analysis pipelines. Next, I extracted the contents of the&amp;nbsp;file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;gunzip GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gff.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The extracted file has a size of about 1 &lt;span class="caps"&gt;GB&lt;/span&gt;. Decently big for my demonstrational&amp;nbsp;purposes.&lt;/p&gt;
&lt;p&gt;But what is a &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; file? The Gene Transfer Format or General Feature Format is a tab-delimited text file format. Bioinformaticians use it to describe genomic features such as genes, exons, introns, putative protein-coding sequences (&lt;span class="caps"&gt;CDS&lt;/span&gt;), transcription factor binding sites, etc. The first two versions (&lt;span class="caps"&gt;GTF&lt;/span&gt; and &lt;span class="caps"&gt;GFF2&lt;/span&gt;) had deficiencies, and &lt;span class="caps"&gt;GFF3&lt;/span&gt; was developed to address them. You can read more about &lt;span class="caps"&gt;GFF3&lt;/span&gt; on &lt;a href="https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md"&gt;Lincoln Stein&amp;#8217;s GitHub page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Every &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; file has nine fields (columns). A dot &lt;code&gt;.&lt;/code&gt; represents missing or null data. The nine columns&amp;nbsp;are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;seqid&lt;/code&gt;: the name of the sequence where the feature is located. For example, a chromosome or&amp;nbsp;contig;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;source&lt;/code&gt;: the program or organization, laboratory, etc. that generated the information regarding the&amp;nbsp;feature;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;type&lt;/code&gt;: qualifiers like &amp;#8220;gene&amp;#8221;, &amp;#8220;exon&amp;#8221;, &amp;#8220;&lt;span class="caps"&gt;CDS&lt;/span&gt;&amp;#8221;. Features can have children: for example, the exons of a gene refer to its gene (their parent). Ideally, all the children features must follow their parents after the parents&amp;#8217; initial definition in the&amp;nbsp;file.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;start&lt;/code&gt;: the base position in the sequence where the feature starts. It has a 1-base offset, in contrast to the &lt;span class="caps"&gt;BED&lt;/span&gt; format, which is&amp;nbsp;0-offset.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;end&lt;/code&gt;: the base position in the sequence where the feature&amp;nbsp;ends.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;score&lt;/code&gt;: numeric value representing the quality of the&amp;nbsp;sequence.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;strand&lt;/code&gt;: indicates the strand of the feature: &lt;code&gt;+&lt;/code&gt; (the sense strand is the default 5&amp;#8217;-3&amp;#8217; representation of the feature), &lt;code&gt;-&lt;/code&gt; (the sense strand is the reverse complement strand of the sequence representation), or &lt;code&gt;.&lt;/code&gt; (undetermined).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;phase&lt;/code&gt;: used to indicate the reading frame of the features that are &lt;span class="caps"&gt;CDS&lt;/span&gt;. Can be &lt;code&gt;0&lt;/code&gt;, &lt;code&gt;1&lt;/code&gt;, &lt;code&gt;2&lt;/code&gt; or &lt;code&gt;.&lt;/code&gt; (undetermined).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;attributes&lt;/code&gt;: all other information relevant for describind the&amp;nbsp;feature.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I will open this file using &lt;code&gt;pandas&lt;/code&gt; and keep just the exons of all annotated human&amp;nbsp;genes.&lt;/p&gt;
&lt;h2&gt;Chunking with &lt;code&gt;pandas&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;I create a python script and import the &lt;code&gt;pandas&lt;/code&gt; module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="err"&gt;`&lt;/span&gt;&lt;span class="n"&gt;pandas&lt;/span&gt;&lt;span class="err"&gt;`&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I define some variables to store the path of the &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; file, the name of the output table, and the chunk&amp;nbsp;size:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;TABLE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gff&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;EDITED_TABLE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;human_gene_exons_GRCh38.gff&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;CHUNKSIZE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;20000000&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The chunk size must be an integer because it represents the number of lines each chunk will have. In the example above, I will tell &lt;code&gt;pandas&lt;/code&gt; to partition the file into parts containing two million lines until it finishes processing the whole&amp;nbsp;dataset.&lt;/p&gt;
&lt;p&gt;Next, I define a list with the column&amp;nbsp;names:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;column_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;seqid&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;source&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;start&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;end&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;score&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;strand&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;phase&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;attributes&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Since the file is tab-delimited, I can use the &lt;code&gt;pd.read_table()&lt;/code&gt; function, passing the &lt;code&gt;column_names&lt;/code&gt; list as the value for the &lt;code&gt;names&lt;/code&gt; argument and the &lt;code&gt;chunksize&lt;/code&gt; as&amp;nbsp;well:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;chunks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TABLE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;column_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;chunksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;CHUNKSIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;comment&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;#&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Or:&lt;/span&gt;
&lt;span class="n"&gt;chunks&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;TABLE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;column_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;chunksize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;CHUNKSIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;comment&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;#&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If I had not decompressed the file, I could also use the argument &lt;code&gt;compression="infer"&lt;/code&gt; so &lt;code&gt;pandas&lt;/code&gt; would decompress it&amp;nbsp;on-the-fly.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;chunksize&lt;/code&gt; argument makes the &lt;code&gt;pd.read_table()&lt;/code&gt; return an &lt;strong&gt;iterator&lt;/strong&gt; object (&lt;code&gt;TextFileReader&lt;/code&gt;). What is an iterator? In Python, an iterator is an object we can traverse through all its values. Python lists, dictionaries, and tuples are all Pythonic iterators. In our case, each element of this iterator is a &lt;code&gt;pd.Dataframe&lt;/code&gt; instead.&lt;/p&gt;
&lt;p&gt;Therefore, the &lt;code&gt;for&lt;/code&gt; loop I wrote will perform the same action on every &lt;code&gt;pd.DataFrame&lt;/code&gt; in the&amp;nbsp;iterator.&lt;/p&gt;
&lt;h2&gt;Filtering each&amp;nbsp;chunk&lt;/h2&gt;
&lt;p&gt;These are the steps I will perform with each&amp;nbsp;chunk:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Filter for rows with &amp;#8220;exon&amp;#8221; values in the &lt;code&gt;type&lt;/code&gt; column;&lt;/li&gt;
&lt;li&gt;Drop (remove) all columns except for &lt;code&gt;seqid&lt;/code&gt;, &lt;code&gt;type&lt;/code&gt;, &lt;code&gt;start&lt;/code&gt;, &lt;code&gt;end&lt;/code&gt; and &lt;code&gt;attributes&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Save the edited chunk directly to disk by appending the chunk to a tab-delimited&amp;nbsp;file.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Below is the loop&amp;nbsp;code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;chunk&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;chunks&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# Step 1&lt;/span&gt;
    &lt;span class="n"&gt;temp_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;chunk&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;chunk&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;exon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# Step 2&lt;/span&gt;
    &lt;span class="n"&gt;temp_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;temp_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;source&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;score&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;strand&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;phase&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="c1"&gt;# Step 3&lt;/span&gt;
    &lt;span class="n"&gt;temp_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EDITED_TABLE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I should explain step 3 in more detail. Since the &lt;code&gt;for&lt;/code&gt; loop will dump each part immediately to disk after finishing my edits, Python&amp;#8217;s &lt;span class="caps"&gt;RAM&lt;/span&gt; usage will be more or less constant during the file processing. You may see other tutorials appending the data frames to a list and concatenating them into a final &lt;code&gt;pd.DataFrame&lt;/code&gt;, but in my opinion, this kind of defeats the purpose of chunking since Python will have to hold everything in memory, risking &lt;span class="caps"&gt;RAM&lt;/span&gt; overuse and killing the&amp;nbsp;process.&lt;/p&gt;
&lt;p&gt;Let me explain the &lt;code&gt;pd.to_csv()&lt;/code&gt; arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;EDITED_TABLE&lt;/code&gt;: the file output&amp;nbsp;name/path;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;header=False&lt;/code&gt;: do not output the column names to&amp;nbsp;file;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;mode="a"&lt;/code&gt;: append each chunk on the output&amp;nbsp;file;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sep="\t"&lt;/code&gt;: write tab-delimited columns on the output&amp;nbsp;file;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;index=False&lt;/code&gt;: do not output index column. Since I did not set the index, it would print the row numbers, which would be undesirable (it would violate &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; format&amp;nbsp;specifications).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Observe that I defined the column names to make filtering/editing easier. The &lt;span class="caps"&gt;GTF&lt;/span&gt;/&lt;span class="caps"&gt;GFF3&lt;/span&gt; format specifications do not require the header names to be present in the file. Therefore, I removed them during Step&amp;nbsp;3.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I demonstrated one way of dealing with big files by chunking with &lt;code&gt;pandas&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;p&gt;I already have written about &lt;code&gt;dask&lt;/code&gt; on my machine learning tutorials. See part 1 &lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-1-making-features-datasets.html"&gt;here&lt;/a&gt; and part 2 &lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-2-making-a-model.html"&gt;here&lt;/a&gt;. Check &lt;a href="https://pandas.pydata.org/docs/index.html"&gt;&lt;code&gt;pandas&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt; documentation&lt;/a&gt; and &lt;a href="https://docs.dask.org/en/stable/"&gt;dask&amp;#8217;s&lt;/a&gt; as well for more&amp;nbsp;information.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.dask.org/"&gt;Dask | Scale the Python tools you&amp;nbsp;love&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pandas.pydata.org/docs/user_guide/scale.html#"&gt;Scaling to large datasets &amp;#8212; pandas 1.4.3&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/"&gt;Index of /genomes/all/&lt;span class="caps"&gt;GCA&lt;/span&gt;/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md"&gt;Specifications/gff3.md at master Â·&amp;nbsp;The-Sequence-Ontology/Specifications&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-1-making-features-datasets.html"&gt;Machine Learning with Python: Supervised Classification of &lt;span class="caps"&gt;TCGA&lt;/span&gt; Prostate Cancer Data (Part 1 - Making Features&amp;nbsp;Datasets)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-2-making-a-model.html"&gt;Machine Learning with Python: Supervised Classification of &lt;span class="caps"&gt;TCGA&lt;/span&gt; Prostate Cancer Data (Part 2 - Making a&amp;nbsp;Model)&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Mon, 27 Jun 2022 10:00:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2022-06-27:/opening-files-of-size-larger-than-ram-with-pandas.html</guid><category>Python</category><category>pandas</category><category>Genomics</category><category>Bioinformatics</category></item><item><title>Integrating R and Python withÂ reticulate</title><link>https://antoniocampos13.github.io/integrating-r-and-python-with-reticulate.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://rstudio.github.io/reticulate/"&gt;reticulate&lt;/a&gt; is an R package that allows interoperability between R and Python. I recently discovered this package, and I have been excited to efficiently run Python scripts inside an R session, bringing the best of both&amp;nbsp;worlds.&lt;/p&gt;
&lt;p&gt;In this post, I will demonstrate &lt;code&gt;reticulate&lt;/code&gt; with two scripts. First, I will start an R session with an R script. Then I will call a Python script inside the R session and manipulate the Python&amp;nbsp;output.&lt;/p&gt;
&lt;p&gt;The demonstration output will be a data frame containing exon coordinates of two genes and the nucleotide sequences of those&amp;nbsp;exons.&lt;/p&gt;
&lt;p&gt;As always, I will post the code of this demo at my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/R/2022-03-21_Integrate_R_Python_Reticulate"&gt;portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;R script: part&amp;nbsp;1/2&lt;/h2&gt;
&lt;p&gt;I will start by listing the R packages I will use for the demonstration. Install them with &lt;code&gt;install.packages()&lt;/code&gt; or &lt;code&gt;BiocManager::install()&lt;/code&gt; if they are &lt;a href="https://bioconductor.org/"&gt;Bioconductor&lt;/a&gt; packages (install &lt;code&gt;BiocManager&lt;/code&gt; with &lt;code&gt;install.packages()&lt;/code&gt; as&amp;nbsp;well).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;here&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tidyverse&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;reticulate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;glue&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Biocondutctor packages&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;GenomicRanges&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BSgenome&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BSgenome.Hsapiens.UCSC.hg38&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, using &lt;code&gt;here()&lt;/code&gt; package, I will create the full path of a &lt;a href="http://gmod.org/wiki/GFF3"&gt;&lt;code&gt;GFF3&lt;/code&gt;&lt;/a&gt; file. The &lt;code&gt;GFF3&lt;/code&gt; format stores genomic features in a text file to help represent genomic data. This file in question contains features of the whole human genome. Since it is a big file, download it at the &lt;a href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/"&gt;&lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;#8217;s &lt;span class="caps"&gt;FTP&lt;/span&gt; site&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;gffPath&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gff&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Edit the path using &lt;a href="https://here.r-lib.org/"&gt;&lt;code&gt;here()&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s&lt;/a&gt; syntax if&amp;nbsp;necessary.&lt;/p&gt;
&lt;p&gt;To keep things simple, I create a vector with just two&amp;nbsp;genes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;genes&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;HTT&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;FMR1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I can start working with the Python script. If you use &lt;code&gt;conda&lt;/code&gt; environments to work with Python as I do, you can select the environment with &lt;code&gt;reticulate&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;use_condaenv()&lt;/code&gt; function. In this case, I will use my &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;&lt;code&gt;bioenv&lt;/code&gt;&lt;/a&gt;&amp;nbsp;environment.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;use_condaenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;bioenv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Using &lt;code&gt;here()&lt;/code&gt; again, I set up the path for the Python script. Then, I use &lt;code&gt;reticulate&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;source_python()&lt;/code&gt; function to run the Python&amp;nbsp;script.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;pythonScript&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;reticulate_demo_Python_side.py&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;source_python&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pythonScript&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let me show you the contents of this&amp;nbsp;script.&lt;/p&gt;
&lt;h2&gt;Python&amp;nbsp;script&lt;/h2&gt;
&lt;p&gt;The modules I will use for the demonstration will be &lt;a href="https://github.com/biocore-ntnu/pyranges"&gt;&lt;code&gt;pyranges&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://pandas.pydata.org/"&gt;&lt;code&gt;pandas&lt;/code&gt;&lt;/a&gt;. I surmise &lt;code&gt;pyranges&lt;/code&gt;  calls &lt;code&gt;pandas&lt;/code&gt; in the background, but &lt;a href="https://peps.python.org/pep-0020/"&gt;explicit is better than implicit&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pyranges&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pr&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;(Remember to install all the necessary Python modules in the environment beforehand with &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;pip&lt;/code&gt;).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda install -c bioconda pyranges
&lt;span class="c1"&gt;# or&lt;/span&gt;
pip install pyranges
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, I read the &lt;code&gt;GFF3&lt;/code&gt; file into the Python session using &lt;code&gt;pyranges&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;read_gff3()&lt;/code&gt; function. Since I saved the file path into R&amp;#8217;s &lt;code&gt;gffPath&lt;/code&gt; object, I must use the &lt;code&gt;r.&lt;/code&gt; prefix to bring it into the Python session, like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;grch38_gff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_gff3&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gffPath&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I append the &lt;code&gt;.df&lt;/code&gt; suffix into the  &lt;code&gt;grch38_gff&lt;/code&gt; Python object to convert it into a &lt;code&gt;pandas&lt;/code&gt; data frame to facilitate the search for the genes I established into the R&amp;nbsp;session:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;grch38_gff&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I will search for the genes with &lt;code&gt;pandas&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;str.contains()&lt;/code&gt; method. To do this, I must create a regex string. I will append a &lt;code&gt;$&lt;/code&gt; to the end of each gene name to match the whole gene&amp;nbsp;string:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;suffix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I will use a list comprehension and Python&amp;#8217;s &lt;code&gt;join()&lt;/code&gt; method to create the search string&amp;nbsp;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;search_string&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;|&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;suffix&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;gene&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;genes&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;search_string&lt;/code&gt; object turns up like this (remember that the pipe &lt;code&gt;|&lt;/code&gt; character means &amp;#8220;or&amp;#8221; in&amp;nbsp;regex):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;search_string&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# outputs: &lt;/span&gt;
&lt;span class="c1"&gt;# HTT$|FMR1$&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I can finally filter the data frame to include only those&amp;nbsp;genes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gene&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contains&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;search_string&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Feature&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;exon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The line above concludes the Python script. Now let us return to the R script to wrap things&amp;nbsp;up.&lt;/p&gt;
&lt;h2&gt;R script: part&amp;nbsp;2/2&lt;/h2&gt;
&lt;p&gt;We can retrieve Python objects into the R session similarly to the other around. We must use the prefix &lt;code&gt;py$&lt;/code&gt; in R to get the objects generated by Python (the ones outputted by the&amp;nbsp;script).&lt;/p&gt;
&lt;p&gt;Therefore, I will retrieve the data frame &lt;code&gt;df&lt;/code&gt; object and assign it to the &lt;code&gt;exonsCoordinates&lt;/code&gt; R&amp;nbsp;object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;exonsCoordinates&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;py&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Before working with it, let me source a function that will retrieve the exon sequences using the &lt;a href="https://bioconductor.org/packages/release/bioc/html/GenomicRanges.html"&gt;&lt;code&gt;GenomicRanges&lt;/code&gt;&lt;/a&gt; (&lt;code&gt;pyranges&lt;/code&gt; is Python&amp;#8217;s &lt;code&gt;GenomicRanges&lt;/code&gt; analog), &lt;a href="https://bioconductor.org/packages/release/bioc/html/BSgenome.html"&gt;&lt;code&gt;BSgenome&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://bioconductor.org/packages/release/data/annotation/html/BSgenome.Hsapiens.UCSC.hg38.html"&gt;&lt;code&gt;BSgenome.Hsapiens.UCSC.hg38&lt;/code&gt;&lt;/a&gt;&amp;nbsp;packages.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;source&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;src&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;getSequence.R&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Function code (getSequence.R file):&lt;/span&gt;
&lt;span class="n"&gt;getSequence&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chr&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;end&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="n"&gt;gr&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;GenomicRanges&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;GRanges&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;glue&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;glue&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;{chr}:{start}-{end}&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="n"&gt;refBase&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;BSgenome&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;getSeq&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BSgenome.Hsapiens.UCSC.hg38&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="n"&gt;BSgenome.Hsapiens.UCSC.hg38&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;refBase&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;as.data.frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;refBase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

  &lt;span class="nf"&gt;return&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;refBase&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;: The first use of BSgenome/BSgenome.Hsapiens.&lt;span class="caps"&gt;UCSC&lt;/span&gt;.hg38 will prompt the download of human genomic sequence caches to be saved into a specific location on your computer. Be sure you have sufficient data allowances and&amp;nbsp;space.&lt;/p&gt;
&lt;p&gt;Finally, using &lt;code&gt;tidyverse&lt;/code&gt; (&lt;code&gt;dplyr&lt;/code&gt;) pipe &lt;code&gt;%&amp;gt;%&lt;/code&gt; syntax, I create a new column in the data frame (&lt;code&gt;mutate()&lt;/code&gt;) using the &lt;code&gt;Chromosome&lt;/code&gt;, &lt;code&gt;Start&lt;/code&gt;, and &lt;code&gt;End&lt;/code&gt; columns as arguments to my custom &lt;code&gt;getSequence()&lt;/code&gt; function. The new column will contain the exon sequences. Observe the use of &lt;code&gt;rowwise()&lt;/code&gt; and &lt;code&gt;ungroup()&lt;/code&gt;. Since my function is not vectorized, I must use it row-wise instead of column-wise. The first function ensures this. The second function restores the data frame to its column-wise nature after &lt;code&gt;getSequence()&lt;/code&gt; finishes its job. Then, I select just some of the columns of the data frame with &lt;code&gt;dplyr&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;select()&lt;/code&gt; function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;exonsCoordinatesClean&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;exonsCoordinates&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span class="nf"&gt;rowwise&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sequence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;getSequence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Chromosome&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Start&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;End&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;ungroup&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Chromosome&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Feature&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Start&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;End&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Strand&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Frame&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ID&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sequence&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion, I&amp;nbsp;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demonstrated how to run a Python script with &lt;code&gt;reticulate&lt;/code&gt; without leaving an active R&amp;nbsp;session;&lt;/li&gt;
&lt;li&gt;Explained how to retrieve Python objects into an R session and&amp;nbsp;vice-versa.&lt;/li&gt;
&lt;li&gt;Demonstrated a simple function that retrieves nucleotide sequences from human genome using chromosome&amp;nbsp;coordinates.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://rstudio.github.io/reticulate/"&gt;Interface to&amp;nbsp;Python&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bioconductor.org/"&gt;Bioconductor -&amp;nbsp;Home&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://gmod.org/wiki/GFF3"&gt;&lt;span class="caps"&gt;GFF3&lt;/span&gt; - &lt;span class="caps"&gt;GMOD&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/"&gt;Index of /genomes/all/&lt;span class="caps"&gt;GCA&lt;/span&gt;/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://here.r-lib.org/"&gt;here | A Simpler Way to Find Your&amp;nbsp;Files&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;Setting Up Your Unix Computer for Bioinformatics&amp;nbsp;Analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/biocore-ntnu/pyranges"&gt;GitHub - biocore-ntnu/pyranges: Performant Pythonic&amp;nbsp;GenomicRanges&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pandas.pydata.org/"&gt;pandas - Python Data Analysis&amp;nbsp;Library&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://peps.python.org/pep-0020/"&gt;&lt;span class="caps"&gt;PEP&lt;/span&gt; 20 â The Zen of Python |&amp;nbsp;peps.python.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bioconductor.org/packages/release/bioc/html/GenomicRanges.html"&gt;GenomicRanges&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bioconductor.org/packages/release/bioc/html/BSgenome.html"&gt;BSgenome&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bioconductor.org/packages/release/data/annotation/html/BSgenome.Hsapiens.UCSC.hg38.html"&gt;BSgenome.Hsapiens.&lt;span class="caps"&gt;UCSC&lt;/span&gt;.hg38&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Sun, 20 Mar 2022 16:40:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2022-03-20:/integrating-r-and-python-with-reticulate.html</guid><category>R</category><category>reticulate</category><category>gff</category><category>GenomicRanges</category><category>pyranges</category><category>BSgenome</category></item><item><title>Genomic Analysis WithÂ Hail</title><link>https://antoniocampos13.github.io/genomic-analysis-with-hail.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Hello, long time no see! Since I lasted posted, many things happened. Since March I have been working as Post-Doc Researcher, hired by the &lt;a href="https://www.einstein.br/Pages/Home.aspx"&gt;Hospital Israelita Albert Einstein (&lt;span class="caps"&gt;HIAE&lt;/span&gt;, SÃ£o Paulo, Brazil)&lt;/a&gt; to work for the Projeto Genomas Raros (&amp;#8220;Rare Genomes Project&amp;#8221;, &lt;span class="caps"&gt;GRAR&lt;/span&gt; from here on), a public-private partnership between &lt;span class="caps"&gt;HIAE&lt;/span&gt; and the Brazilian Health Ministry to further the implementation of genomic analysis into the Brazilian public healthcare system (&lt;span class="caps"&gt;SUS&lt;/span&gt;), with the intention of improve diagnostic rates of rare diseases in Brazil. Since 2020, thousands of genomes of Brazilian patients with suspected rare diseases have been sequenced, and many more will come in the next two&amp;nbsp;years.&lt;/p&gt;
&lt;p&gt;Thus, I have been tasked to develop/adapt analysis pipelines to handle whole-genome data at large scales compatible with the scope of &lt;span class="caps"&gt;GRAR&lt;/span&gt;. My team asked me to explore &lt;a href="https://hail.is/"&gt;Hail, a Python/Spark framework for scalable genomic analysis&lt;/a&gt;. It has been developed at &lt;a href="https://www.broadinstitute.org/"&gt;Broad Institute&lt;/a&gt; and was used to generate the &lt;a href="https://gnomAD.broadinstitute.org/"&gt;Genome Aggregation Database (gnomAD)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post I will share some use cases of this tool full of potential. Please notice that it is not intended to substitute the official documentation of Hail, &lt;span class="caps"&gt;GATK&lt;/span&gt; and other software used here. Just consider it as a demonstration of Hail use cases with commentaries. Also, notice two important things: first, Hail implements &amp;#8220;lazy evaluation&amp;#8221;; new users may think a specific command run blazingly fast, but in reality, Hail just mapped the execution order of functions needed and only will compute anything when necessary, such as saving results to disk and printing the first few rows of a dataset to Python&amp;#8217;s standard output stream. Second, Hail need a lot of &lt;span class="caps"&gt;RAM&lt;/span&gt; and &lt;span class="caps"&gt;CPU&lt;/span&gt; to work properly with big datasets. Hail is intended to be used in cloud/cluster computing environments, but a computer with relatively good hardware configuration can run small&amp;nbsp;datasets.&lt;/p&gt;
&lt;h2&gt;Installing&amp;nbsp;software&lt;/h2&gt;
&lt;p&gt;Prepare your Unix computing environment by installing &lt;a href="https://hail.is/#install"&gt;Hail&lt;/a&gt;, &lt;a href="https://pypi.org/project/gnomAD/"&gt;gnomAD utilities for Hail&lt;/a&gt; and &lt;a href="https://gatk.broadinstitute.org/hc/en-us"&gt;Genome Analysis Toolkit version 4 (&lt;span class="caps"&gt;GATK4&lt;/span&gt;)&lt;/a&gt;. Tools for manipulating &lt;span class="caps"&gt;VCF&lt;/span&gt; files are essential too, such as &lt;a href="http://samtools.github.io/bcftools/bcftools.html"&gt;bcftools&lt;/a&gt;. I advise installing the tools into a virtual environment for convenience (see &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;my previous post&lt;/a&gt; for some pointers on how to use conda&amp;nbsp;environments).&lt;/p&gt;
&lt;h2&gt;Preparing the multi-sample &lt;span class="caps"&gt;VCF&lt;/span&gt;&amp;nbsp;input&lt;/h2&gt;
&lt;p&gt;Since the &lt;span class="caps"&gt;GRAR&lt;/span&gt; data is coming from whole-genome sequencing (&lt;span class="caps"&gt;WGS&lt;/span&gt;), we have been generating one &lt;a href="https://gatk.broadinstitute.org/hc/en-us/articles/360035531812-GVCF-Genomic-Variant-Call-Format"&gt;genomic variant call format (gVCF) file&lt;/a&gt; per participant through &lt;a href="https://www.illumina.com/products/by-type/informatics-products/dragen-bio-it-platform.html"&gt;Illumina&amp;#8217;s &lt;span class="caps"&gt;DRAGEN&lt;/span&gt; sequencing read analysis platform&lt;/a&gt;. A gVCF file has all the characteristics of a &lt;a href="https://samtools.github.io/hts-specs/VCFv4.2.pdf"&gt;&lt;span class="caps"&gt;VCF&lt;/span&gt; file&lt;/a&gt;, the difference being that the gVCF files have information for all sites in the genome, which give more precision during the integration of variant calls coming from several&amp;nbsp;samples.&lt;/p&gt;
&lt;p&gt;By integration I mean &lt;strong&gt;combining&lt;/strong&gt; several gVCF files into a single multi-sample &lt;span class="caps"&gt;VCF&lt;/span&gt; file so we can perform analysis (calculate allelic frequencies, assess genotyping quality, and so on) with the &lt;strong&gt;whole cohort&lt;/strong&gt;. We are currently using &lt;span class="caps"&gt;GATK&lt;/span&gt;&amp;#8217;s GenomicsDBImport tool. See &lt;a href="https://gatk.broadinstitute.org/hc/en-us/articles/360036883491-GenomicsDBImport"&gt;here&lt;/a&gt; for a tutorial of how to use it. Briefly, we intend to create a GenomicsDB object and we will update it regularly by appending new gVCFs as they become available, until the last participant is recruited and we have their genome sequenced. When this moment comes, we will extract a multi-sample &lt;span class="caps"&gt;VCF&lt;/span&gt; with &lt;span class="caps"&gt;GATK&lt;/span&gt;&amp;#8217;s &lt;code&gt;GenotypeGVCFs&lt;/code&gt; tool:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;gatk GenotypeGVCFs -R &lt;span class="nv"&gt;$REF&lt;/span&gt; -V &lt;span class="nv"&gt;$DBPATH&lt;/span&gt; -G StandardAnnotation -O cohort.vcf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Where &lt;code&gt;$REF&lt;/code&gt; and &lt;code&gt;$DBPATH&lt;/code&gt; are the paths of the genome reference (the same used during the variant call process) and the GenomicsDB, respectively (which should have a &lt;code&gt;gendb://&lt;/code&gt; prefix as noted in the GenomicsDBImport tutorial, something like &lt;code&gt;gendb://my_database&lt;/code&gt;). The &lt;code&gt;-O&lt;/code&gt; flag indicates the output name. If you wish to compress the &lt;span class="caps"&gt;VCF&lt;/span&gt; file, you may use &lt;code&gt;bcftools&lt;/code&gt; or &lt;code&gt;bgzip&lt;/code&gt;. Remember to index the compressed&amp;nbsp;file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;bcftools view cohort.vcf -Oz -o cohort.vcf.gz
bcftools index cohort.vcf.gz

&lt;span class="c1"&gt;# or&lt;/span&gt;
bgzip -@ &lt;span class="m"&gt;4&lt;/span&gt; cohort.vcf
tabix cohort.vcf.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The next step is to perform &lt;span class="caps"&gt;GATK&lt;/span&gt;&amp;#8217;s Variant Quality Score Recalibration (&lt;span class="caps"&gt;VQSR&lt;/span&gt;) in the output &lt;span class="caps"&gt;VCF&lt;/span&gt; with &lt;span class="caps"&gt;GATK&lt;/span&gt;&amp;#8217;s &lt;code&gt;VariantRecalibrator&lt;/code&gt;.  See their original tutorial &lt;a href="https://gatk.broadinstitute.org/hc/en-us/articles/360035531112--How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering"&gt;here&lt;/a&gt;. To put it simply, &lt;span class="caps"&gt;VQSR&lt;/span&gt; works by comparing the detected variants with high-confidence variant sites observed by several consortia (HapMap, 1000 Genomes etc.) and applies a filter deeming the variant a true positive (i.e. the observed variation is a true biological event) or a false positive (i.e. the observed variation is not real, it is in a fact sequencing artifact). To this end, I downloaded high-confidence datasets to perform the &lt;span class="caps"&gt;VQSR&lt;/span&gt;. The datasets can be found at the &lt;a href="https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0;tab=objects?prefix=&amp;amp;forceOnObjectsSortingFiltering=false"&gt;&lt;span class="caps"&gt;GATK&lt;/span&gt; Google Cloud Storage (Resource Bundle)&lt;/a&gt; and can be downloaded with &lt;a href="https://cloud.google.com/storage/docs/gsutil"&gt;&lt;code&gt;gsutil&lt;/code&gt; application&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I will post a script with slight modification in the steps of the &lt;span class="caps"&gt;VQSR&lt;/span&gt; tutorial in my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2021_07_07_Genomic_Analysis_With_Hail"&gt;portfolio&lt;/a&gt; (I was having errors so I noticed that I had to put spaces after the &lt;code&gt;-resource&lt;/code&gt; flags of the &lt;code&gt;VariantRecalibrator&lt;/code&gt; command). Briefly, the steps&amp;nbsp;are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Filtering samples with excess of heterozygotes (recommended when working with thousands of&amp;nbsp;samples);&lt;/li&gt;
&lt;li&gt;Make a sites-only &lt;span class="caps"&gt;VCF&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;Recalibration step (separately by indels/mixed and &lt;span class="caps"&gt;SNP&lt;/span&gt;&amp;nbsp;loci);&lt;/li&gt;
&lt;li&gt;Apply the recalibration&amp;nbsp;filters.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The output of the &lt;span class="caps"&gt;VQSR&lt;/span&gt; is the &lt;code&gt;snp.recalibrated.vcf.gz&lt;/code&gt; file (despite the name, the indels/mixed variants in the file have been recalibrated as well). We can now import the dataset into&amp;nbsp;Hail.&lt;/p&gt;
&lt;h2&gt;Initiating&amp;nbsp;Hail&lt;/h2&gt;
&lt;p&gt;Hail&amp;#8217;s frontend is written in Python. Simply importing the Hail module is not sufficient. We must initiate it so it can communicate with Spark. I created the &lt;code&gt;hail_demo_init.py&lt;/code&gt; file as an &amp;#8220;init template&amp;#8221; that can be reused between Hail&amp;nbsp;scripts/sessions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# hail_demo_init.py&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;hail&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;hl&lt;/span&gt;

&lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;GRCh38&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;init&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;idempotent&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;quiet&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;skip_logging_configuration&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;default_reference&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;spark_conf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;spark.executor.cores&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;4&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;spark.driver.memory&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;16g&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Notice the &lt;code&gt;DEFAULT_REF&lt;/code&gt; variable: it establishes that I intend to use the genomic coordinates considering the Genome Reference Consortium Human Reference 38 (GRCh38), the most recent genome version being accepted by the human genome community. Otherwise Hail would use GRCh37 as default. The &lt;code&gt;spark_conf&lt;/code&gt; argument where I setup Spark so it uses four &lt;span class="caps"&gt;CPU&lt;/span&gt; cores and 16 &lt;span class="caps"&gt;GB&lt;/span&gt; of &lt;span class="caps"&gt;RAM&lt;/span&gt;. Change these values as&amp;nbsp;appropriate.&lt;/p&gt;
&lt;h2&gt;Importing the &lt;span class="caps"&gt;VCF&lt;/span&gt;&amp;nbsp;input&lt;/h2&gt;
&lt;p&gt;I am now ready to import &lt;code&gt;snp.recalibrated.vcf.gz&lt;/code&gt; into the Hail session and convert it to Hail&amp;#8217;s &lt;code&gt;MatrixTable&lt;/code&gt; object and write it to the disk. In simple terms, a &lt;code&gt;MatrixTable&lt;/code&gt; is a representation of a &lt;span class="caps"&gt;VCF&lt;/span&gt; file that is amenable to be manipulated by Spark. Read the &lt;a href="https://hail.is/docs/0.2/index.html"&gt;Hail Docs&lt;/a&gt; for more details. I initiate Hail and then import and convert the dataset with two chained steps (&lt;code&gt;hl.import_vcf().write()&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# hail_demo_import_vcf.py&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;hail&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;hl&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;hail_demo_init&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt;

&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;import_vcf&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;snp.recalibrated.vcf.gz&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;force_bgz&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;reference_genome&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;array_elements_required&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;recalibrated.mt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;overwrite&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I can now proceed to prepare the dataset for sample and variant quality control (&lt;span class="caps"&gt;QC&lt;/span&gt;).&lt;/p&gt;
&lt;h2&gt;Preparing for Sample &lt;span class="caps"&gt;QC&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Before reading the created matrix table I create some &amp;#8220;magic numbers&amp;#8221; variables to hold some quality metrics thresholds for sample &lt;span class="caps"&gt;QC&lt;/span&gt;&amp;nbsp;later:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# hail_demo_sample_qc.py&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;hail&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;hl&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;hail_demo_init&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;gnomAD.utils.filtering&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;filter_to_autosomes&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;gnomAD.utils.annotations&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;add_variant_type&lt;/span&gt;

&lt;span class="c1"&gt;# Magic numbers&lt;/span&gt;
&lt;span class="n"&gt;CALL_RATE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.90&lt;/span&gt;  &lt;span class="c1"&gt;# Hail team value = 0.97. gnomAD value = 0.99&lt;/span&gt;
&lt;span class="n"&gt;RELATEDNESS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.088&lt;/span&gt;  &lt;span class="c1"&gt;# Hail team value. gnomAD value = 0.08838835&lt;/span&gt;
&lt;span class="n"&gt;READ_DEPTH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;  &lt;span class="c1"&gt;# Hail team value.&lt;/span&gt;
&lt;span class="n"&gt;FREEMIX_CONTAMINATION&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;  &lt;span class="c1"&gt;# gnomAD value.&lt;/span&gt;
&lt;span class="n"&gt;CHIMERIC_READS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;  &lt;span class="c1"&gt;# gnomAD value.&lt;/span&gt;
&lt;span class="n"&gt;MEDIAN_LENGTH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;250&lt;/span&gt;  &lt;span class="c1"&gt;# gnomAD value.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The comment in each line contains the value used by Hail team or gnomAD team. Since this is a demonstration, I may have used different values. Change these values as you feel appropriate as&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;I will now read the matrix table from disk and assign it to the &lt;code&gt;mt&lt;/code&gt; object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_matrix_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;recalibrated.mt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To check the first few sample ids, I use the &lt;code&gt;show()&lt;/code&gt; method (I explain the &lt;code&gt;s&lt;/code&gt; here&amp;nbsp;later):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To check the matrix table structure, I use the &lt;code&gt;describe()&lt;/code&gt; method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;describe&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output is as&amp;nbsp;follows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;----------------------------------------
Global fields:
    None
----------------------------------------
Column fields:
    &amp;#39;s&amp;#39;: str
----------------------------------------
Row fields:
    &amp;#39;locus&amp;#39;: locus&amp;lt;GRCh38&amp;gt;
    &amp;#39;alleles&amp;#39;: array&amp;lt;str&amp;gt;
    &amp;#39;rsid&amp;#39;: str
    &amp;#39;qual&amp;#39;: float64
    &amp;#39;filters&amp;#39;: set&amp;lt;str&amp;gt;
    &amp;#39;info&amp;#39;: struct {
        AC: array&amp;lt;int32&amp;gt;, 
        AF: array&amp;lt;float64&amp;gt;, 
        AN: int32, 
        BaseQRankSum: float64, 
        DB: bool, 
        DP: int32, 
        END: int32, 
        ExcessHet: float64, 
        FS: float64, 
        FractionInformativeReads: float64, 
        InbreedingCoeff: float64, 
        LOD: float64, 
        MLEAC: array&amp;lt;int32&amp;gt;, 
        MLEAF: array&amp;lt;float64&amp;gt;, 
        MQ: float64, 
        MQRankSum: float64, 
        NEGATIVE_TRAIN_SITE: bool, 
        POSITIVE_TRAIN_SITE: bool, 
        QD: float64, 
        R2_5P_bias: float64, 
        ReadPosRankSum: float64, 
        SOR: float64, 
        VQSLOD: float64, 
        culprit: str
    }
----------------------------------------
Entry fields:
    &amp;#39;AD&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;AF&amp;#39;: array&amp;lt;float64&amp;gt;
    &amp;#39;DP&amp;#39;: int32
    &amp;#39;F1R2&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;F2R1&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;GP&amp;#39;: array&amp;lt;float64&amp;gt;
    &amp;#39;GQ&amp;#39;: int32
    &amp;#39;GT&amp;#39;: call
    &amp;#39;ICNT&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;MB&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;MIN_DP&amp;#39;: int32
    &amp;#39;PL&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;PRI&amp;#39;: array&amp;lt;float64&amp;gt;
    &amp;#39;PS&amp;#39;: int32
    &amp;#39;RGQ&amp;#39;: int32
    &amp;#39;SB&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;SPL&amp;#39;: array&amp;lt;int32&amp;gt;
    &amp;#39;SQ&amp;#39;: float64
----------------------------------------
Column key: [&amp;#39;s&amp;#39;]
Row key: [&amp;#39;locus&amp;#39;, &amp;#39;alleles&amp;#39;]
----------------------------------------
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can see that a Hail &lt;code&gt;MatrixTable&lt;/code&gt; objects has four types of information&amp;nbsp;&amp;#8220;compartments&amp;#8221;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Global fields: information values that are identical for every&amp;nbsp;row&lt;/li&gt;
&lt;li&gt;Column fields: sample-level information (id, phenotype, sex&amp;nbsp;etc.)&lt;/li&gt;
&lt;li&gt;Row fields: variant-level information (locus, alleles, type of variant, allele frequency&amp;nbsp;etc.)&lt;/li&gt;
&lt;li&gt;Entry fields: variant-by-sample-level (genotype, genotype quality,&amp;nbsp;etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also notice that the &lt;code&gt;MatrixTable&lt;/code&gt; is keyed by column (&lt;code&gt;s&lt;/code&gt;: &lt;strong&gt;s&lt;/strong&gt;ample id field) and rows (locus and allele row fields) allowing us to perform &lt;span class="caps"&gt;SQL&lt;/span&gt;-style table joins. For each field type you can see the name of each field. The info field contains the &lt;span class="caps"&gt;INFO&lt;/span&gt; field from the input &lt;span class="caps"&gt;VCF&lt;/span&gt;. The entries fields contain the values listed into the &lt;span class="caps"&gt;FORMAT&lt;/span&gt; &lt;span class="caps"&gt;VCF&lt;/span&gt; field. For more details check the &lt;a href="https://hail.is/docs/0.2/tutorials/07-matrixtable.html"&gt;&lt;code&gt;MatrixTable&lt;/code&gt; Hail Docs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now I will modify an entry field and create other row fields that I will need to use later during variant &lt;span class="caps"&gt;QC&lt;/span&gt;. The commands to create/modify fields is &lt;code&gt;hl.annotate_cols()&lt;/code&gt;, &lt;code&gt;hl.annotate_rows()&lt;/code&gt; or &lt;code&gt;hl.annotate_entries()&lt;/code&gt; depending on the field type (columns, rows, entries, respectively). Notice that in one of them I used a gnomAD&amp;nbsp;function.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Mixture of non-empty with empty PL fields causes problems with sample QC for some reason; setting field to all empty&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_entries&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;PL&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;missing&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;PL&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Add variant-level annotations necessary for variant QC later&lt;/span&gt;
&lt;span class="c1"&gt;## Annotate variants in one of the categories: SNV, multi-SNV, indel, multi-indel, mixed&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;add_variant_type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alleles&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="c1"&gt;# gnomAD function&lt;/span&gt;

&lt;span class="c1"&gt;## Number of alleles at the site&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_alleles&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alleles&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;## Mixed sites (SNVs and indels present at the site)&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mixed_site&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;if_else&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variant_type&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;mixed&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;## Spanning deletions&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;spanning_deletion&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alleles&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To check the dimensions of the &lt;code&gt;MatrixTable&lt;/code&gt;, I can use the following&amp;nbsp;commands:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Number of Rows, Columns&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# Number of Columns&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count_cols&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To see a variant breakdown (types, number of variants per chromosome and several other information), there is the following&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;summarize_variants&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;A recommended step for further downstream analyses is to split multiallelic variants into biallelic&amp;nbsp;configuration:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split_multi_hts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To remove any monomorphic (invariant) loci I&amp;nbsp;use:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_alleles&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Try the &lt;code&gt;hl.summarize_variants()&lt;/code&gt; to check how the numbers changed after&amp;nbsp;splitting.&lt;/p&gt;
&lt;p&gt;Up until this moment, I have only genetics-related information into the &lt;code&gt;MatrixTable&lt;/code&gt;. I can instruct Hail to import &lt;span class="caps"&gt;TSV&lt;/span&gt;/&lt;span class="caps"&gt;CSV&lt;/span&gt; file with sample-level (column) annotations with the &lt;code&gt;hl.import_table()&lt;/code&gt; command and then associate each observation by keying the &lt;code&gt;s&lt;/code&gt; field with &lt;code&gt;hl.annotate_cols()&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;sa&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;import_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sample_info.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;impute&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;key&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;s&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_cols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_info&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sa&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;sample_info.txt&lt;/code&gt; example file has the following&amp;nbsp;format:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;s sex phenotype mean_coverage chimeric_reads contamination median_length
sample1 XX case 25 0.02 0.00 300
sample2 XY case 22 0.01 0.00 250
sample3 XY control 30 0.03 0.00 265
sample4 XX control 29 0.01 0.00 250
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this mock sample information file I included the sex karyotype, a fictitious phenotype and sequencing metrics to support the sample &lt;span class="caps"&gt;QC&lt;/span&gt; procedure. As you may have guessed, any number and type (numeric, string, float, integer, Boolean) of important sample metadata columns can be&amp;nbsp;included.&lt;/p&gt;
&lt;p&gt;Whenever we annotate columns, rows or entries in Hail, we must provide the name of the new field. In this case is &lt;code&gt;sample_info&lt;/code&gt;. So if I wanted to manipulate, say the sex karyotype field, I would refer the field name this&amp;nbsp;way:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_info&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sex&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is because the information in the file will be assigned to a dictionary-like field named &lt;code&gt;sample_info&lt;/code&gt; within the &lt;code&gt;MatrixTable&lt;/code&gt;; the &lt;code&gt;sex&lt;/code&gt;, &lt;code&gt;phenotype&lt;/code&gt;, etc. fields are &lt;em&gt;inside&lt;/em&gt; &lt;code&gt;sample_info&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Performing Sample &lt;span class="caps"&gt;QC&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;Plotting quality&amp;nbsp;metrics&lt;/h3&gt;
&lt;p&gt;Hail has a very convenient function to calculate descriptive statistics of fields in a &lt;code&gt;MatrixTable&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_qc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This function will calculate overall call rate per sample, mean read depth (coverage) and other &lt;a href="https://hail.is/docs/0.2/methods/genetics.html#hail.methods.sample_qc"&gt;things&lt;/a&gt;. We can plot the calculated fields to assess call rate and coverage (read depth) across samples. The code below will output a &lt;span class="caps"&gt;HTML&lt;/span&gt; plot to disk (make sure you have the &lt;a href="https://docs.bokeh.org/en/latest/index.html"&gt;bokeh&lt;/a&gt; Python module&amp;nbsp;installed).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bokeh.embed&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;file_html&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bokeh.resources&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;CDN&lt;/span&gt;

&lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_qc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dp_stats&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_qc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;call_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Mean DP&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Call Rate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;hover_fields&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ID&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;html&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;file_html&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;CDN&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Chart&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Call Rate by Mean DP.html&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;html&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Filter samples by quality&amp;nbsp;thresholds&lt;/h3&gt;
&lt;p&gt;I can finally filter out samples that do not meet the quality thresholds I established before. For example, the lines below will keep only samples that meet overall call rate and mean coverage quality&amp;nbsp;criteria:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_cols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_qc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;call_rate&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;CALL_RATE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_cols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_qc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dp_stats&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;READ_DEPTH&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If you have other quality criteria, you could filter in a similar way by correctly referring the field name (remember that dictionary-like fields contain other fields, as is the case of &lt;code&gt;sample_info&lt;/code&gt; I mentioned earlier and &lt;code&gt;sample_qc&lt;/code&gt; above) and a logical expression (which must follow the Python syntax for equalities and&amp;nbsp;inequalities).&lt;/p&gt;
&lt;h3&gt;Principal component analysis (&lt;span class="caps"&gt;PCA&lt;/span&gt;) to filter related&amp;nbsp;samples&lt;/h3&gt;
&lt;p&gt;To ensure that each sample in the cohort is unrelated to any other sample, we can run a principal component analysis (&lt;span class="caps"&gt;PCA&lt;/span&gt;) to evidence samples with kinship too higher according to our chosen threshold. The &lt;span class="caps"&gt;PCA&lt;/span&gt; will only work with autosome (diploid) biallelic variants. The code below will filter our &lt;code&gt;MatrixTable&lt;/code&gt; keeping only variants meeting these&amp;nbsp;conditions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;for_pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;filter_to_autosomes&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# gnomAD function. Will keep variants in chromosomes 1 to 22 only.&lt;/span&gt;
&lt;span class="n"&gt;for_pca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;for_pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;for_pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_alleles&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# will remove sites that were multi-allelic before splitting as well to ensure &amp;quot;pure&amp;quot; biallelic sites&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, I determine the sample number to calculate &lt;code&gt;k&lt;/code&gt;, the number of principal&amp;nbsp;components.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;sample_num&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;for_pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cols&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, using the &lt;a href="https://hail.is/docs/0.2/methods/genetics.html#hail.methods.hwe_normalized_pca"&gt;&lt;code&gt;hl.hwe_normalized_pca()&lt;/code&gt; function&lt;/a&gt; I calculate the principal component&amp;nbsp;scores:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hwe_normalized_pca&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;for_pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sample_num&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;compute_loadings&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With big sample sizes, the code above will restrict &lt;code&gt;k&lt;/code&gt; to the maximum of 10 principal&amp;nbsp;components.&lt;/p&gt;
&lt;p&gt;The scores are one of the inputs of the &lt;a href="https://hail.is/docs/0.2/methods/relatedness.html#hail.methods.pc_relate"&gt;&lt;code&gt;hl.pc_relate()&lt;/code&gt; function&lt;/a&gt; that will estimate the relatedness between samples in a pairwise manner. To speed things up, I will estimate only the kinship statistic (&lt;code&gt;statistics="kin"&lt;/code&gt;, while the default is &lt;code&gt;statistics="all"&lt;/code&gt;. Check the function documentation in the previous link for more&amp;nbsp;details).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;relatedness_ht&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pc_relate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;for_pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;min_individual_maf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;scores_expr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;for_pca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;col_key&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;block_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;4096&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;min_kinship&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;statistics&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;kin&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;relatedness_ht&lt;/code&gt; object is a Hail &lt;code&gt;Table&lt;/code&gt;. It differs from a &lt;code&gt;MatrixTable&lt;/code&gt; by not having column nor entries fields. We determine related samples by filtering this table to keep only the samples above the kinship threshold and then passing the object to &lt;a href="https://hail.is/docs/0.2/methods/misc.html#hail.methods.maximal_independent_set"&gt;&lt;code&gt;hl.maximal_independent_set()&lt;/code&gt; function&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;pairs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;relatedness_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;relatedness_ht&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;kin&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;RELATEDNESS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;related_samples_to_remove&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;maximal_independent_set&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pairs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pairs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;related_samples_to_remove&lt;/code&gt; object will contain a selection of samples to be removed from the dataset because they come from related individuals in the sample. We perform this filtering with the command below. Notice the use of the keyword &lt;code&gt;keep=False&lt;/code&gt; to &lt;em&gt;negate&lt;/em&gt; the selection (I do &lt;em&gt;not&lt;/em&gt; want to keep related&amp;nbsp;samples).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_cols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_defined&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;related_samples_to_remove&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;col_key&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt; &lt;span class="n"&gt;keep&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Use &lt;code&gt;mt.count_cols()&lt;/code&gt; to assess if any sample was&amp;nbsp;removed.&lt;/p&gt;
&lt;h3&gt;Wrapping up the sample &lt;span class="caps"&gt;QC&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Here I finish the sample &lt;span class="caps"&gt;QC&lt;/span&gt; process. I then save the &lt;code&gt;relatedness_ht&lt;/code&gt; and the dataset object &lt;code&gt;mt&lt;/code&gt; to disk with &lt;code&gt;write()&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;relatedness_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;relatedness.ht&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;overwrite&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sampleqc_pass.mt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;overwrite&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I can now proceed to variant &lt;span class="caps"&gt;QC&lt;/span&gt;.&lt;/p&gt;
&lt;h2&gt;Variant &lt;span class="caps"&gt;QC&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;I init Hail again if needed, import some more functions from gnomAD, create variables with variant quality thresholds, read the &lt;code&gt;MatrixTable&lt;/code&gt; with the data passing sample &lt;span class="caps"&gt;QC&lt;/span&gt; and calculate common variant statistics (such as allelic frequency) with &lt;a href="https://hail.is/docs/0.2/methods/genetics.html#hail.methods.variant_qc"&gt;&lt;code&gt;hl.variant_qc()&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# hail_demo_variant_qc.py&lt;/span&gt;
&lt;span class="c1"&gt;# Import modules and init Hail&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;hail&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;hl&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;gnomAD.utils.annotations&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;bi_allelic_site_inbreeding_expr&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;gnomAD.variant_qc.random_forest&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;apply_rf_model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;median_impute_features&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;gnomAD.variant_qc.pipeline&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_rf_model&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;hail_init&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt;

&lt;span class="c1"&gt;# Variant Quality hard filters&lt;/span&gt;
&lt;span class="n"&gt;INBR_COEFF&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;
&lt;span class="n"&gt;AB_LOWER_LIM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.2&lt;/span&gt;
&lt;span class="n"&gt;AB_UPPER_LIM&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;AB_LOWER_LIM&lt;/span&gt;

&lt;span class="c1"&gt;# Read MatrixTable with sample QC-passing dataset&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_matrix_table&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sampleqc_pass.mt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variant_qc&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Even though allelic frequency may be already been lift over from the original &lt;span class="caps"&gt;VCF&lt;/span&gt; input, I recommend using &lt;code&gt;hl.variant_qc()&lt;/code&gt; since it calculates potentially useful information besides allelic frequency, such as p-values from the test of Hardy-Weinberg equilibrium. Check the function documentation at Hail to see the complete list of&amp;nbsp;statistics.&lt;/p&gt;
&lt;h3&gt;Filter variants by genotype-related quality&amp;nbsp;thresholds&lt;/h3&gt;
&lt;p&gt;Next, I calculate two variant-level metrics: inbreeding coefficient and the maximum p-value for sampling the observed allele balance under a binomial&amp;nbsp;model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;inbr_coeff&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;bi_allelic_site_inbreeding_expr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GT&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;pab_max&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;binom_test&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AD&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DP&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;two-sided&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, I remove variants with excess of heterozygotes by inbreeding coefficient and variants for which no sample had high-quality genotypes by evaluating allele balance (the proportion of sequencing reads that support the&amp;nbsp;variant):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inbr_coeff&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;INBR_COEFF&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Removing variants for which no sample had high quality genotypes with hl.any()&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GQ&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DP&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_entries&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AD&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AD&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt; &lt;span class="c1"&gt;# AB = allele balance&lt;/span&gt;

&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GT&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_hom_ref&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;AB_LOWER_LIM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GT&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_het&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;AB_LOWER_LIM&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;AB_UPPER_LIM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="o"&gt;|&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GT&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;is_hom_var&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AB&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;AB_UPPER_LIM&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Variant &lt;span class="caps"&gt;QC&lt;/span&gt; by random forest&amp;nbsp;model&lt;/h3&gt;
&lt;p&gt;The gnomAD team adopted a &lt;a href="https://gnomAD.broadinstitute.org/news/2018-10-gnomAD-v2-1/"&gt;random forest model&lt;/a&gt; to filter out sequencing artifacts. Briefly, they labelled variants passing quality thresholds (such as &lt;span class="caps"&gt;GATK&lt;/span&gt;&amp;#8217;s &lt;span class="caps"&gt;VQSR&lt;/span&gt;) as true positives and variants not passing as false positives. Next, they performed a supervised random forest training with some variant-level features. From now on, I try to replicate their method with the best of my&amp;nbsp;understanding.&lt;/p&gt;
&lt;p&gt;I label the variants as true and false&amp;nbsp;positives:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;if_else&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;if_else&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filters&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The logic is that if the &lt;code&gt;filters&lt;/code&gt; field (which is carried over from the GenomicsDB-exported &lt;span class="caps"&gt;VCF&lt;/span&gt;) is empty, it indicates the variant passed the &lt;span class="caps"&gt;VQSR&lt;/span&gt; filter and is a false positive otherwise. Thus, I create two Boolean-type columns indicating it. Next, I create a Hail &lt;code&gt;Table&lt;/code&gt; extracting the needed features and the &lt;code&gt;tp&lt;/code&gt; and &lt;code&gt;fp&lt;/code&gt; fields and assigning it to the &lt;code&gt;rf_ht&lt;/code&gt; object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;rf_ht&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;inbr_coeff&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SOR&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ReadPosRankSum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;MQRankSum&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;QD&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pab_max&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variant_type&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;n_alleles&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mixed_site&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;spanning_deletion&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rows&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Remember that most of these features are brought over from the &lt;span class="caps"&gt;VCF&lt;/span&gt; &lt;span class="caps"&gt;INFO&lt;/span&gt; field, while the others were generated with the help of Hail. I also generate a Python list with the features&amp;nbsp;names:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;inbr_coeff&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;SOR&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;ReadPosRankSum&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;MQRankSum&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;QD&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;pab_max&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;variant_type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;n_alleles&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;mixed_site&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;spanning_deletion&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Since random forest models do not tolerate missing data, I use a gnomAD function to impute any missing data with the median of the&amp;nbsp;field:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;rf_ht&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;median_impute_features&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rf_ht&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I will reserve all variants located in chromosome 20 to perform model evaluation with the help of the &lt;a href="https://hail.is/docs/0.2/functions/genetics.html#hail.expr.functions.parse_locus_interval"&gt;&lt;code&gt;hl.parse_locus_interval()&lt;/code&gt; function&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;test_intervals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;chr20&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;test_intervals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_locus_interval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reference_genome&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;GRCh38&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;test_intervals&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I now may train the model with the help of gnomAD&amp;#8217;s &lt;code&gt;train_rf_model()&lt;/code&gt; function. Internally, the function will select a balanced dataset of true positives and false positives to train the&amp;nbsp;model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;rf_trained_ht&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rf_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_rf_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;rf_ht&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;rf_features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;tp_expr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;rf_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;fp_expr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;rf_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;test_expr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;literal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;test_intervals&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;interval&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;interval&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contains&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rf_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;locus&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;),&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;rf_trained_ht&lt;/code&gt; output is a Hail &lt;code&gt;Table&lt;/code&gt; with annotations related with the random forest model training. The &lt;code&gt;rf_model&lt;/code&gt; object is the model binary generated by Spark. The inputs include the &lt;code&gt;rf_ht Table&lt;/code&gt;, the &lt;code&gt;features&lt;/code&gt; list, the &lt;code&gt;rf_ht.tp&lt;/code&gt; and &lt;code&gt;rf_ht.fp&lt;/code&gt; Boolean columns and a &lt;code&gt;test_expr&lt;/code&gt; argument receives a expression that will ensure that the loci contained in the interval object &lt;code&gt;test_intervals&lt;/code&gt; will be used for model&amp;nbsp;evaluation.&lt;/p&gt;
&lt;p&gt;After model training, I left join the &lt;code&gt;rf_ht&lt;/code&gt; with the model-annotated &lt;code&gt;rf_trained_ht&lt;/code&gt; into the &lt;code&gt;ht Table&lt;/code&gt;. I use it as the input for the gnomAD&amp;#8217;s &lt;code&gt;apply_rf_model()&lt;/code&gt; function. It will apply the random forest model in all variants in the genome, including those not selected for&amp;nbsp;training.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;ht&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rf_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rf_trained_ht&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;left&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;rf_results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;apply_rf_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;ht&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ht&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;rf_model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;rf_model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;eval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rf_trained_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;features&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rf_label&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;prediction_col_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rf_prediction&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I then write to disk the Hail &lt;code&gt;Table&lt;/code&gt; containing a summary of the number of variants originally labeled as true or false positives and the prediction by the model. In other words - a confusion&amp;nbsp;matrix:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;rf_summary_ht&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;rf_results&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;tp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;fp&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rf_train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rf_label&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rf_prediction&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;aggregate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;agg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;count&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;

&lt;span class="n"&gt;rf_summary_ht&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rf_summary.ht&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;overwrite&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I unpack the &lt;code&gt;rf_results Table&lt;/code&gt; fields and join them in the sample &lt;span class="caps"&gt;QC&lt;/span&gt; &lt;code&gt;MatrixTable&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;variantqc_pass&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;annotate_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;rf_results&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;locus&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;alleles&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It can be easily filtered to keep only variants predicted to be true positives by the model and then written to&amp;nbsp;disk:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;variantqc_pass&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;variantqc_pass&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rf_prediction&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;TP&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;variantqc_pass&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;variantqc_pass.mt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;overwrite&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Filter loci by coordinates or allelic&amp;nbsp;frequency&lt;/h2&gt;
&lt;p&gt;Now the dataset has passed all &lt;span class="caps"&gt;QC&lt;/span&gt;, I may query it to search for new variants or answer other scientific questions. To illustrate that, I will filter the dataset to contain variants in delimited regions of the genome with a certain range of allelic frequencies. To parse specific regions from a genome, I can create a Python list of strings representing exact or approximate&amp;nbsp;coordinates:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;intervals&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;chr10:52765380-52772784&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;chr1:100M-200M&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I will apply the filter to a &lt;code&gt;MatrixTable&lt;/code&gt; with &lt;code&gt;hl.parse_locus_interval()&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;filtered_mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_intervals&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;variantqc_pass&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_locus_interval&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reference_genome&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;intervals&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I can also pinpoint an individual locus and create a window of nucleotides before and after it. In the code below I create a window of 100,000 nucleotides before and after a specific position in chromosome&amp;nbsp;X.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;locus&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;hl&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parse_locus&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;chrX:23833353&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;DEFAULT_REF&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;window&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;locus&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;filtered_mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;variantqc_pass&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;window&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contains&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;variantqc_pass&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;locus&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If I wanted to check the first few genotypes of the filtered &lt;code&gt;MatrixTable&lt;/code&gt; by the specified window I would use the command&amp;nbsp;below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;filtered_mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;GT&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;show&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Since this coordinate is not on the pseudoautosomal region (&lt;span class="caps"&gt;PAR&lt;/span&gt;) of the X chromosomes, karyotypically normal male individuals will be haploid around this region, and Hail would correctly show only one allele instead of two in the &lt;code&gt;GT&lt;/code&gt; call.&lt;/p&gt;
&lt;p&gt;Since I used &lt;code&gt;hl.variant_qc()&lt;/code&gt; at the beginning of variant &lt;span class="caps"&gt;QC&lt;/span&gt;, I may filter the variants by their allelic frequency. For example, If I wanted to keep only the variants with less than 1% frequency I would&amp;nbsp;do:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;filtered_mt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;filtered_mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;filter_rows&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filtered_mt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;variant_qc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AF&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Remember to &lt;code&gt;write()&lt;/code&gt; the &lt;code&gt;MatrixTable&lt;/code&gt; if you want to save the dataset with any applied&amp;nbsp;filters.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post&amp;nbsp;I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introduced the Hail&amp;nbsp;framework;&lt;/li&gt;
&lt;li&gt;Mentioned software used for preparing multi-sample &lt;span class="caps"&gt;VCF&lt;/span&gt; input starting with multiple gVCF&amp;nbsp;files;&lt;/li&gt;
&lt;li&gt;Demonstrated how to perform sample and variant &lt;span class="caps"&gt;QC&lt;/span&gt; with&amp;nbsp;Hail;&lt;/li&gt;
&lt;li&gt;Demonstrated how filter dataset according to quality metrics, locus or loci&amp;nbsp;interval.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This demonstration was made possible with the help of by insights acquired by reading through&amp;nbsp;the:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://discuss.hail.is/"&gt;Hail Discussion Forum&lt;/a&gt;&amp;nbsp;posts;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/populationgenomics/joint-calling"&gt;Centre for Population Genomics GitHub repository&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/broadinstitute/gnomad_methods"&gt;gnomADs&amp;#8217; utilities GitHub repository&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;gnomAD team&amp;#8217;s supplementary material from their &lt;a href="https://www.nature.com/articles/s41586-020-2308-7"&gt;2020 Nature paper&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;p&gt;In this post I gave general directions how to combine multiple gVCF files into one single &lt;span class="caps"&gt;VCF&lt;/span&gt; input. Hail actually has an experimental function that has this very purpose: &lt;a href="https://hail.is/docs/0.2/experimental/vcf_combiner.html"&gt;&lt;code&gt;hl.experimental.run_combiner()&lt;/code&gt;&lt;/a&gt;. However, I tried to use this function and had problems with it. It generates a &amp;#8220;sparse&amp;#8221; &lt;code&gt;MatrixTable&lt;/code&gt; and unfortunately I found the function documentation insufficiently clear on how to work with this slightly different form of intermediate input, so I resorted to &lt;span class="caps"&gt;GATK&lt;/span&gt;&amp;#8217;s &lt;code&gt;GenomicsDBImport&lt;/code&gt; as stated. Since Hail is in active development, I expect improvement on both the function and on its&amp;nbsp;documentation.&lt;/p&gt;
&lt;p&gt;Throughout the demonstration I used &lt;code&gt;write()&lt;/code&gt; method to write &lt;code&gt;MatrixTable&lt;/code&gt;s to disk and later read them back into the session with &lt;code&gt;read_matrix_table()&lt;/code&gt;. Alternatively I could have used Hail&amp;#8217;s &lt;code&gt;checkpoint()&lt;/code&gt; method as an alias for these sequential operations. Read the documentation &lt;a href="https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.checkpoint"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.einstein.br/Pages/Home.aspx"&gt;Hospital Israelita Albert&amp;nbsp;Einstein&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/"&gt;Hail |&amp;nbsp;Index&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.broadinstitute.org/"&gt;Broad&amp;nbsp;Institute&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gnomAD.broadinstitute.org/"&gt;gnomAD&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/#install"&gt;Hail |&amp;nbsp;Index&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pypi.org/project/gnomAD/"&gt;gnomAD module |&amp;nbsp;PyPi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gatk.broadinstitute.org/hc/en-us"&gt;Genomic Analysis&amp;nbsp;Toolkit&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://samtools.github.io/bcftools/bcftools.html"&gt;bcftools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;Setting Up Your Unix Computer for Bioinformatics&amp;nbsp;Analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gatk.broadinstitute.org/hc/en-us/articles/360035531812-GVCF-Genomic-Variant-Call-Format"&gt;Genomic Variant Call Format&amp;nbsp;(gVCF)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.illumina.com/products/by-type/informatics-products/dragen-bio-it-platform.html"&gt;Illumina &lt;span class="caps"&gt;DRAGEN&lt;/span&gt; Bio-&lt;span class="caps"&gt;IT&lt;/span&gt; Platform| Variant calling &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; secondary genomic analysis software&amp;nbsp;tool&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://samtools.github.io/hts-specs/VCFv4.2.pdf"&gt;Variant Call Format specification | version&amp;nbsp;4.2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gatk.broadinstitute.org/hc/en-us/articles/360036883491-GenomicsDBImport"&gt;&lt;span class="caps"&gt;GATK&lt;/span&gt; |&amp;nbsp;GenomicsDBImport&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gatk.broadinstitute.org/hc/en-us/articles/360035531112--How-to-Filter-variants-either-with-VQSR-or-by-hard-filtering"&gt;&lt;span class="caps"&gt;GATK&lt;/span&gt; | Variant Qualit Score Recalibration (&lt;span class="caps"&gt;VQSR&lt;/span&gt;)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://console.cloud.google.com/storage/browser/genomics-public-data/resources/broad/hg38/v0;tab=objects?prefix=&amp;amp;forceOnObjectsSortingFiltering=false"&gt;&lt;span class="caps"&gt;GATK&lt;/span&gt; Resource Bundle at Google&amp;nbsp;Cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://cloud.google.com/storage/docs/gsutil"&gt;gsutil tool | Google&amp;nbsp;Cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/index.html"&gt;Hail | Hail&amp;nbsp;0.2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/tutorials/07-matrixtable.html"&gt;Hail | MatrixTable&amp;nbsp;Tutorial&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/methods/genetics.html#hail.methods.sample_qc"&gt;Hail | Genetics |&amp;nbsp;sample_qc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.bokeh.org/en/latest/index.html"&gt;Bokeh&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/methods/genetics.html#hail.methods.hwe_normalized_pca"&gt;Hail | Genetics |&amp;nbsp;hwe_normalized_pca&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/methods/relatedness.html#hail.methods.pc_relate"&gt;Hail |&amp;nbsp;Relatedness&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/methods/misc.html#hail.methods.maximal_independent_set"&gt;Hail |&amp;nbsp;Miscellaneous&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/methods/genetics.html#hail.methods.variant_qc"&gt;Hail | Genetics |&amp;nbsp;variant_qc&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://gnomAD.broadinstitute.org/news/2018-10-gnomAD-v2-1/"&gt;gnomAD v2.1 | gnomAD&amp;nbsp;news&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/functions/genetics.html#hail.expr.functions.parse_locus_interval"&gt;Hail | Genetics&amp;nbsp;functions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://discuss.hail.is/"&gt;Hail&amp;nbsp;Discussion&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/populationgenomics/joint-calling"&gt;Centre for Population Genomics GitHub&amp;nbsp;repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/broadinstitute/gnomad_methods"&gt;gnomADs&amp;#8217; utilities GitHub&amp;nbsp;repository&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.nature.com/articles/s41586-020-2308-7"&gt;The mutational constraint spectrum quantified from variation in 141,456&amp;nbsp;humans&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/experimental/vcf_combiner.html"&gt;Hail | &lt;span class="caps"&gt;VCF&lt;/span&gt;&amp;nbsp;Combiner&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hail.is/docs/0.2/hail.MatrixTable.html#hail.MatrixTable.checkpoint"&gt;Hail | MatrixTable |&amp;nbsp;checkpoint&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Fri, 09 Jul 2021 16:45:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2021-07-09:/genomic-analysis-with-hail.html</guid><category>Python</category><category>Bioinformatics</category><category>Genomics</category><category>Hail</category></item><item><title>Making an Interactive Map with Shiny and Leaflet inÂ R</title><link>https://antoniocampos13.github.io/making-an-interactive-map-with-shiny-and-leaflet-in-r.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://shiny.rstudio.com/"&gt;Shiny&lt;/a&gt; is a R package developed and maintained by the &lt;a href="https://rstudio.com/"&gt;RStudio&lt;/a&gt; team. With Shiny, anyone can build interactive web apps to help data visualization. Here I present a simple template of an interactive Brazilian map displaying fictitious allelic frequencies with samples sizes across the country. It is a useful visualization for multicentric studies results and systematic reviews, for&amp;nbsp;example.&lt;/p&gt;
&lt;p&gt;As always, the code of this demo will be posted at my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/R/2021-02-18_Interactive_map_with_shiny"&gt;portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The intention of this interactive map is that the user can choose a gene and then a variant in two separate dropdown menus and check the allelic frequencies and sample sizes being automatically plotted on the map. See below a print screen of the final&amp;nbsp;product:&lt;/p&gt;
&lt;p&gt;&lt;img alt="The final product: an interactive Shiny/leaflet map" src="https://antoniocampos13.github.io/images/shiny_map_result.jpg"&gt;&lt;/p&gt;
&lt;h2&gt;Loading necessary&amp;nbsp;packages&lt;/h2&gt;
&lt;p&gt;First, I will load some packages that will help me create a toy&amp;nbsp;dataset:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;here&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dplyr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;openxlsx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ids&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;maps&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I have talked about &lt;code&gt;here&lt;/code&gt; and &lt;code&gt;dplyr&lt;/code&gt; (from &lt;code&gt;tidyverse&lt;/code&gt;) packages &lt;a href="https://antoniocampos13.github.io/data-manipulation-with-r.html"&gt;before&lt;/a&gt;. In my opinion, &lt;code&gt;openxlsx&lt;/code&gt; is the best option to read Excel spreadsheets, since it does not require external dependencies, such as Java, to work (I had some problems with Java before trying to read large spreadsheets in R). The package &lt;a href="https://cran.r-project.org/web/packages/ids/index.html"&gt;&lt;code&gt;ids&lt;/code&gt;&lt;/a&gt; serves to generate random or human readable and pronounceable identifiers. Lastly, the &lt;a href="https://cran.r-project.org/web/packages/maps/maps.pdf"&gt;&lt;code&gt;maps&lt;/code&gt;&lt;/a&gt; package will provide geographic coordinates of the Brazilian states capitals to help center the information in my interactive&amp;nbsp;map.&lt;/p&gt;
&lt;h2&gt;Creating the toy&amp;nbsp;dataset&lt;/h2&gt;
&lt;h3&gt;Creating data&amp;nbsp;points&lt;/h3&gt;
&lt;p&gt;The dataset will contain fictitious allele frequencies from samples across Brazil. Brazil is a large country that has 26 states plus a federal district, making it 27 federative units, but to simplify things, I will call it &amp;#8220;states&amp;#8221; hereafter. Now, let&amp;#8217;s imagine that I would genotype two variants from three genes each in all 27 states and determined the minor allele frequency by counting how many alleles were present among the sample size of the state. Let&amp;#8217;s assume the genes are located on autosomes (the non-sexual chromosomes). Thus, I would have &lt;code&gt;27 * 3 * 2 = 162&lt;/code&gt; data points corresponding to the allele frequency for each variant in each state. To make the script customizable, I assign every number to an&amp;nbsp;object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;GENES&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;VARIANTS&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;STATES&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;27&lt;/span&gt;
&lt;span class="n"&gt;DATAPOINTS&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;GENES&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;VARIANTS&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;STATES&lt;/span&gt;  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I will set the random seed to make some reproducible dataset and then create two vectors with the base R &lt;code&gt;sample()&lt;/code&gt; function. The first vector will contain 162 random numbers between 25 and 80 to represent the allele counts from the variants. The second vector will contain 27 random numbers between 100 and 500 to represent sample sizes for each state. Notice how I am multiplying by two, to ensure that sample sizes will be even numbers only, since every individual usually contributes two alleles to the sample&amp;nbsp;size:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;set.seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;123&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;alleles_count&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;25&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;80&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;DATAPOINTS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;replace&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;alleles_total&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nf"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;50&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;250&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;STATES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;replace&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, using the &lt;code&gt;ids&lt;/code&gt; package I create two more vectors: one representing fictitious genes and the other, fictitious variants, respectively with the &lt;code&gt;random_id()&lt;/code&gt; function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;genes&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;random_id&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;GENES&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_openssl&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;FALSE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;var_ids&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;random_id&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;GENES&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;VARIANTS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;use_openssl&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;FALSE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Notice the first argument is the number of desired random ids, so I put the constants I defined&amp;nbsp;before.&lt;/p&gt;
&lt;p&gt;Now I have two vectors that will generate the 162 data points (allelic frequencies). I will now prepare the geographic coordinates that will be needed when plotting the&amp;nbsp;map.&lt;/p&gt;
&lt;h3&gt;Getting the&amp;nbsp;coordinates&lt;/h3&gt;
&lt;p&gt;To simplify things, I listed all 27 state capitals in a spreadsheet titled &lt;code&gt;states_capitals.xlsx&lt;/code&gt;. I then loaded it in R and associated one sample size to one state (&lt;code&gt;alleles_states&lt;/code&gt; data&amp;nbsp;frame):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;br&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;read.xlsx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;states_capitals.xlsx&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;alleles_states&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;bind_cols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;br&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alleles_total&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alleles_total&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="State capitals spreadsheet" src="https://antoniocampos13.github.io/images/state_capitals.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Next, I filter a special dataset named &lt;code&gt;world.cities&lt;/code&gt; with the name of the Brazilian state capitals using &lt;code&gt;dplyr&lt;/code&gt; pipes. This huge dataset contains names, countries, latitude and longitude from several cities of the world and is imported by the &lt;code&gt;maps&lt;/code&gt; package when I loaded&amp;nbsp;it.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;dim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;world.cities&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# prints out --&amp;gt; [1] 43645     6&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Check the &lt;code&gt;dplyr&lt;/code&gt; pipes&amp;nbsp;below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;coords&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;world.cities&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;country.etc&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Brazil&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;%in%&lt;/span&gt; &lt;span class="n"&gt;br&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;capital&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lat&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="m"&gt;-26.48&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;br&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span class="nf"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;long&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now let me explain. First, I obtaining only Brazilian cities by using &lt;code&gt;filter(country.etc == "Brazil")&lt;/code&gt;. Then, I filtered the &lt;code&gt;name&lt;/code&gt; column to get only the Brazilian state capitals by using the keyword &lt;code&gt;%in%&lt;/code&gt; and using the &lt;code&gt;capital&lt;/code&gt; column of the &lt;code&gt;br&lt;/code&gt; data frame. The &lt;code&gt;filter(lat != -26.48)&lt;/code&gt; argument is to remove a city from ParanÃ¡ state that has the same name of the Tocantins state capital (Palmas). The &lt;code&gt;lat&lt;/code&gt; means the latitude column. Next, I create a new column with &lt;code&gt;mutate()&lt;/code&gt; to unite latitude and longitude coordinates to their respective capital. Notice that I could only do that because &lt;code&gt;br$capital&lt;/code&gt; filter maintained the same order of the cities as it was in the input spreadsheet. Finally, I clean the data frame up by selecting only the coordinates and the state abbreviation. The result is&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coords&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
 &lt;span class="n"&gt;lat&lt;/span&gt;   &lt;span class="n"&gt;long&lt;/span&gt; &lt;span class="n"&gt;state&lt;/span&gt;
&lt;span class="m"&gt;1&lt;/span&gt; &lt;span class="m"&gt;-10.91&lt;/span&gt; &lt;span class="m"&gt;-37.07&lt;/span&gt;    &lt;span class="n"&gt;SE&lt;/span&gt;
&lt;span class="m"&gt;2&lt;/span&gt;  &lt;span class="m"&gt;-1.44&lt;/span&gt; &lt;span class="m"&gt;-48.50&lt;/span&gt;    &lt;span class="n"&gt;PA&lt;/span&gt;
&lt;span class="m"&gt;3&lt;/span&gt; &lt;span class="m"&gt;-19.92&lt;/span&gt; &lt;span class="m"&gt;-43.94&lt;/span&gt;    &lt;span class="n"&gt;MG&lt;/span&gt;
&lt;span class="m"&gt;4&lt;/span&gt;   &lt;span class="m"&gt;2.83&lt;/span&gt; &lt;span class="m"&gt;-60.66&lt;/span&gt;    &lt;span class="n"&gt;RR&lt;/span&gt;
&lt;span class="m"&gt;5&lt;/span&gt; &lt;span class="m"&gt;-15.78&lt;/span&gt; &lt;span class="m"&gt;-47.91&lt;/span&gt;    &lt;span class="n"&gt;DF&lt;/span&gt;
&lt;span class="m"&gt;6&lt;/span&gt; &lt;span class="m"&gt;-20.45&lt;/span&gt; &lt;span class="m"&gt;-54.63&lt;/span&gt;    &lt;span class="n"&gt;MS&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Joining&amp;nbsp;everything&lt;/h3&gt;
&lt;p&gt;Now let&amp;#8217;s combine three vectors: state abbreviations, genes and variants in a single data frame. First I create a small data frame combining the gene ids with the variants ids to ensure each gene has two unique&amp;nbsp;variants:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;gene_var_comb&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;bind_cols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;rep&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;genes&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;each&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;VARIANTS&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;variant&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;var_ids&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I combine the state abbreviations vector with the variants vector, making the 162 data points skeleton, and with two inner joins, I can now identify each variant by its corresponding gene and each state with its corresponding sample&amp;nbsp;size:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;combinations&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;expand.grid&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;br&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;state&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;variant&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gene_var_comb&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;variant&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; 
  &lt;span class="nf"&gt;inner_join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene_var_comb&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;by&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;variant&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;inner_join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alleles_states&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;by&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;state&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;dim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;combinations&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# [1] 162   4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;See that it has the correct dimensions: 162 rows (data points) and four columns (state, gene, variant and sample size). Now let&amp;#8217;s complete the data frame by merging the &lt;code&gt;alleles_count&lt;/code&gt; vector, joining the coordinates and calculating the minor allele&amp;nbsp;frequency:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;map_data&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;bind_cols&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;combinations&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; 
&lt;span class="c1"&gt;#                  alleles_count = alleles_count) %&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;inner_join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coords&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;by&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;state&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;freq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;alleles_count&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;alleles_total&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The result is&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nf"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;map_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="n"&gt;state&lt;/span&gt;      &lt;span class="n"&gt;variant&lt;/span&gt;     &lt;span class="n"&gt;gene&lt;/span&gt; &lt;span class="n"&gt;alleles_total&lt;/span&gt; &lt;span class="n"&gt;alleles_count&lt;/span&gt;    &lt;span class="n"&gt;lat&lt;/span&gt;   &lt;span class="n"&gt;long&lt;/span&gt;       &lt;span class="n"&gt;freq&lt;/span&gt;
&lt;span class="m"&gt;1&lt;/span&gt;    &lt;span class="n"&gt;SE&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="n"&gt;cf8ac7cc786&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="n"&gt;b290a87&lt;/span&gt;           &lt;span class="m"&gt;426&lt;/span&gt;            &lt;span class="m"&gt;55&lt;/span&gt; &lt;span class="m"&gt;-10.91&lt;/span&gt; &lt;span class="m"&gt;-37.07&lt;/span&gt; &lt;span class="m"&gt;0.12910798&lt;/span&gt;
&lt;span class="m"&gt;2&lt;/span&gt;    &lt;span class="n"&gt;PA&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="n"&gt;cf8ac7cc786&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="n"&gt;b290a87&lt;/span&gt;           &lt;span class="m"&gt;264&lt;/span&gt;            &lt;span class="m"&gt;39&lt;/span&gt;  &lt;span class="m"&gt;-1.44&lt;/span&gt; &lt;span class="m"&gt;-48.50&lt;/span&gt; &lt;span class="m"&gt;0.14772727&lt;/span&gt;
&lt;span class="m"&gt;3&lt;/span&gt;    &lt;span class="n"&gt;MG&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="n"&gt;cf8ac7cc786&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="n"&gt;b290a87&lt;/span&gt;           &lt;span class="m"&gt;176&lt;/span&gt;            &lt;span class="m"&gt;75&lt;/span&gt; &lt;span class="m"&gt;-19.92&lt;/span&gt; &lt;span class="m"&gt;-43.94&lt;/span&gt; &lt;span class="m"&gt;0.42613636&lt;/span&gt;
&lt;span class="m"&gt;4&lt;/span&gt;    &lt;span class="n"&gt;RR&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="n"&gt;cf8ac7cc786&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="n"&gt;b290a87&lt;/span&gt;           &lt;span class="m"&gt;206&lt;/span&gt;            &lt;span class="m"&gt;38&lt;/span&gt;   &lt;span class="m"&gt;2.83&lt;/span&gt; &lt;span class="m"&gt;-60.66&lt;/span&gt; &lt;span class="m"&gt;0.18446602&lt;/span&gt;
&lt;span class="m"&gt;5&lt;/span&gt;    &lt;span class="n"&gt;DF&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="n"&gt;cf8ac7cc786&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="n"&gt;b290a87&lt;/span&gt;           &lt;span class="m"&gt;372&lt;/span&gt;            &lt;span class="m"&gt;27&lt;/span&gt; &lt;span class="m"&gt;-15.78&lt;/span&gt; &lt;span class="m"&gt;-47.91&lt;/span&gt; &lt;span class="m"&gt;0.07258065&lt;/span&gt;
&lt;span class="m"&gt;6&lt;/span&gt;    &lt;span class="n"&gt;MS&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="n"&gt;cf8ac7cc786&lt;/span&gt; &lt;span class="m"&gt;6&lt;/span&gt;&lt;span class="n"&gt;b290a87&lt;/span&gt;           &lt;span class="m"&gt;496&lt;/span&gt;            &lt;span class="m"&gt;66&lt;/span&gt; &lt;span class="m"&gt;-20.45&lt;/span&gt; &lt;span class="m"&gt;-54.63&lt;/span&gt; &lt;span class="m"&gt;0.13306452&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, I will save the toy dataset into a R object that will be loaded when we launch the Shiny app. I saved inside the &lt;code&gt;map/data&lt;/code&gt; folder for reasons that will be clear in a&amp;nbsp;moment.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;map_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;map&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;map_data.RData&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Creating named list of genes and&amp;nbsp;variants&lt;/h2&gt;
&lt;p&gt;I need a named list of genes and variants to make the dropdown menus as intended. So, again I used some &lt;code&gt;dplyr&lt;/code&gt; pipes and a &lt;code&gt;lapply&lt;/code&gt; loop to accomplish&amp;nbsp;it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;gene_list&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;map_data&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="nf"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;variant&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;distinct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;variant&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;group_by&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;varstring&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;variant&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;collapse&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;variant&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
  &lt;span class="nf"&gt;distinct&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;varstring&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;genes_variants&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;lapply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;seq_along&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene_list&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="nf"&gt;unlist&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;strsplit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene_list&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;varstring&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;,&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;genes_variants&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;gene_list&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;

&lt;span class="nf"&gt;save&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;genes_variants&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;map&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;genes_variants.RData&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The result is a named list: each element of the list is a vector, and each of these vectors are named after a gene. Each vector, in turn, contains the corresponding&amp;nbsp;variants:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;genes_variants&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;`6b290a87`&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;6cf8ac7cc786&amp;quot;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;c1c7fc20d1b9&amp;quot;&lt;/span&gt;

&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;`153a782d`&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;6aab8d277c3c&amp;quot;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;4c9ff4a7f4fb&amp;quot;&lt;/span&gt;

&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;b053b654&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;adf90f0908c9&amp;quot;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;c523d6d80697&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Notice that I also saved it into the &lt;code&gt;map/data&lt;/code&gt; folder.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
âââ map
â   âââ data
â       âââ genes_variants.RData
â       âââ map_data.RData
âââ map_data.R # contains the code demonstrated here
âââ state_capitals.xlsx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Creating Shiny &lt;code&gt;ui.R&lt;/code&gt; file&lt;/h2&gt;
&lt;p&gt;We can divide the Shiny app internals in two main functions: &lt;code&gt;ui&lt;/code&gt; and &lt;code&gt;server&lt;/code&gt;. The former will manage the user interface of the app (the front-end) and the latter will manipulate the data for interactive visualization (the back-end). I will save each function into two separate files, &lt;code&gt;server.R&lt;/code&gt; and &lt;code&gt;ui.R&lt;/code&gt;. Let&amp;#8217;s examine the &lt;code&gt;ui.R&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;ui&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;fluidPage&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="nf"&gt;titlePanel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Toy Dataset: Variants in Brazil&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;

  &lt;span class="nf"&gt;sidebarLayout&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="nf"&gt;sidebarPanel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="nf"&gt;selectInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;gene&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Choose a gene&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;genes_variants&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
      &lt;span class="nf"&gt;selectInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;variant&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Choose a variant&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;genes_variants&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]),&lt;/span&gt;
    &lt;span class="p"&gt;),&lt;/span&gt;


    &lt;span class="nf"&gt;mainPanel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
      &lt;span class="nf"&gt;h4&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;The circles&amp;#39; diameters are proportional to sample size. Hover them with the cursor to see allelic frequencies. Click to see sample size (number of alleles).&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;

      &lt;span class="nf"&gt;leafletOutput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outputId&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;map&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;fluidPage()&lt;/code&gt; Shiny function creates an webpage that fits browser dimensions and generate the overall layout of the app. This function receives other functions as arguments. Each function will take care of one aspect of the layout. The first function, &lt;code&gt;titlePanel()&lt;/code&gt; generates the main title of the app, the second, &lt;code&gt;sidebarLayout()&lt;/code&gt; instructs Shiny to create an app with a sidebar and a main panel. The arguments of this function will control the elements inside each&amp;nbsp;area.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;sidebarPanel()&lt;/code&gt; function builds the sidebar. Since I wanted the dropdown menus to be placed in the sidebar, I enclose two &lt;code&gt;selectInput()&lt;/code&gt; functions, one for the genes and one for the variants. Notice that it requires three arguments: an internal name (&amp;#8220;genes&amp;#8221; or &amp;#8220;variant&amp;#8221;) so the &lt;code&gt;server&lt;/code&gt; function can access the inputs, a human-readable label (&amp;#8220;Choose a gene&amp;#8221;, &amp;#8220;Choose a variant&amp;#8221;) and the input options. Notice that I am referring to the &lt;code&gt;genes_variants&lt;/code&gt; named list I created in the previous&amp;nbsp;step.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;mainPanel()&lt;/code&gt; function builds the main panel. The map will be placed there. The &lt;code&gt;hx()&lt;/code&gt; function creates a header, where &lt;code&gt;x&lt;/code&gt; is an integer between 1 and 6. The lower the number, the higher is the level of the header. Thus, &lt;code&gt;h1()&lt;/code&gt; generates big headers, &lt;code&gt;h2()&lt;/code&gt; generates a smaller header and so on. I chose &lt;code&gt;h4()&lt;/code&gt; to enclose a brief description of how to interact with the map. The &lt;code&gt;leafletOutput()&lt;/code&gt; function will build the map. It is a function from the &lt;code&gt;leaflet&lt;/code&gt; package. The argument &lt;code&gt;outputId&lt;/code&gt; creates the internal name of the output to be accessed by the &lt;code&gt;server&lt;/code&gt; function.&lt;/p&gt;
&lt;h2&gt;Creating Shiny &lt;code&gt;server.R&lt;/code&gt; file&lt;/h2&gt;
&lt;p&gt;It contains the &lt;code&gt;server&lt;/code&gt; function that receive three arguments: &lt;code&gt;input&lt;/code&gt;, &lt;code&gt;output&lt;/code&gt; and &lt;code&gt;session&lt;/code&gt;. Notice that we must wrote the &lt;code&gt;server&lt;/code&gt; function, whereas the &lt;code&gt;ui&lt;/code&gt; function is mostly controlled by the Shiny package itself. See&amp;nbsp;below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;server&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;session&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

  &lt;span class="nf"&gt;observe&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="nf"&gt;updateSelectInput&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;session&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;variant&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;choices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;genes_variants&lt;/span&gt;&lt;span class="p"&gt;[[&lt;/span&gt;&lt;span class="n"&gt;input&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="p"&gt;]])&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;

  &lt;span class="n"&gt;data_subset&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;reactive&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;map_data&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="nf"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;input&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;variant&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;input&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;variant&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nf"&gt;return&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;

  &lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;renderLeaflet&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="nf"&gt;leaflet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;data_subset&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
      &lt;span class="nf"&gt;setView&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;-14.235004&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lng&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;-51.92528&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;zoom&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
      &lt;span class="nf"&gt;addTiles&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt;
      &lt;span class="nf"&gt;addCircles&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;lat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;lat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;lng&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;long&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;weight&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;radius&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;alleles_total&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="m"&gt;5000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;popup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Alleles: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alleles_total&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="nf"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;input&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;variant&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot; Allele frequency: &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nf"&gt;round&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;freq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;digits&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))),&lt;/span&gt;
        &lt;span class="n"&gt;fillOpacity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.5&lt;/span&gt;
      &lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The user options during the interaction with the app will be stored into the &lt;code&gt;input&lt;/code&gt; object. The &lt;code&gt;output&lt;/code&gt; will access the output id defined in the &lt;code&gt;ui.R&lt;/code&gt; file (&amp;#8220;map&amp;#8221;). The &lt;code&gt;session&lt;/code&gt; object will keep track of the different options of the user. For example, the user must choose a gene. Then, the variant dropdown menu must change accordingly to display the variants of said gene. This is why I included the &lt;code&gt;observe()&lt;/code&gt; function. Anytime the user chooses a different gene, the &lt;code&gt;updateSelectInput()&lt;/code&gt; will change the &amp;#8220;variant&amp;#8221; &lt;code&gt;selectInput()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Each user selection will display different data on the map. Therefore, the &lt;code&gt;data_subset(reactive())&lt;/code&gt; nested functions will take the user input and &lt;strong&gt;filter&lt;/strong&gt; the desired data stored in the &lt;code&gt;map_data&lt;/code&gt; object I created earlier. Notice that &lt;code&gt;input$gene&lt;/code&gt; and &lt;code&gt;input$variant&lt;/code&gt; are referring to the &lt;code&gt;selectInput()&lt;/code&gt; defined in the &lt;code&gt;ui.R&lt;/code&gt; file. They get the user option and populate the&amp;nbsp;filters.&lt;/p&gt;
&lt;p&gt;Lastly, the &lt;code&gt;renderLeaflet()&lt;/code&gt; function output (controlled by the definitions enclosed in the &lt;code&gt;leaflet()&lt;/code&gt; function) will be assigned to the &lt;code&gt;output$map&lt;/code&gt; object. &amp;#8220;map&amp;#8221; is referring to the &lt;code&gt;outputId&lt;/code&gt; defined in the &lt;code&gt;ui.R&lt;/code&gt; file, therefore making the map appear on the main&amp;nbsp;panel.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s talk more about the &lt;code&gt;leaflet()&lt;/code&gt; function arguments. First, it receives as input the &lt;code&gt;data_subset()&lt;/code&gt; output, which is the filtered data based on the user input. Using &lt;code&gt;dplyr&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s pipes, I pass other options to the function. The &lt;code&gt;setView()&lt;/code&gt; function serves to center the map on the specified latitude and longitude, as well as the zoom level. The &lt;code&gt;addCircles()&lt;/code&gt; will plot circles based on the geographic coordinates stored in the filtered &lt;code&gt;map_data&lt;/code&gt; object. To make circle size appear proportional as the sample size, I multiply the square root of the sample size by a constant. Next, I created two labels: one that appears on clicking and the other appears on hovering the circle so the user can see sample size and allele frequency,&amp;nbsp;respectively.&lt;/p&gt;
&lt;h2&gt;Creating &lt;code&gt;global.R&lt;/code&gt; file&lt;/h2&gt;
&lt;p&gt;The optional &lt;code&gt;global.R&lt;/code&gt; file contents are read during the Shiny app initialization and are placed in global scope during the app execution session. Therefore, it is useful when there is the need to load data and packages necessary for the app buildup, which is exactly my case: I need to load the map data, the gene lists, and the &lt;code&gt;leaflet&lt;/code&gt; and &lt;code&gt;dplyr&lt;/code&gt; packages. Provided that the &lt;code&gt;ui&lt;/code&gt; and &lt;code&gt;server&lt;/code&gt; functions are saved in two different files (&lt;code&gt;ui.R&lt;/code&gt; and &lt;code&gt;server.R&lt;/code&gt;), the &lt;code&gt;global.R&lt;/code&gt; is automatically loaded during app&amp;nbsp;initialization.&lt;/p&gt;
&lt;p&gt;Check the &lt;code&gt;global.R&lt;/code&gt; file&amp;nbsp;contents:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;data/map_data.RData&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;data/genes_variants.RData&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="nf"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;leaflet&lt;/span&gt;&lt;span class="p"&gt;)){&lt;/span&gt;
  &lt;span class="nf"&gt;install.packages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;leaflet&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;leaflet.extras&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
  &lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;leaflet&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nf"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="nf"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dplyr&lt;/span&gt;&lt;span class="p"&gt;)){&lt;/span&gt;
  &lt;span class="nf"&gt;install.packages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;dplyr&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dplyr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;First, I load the data stored into the &lt;code&gt;map/data&lt;/code&gt; folder. Next, the &lt;code&gt;if(!require()){}&lt;/code&gt; constructs will check if the packages exist and load them or install and then load them&amp;nbsp;otherwise.&lt;/p&gt;
&lt;h2&gt;Executing the Shiny&amp;nbsp;app&lt;/h2&gt;
&lt;p&gt;It is very simple to execute a Shiny app locally. Simply place the &lt;code&gt;ui.R&lt;/code&gt;, &lt;code&gt;server.R&lt;/code&gt; and &lt;code&gt;global.R&lt;/code&gt; (if it exists) inside a folder. I placed everything into the &lt;code&gt;map&lt;/code&gt; folder:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
âââ map
â   âââ global.R
â   âââ server.R
â   âââ ui.R
â   âââ data
â       âââ genes_variants.RData
â       âââ map_data.RData
âââ map_data.R
âââ state_capitals.xlsx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, simply run the &lt;code&gt;runApp()&lt;/code&gt; Shiny function with the name of the folder (install Shiny if you have not&amp;nbsp;yet):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt;&lt;span class="nf"&gt;require&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shiny&lt;/span&gt;&lt;span class="p"&gt;)){&lt;/span&gt;
  &lt;span class="nf"&gt;install.packages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;shiny&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;shiny&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nf"&gt;runApp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;map&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# or&lt;/span&gt;
&lt;span class="c1"&gt;# shiny::runApp(&amp;quot;map&amp;quot;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Just make sure the R session current working directory is the parent of the Shiny app folder, otherwise it will not work. Since I am running RStudio, a window containing the interactive map opens up and I can interact with the map. Otherwise you would have to open a browser window and go to the address displayed on the R console, something like &lt;code&gt;http://127.0.0.1:&amp;lt;some_port&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The interactive map is rather barebones, but it works. Try to replicate and improve it if you&amp;nbsp;wish!&lt;/p&gt;
&lt;h2&gt;Hosting Shiny apps on the&amp;nbsp;web&lt;/h2&gt;
&lt;p&gt;This demo showed how to locally execute a Shiny app. There are some options if you wish to host your app on the web to everyone use, both free and paid. For example, &lt;a href="https://www.shinyapps.io/"&gt;RStudio shinyapps.io&lt;/a&gt; offers free limited hosting of up to three apps. &lt;a href="https://www.digitalocean.com/products/droplets/"&gt;DigitalOcean&lt;/a&gt; offer small-scale paid services that can host Shiny apps (check a &lt;a href="https://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/"&gt;great tutorial by Dean Attali&lt;/a&gt;).&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In conclusion,&amp;nbsp;I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Created toy data simulating allele counts across several regions of&amp;nbsp;Brazil;&lt;/li&gt;
&lt;li&gt;Showed how to obtain geographic&amp;nbsp;coordinates;&lt;/li&gt;
&lt;li&gt;Demonstrated how to create an interactive map with Shiny to visualize sample sizes and allelic frequencies displayed on a Brazilian&amp;nbsp;map.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://shiny.rstudio.com/"&gt;Shiny&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://rstudio.com/"&gt;RStudio | Open source &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; professional software for data science&amp;nbsp;teams&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/data-manipulation-with-r.html"&gt;Data manipulation with&amp;nbsp;R&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://cran.r-project.org/web/packages/ids/index.html"&gt;&lt;span class="caps"&gt;CRAN&lt;/span&gt; - Package&amp;nbsp;ids&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://cran.r-project.org/web/packages/maps/maps.pdf"&gt;&lt;span class="caps"&gt;CRAN&lt;/span&gt; - Package&amp;nbsp;maps&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.shinyapps.io/"&gt;shinyapps.io&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.digitalocean.com/products/droplets/"&gt;Droplets - Scalable Virtual Machines |&amp;nbsp;DigitalOcean&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://deanattali.com/2015/05/09/setup-rstudio-shiny-server-digital-ocean/"&gt;How to get your very own RStudio Server and Shiny Server with&amp;nbsp;DigitalOcean&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Thu, 18 Feb 2021 09:00:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2021-02-18:/making-an-interactive-map-with-shiny-and-leaflet-in-r.html</guid><category>R</category><category>shiny</category><category>leaflet</category><category>data visualization</category><category>web app</category></item><item><title>How to Query Ensembl BioMart withÂ Python</title><link>https://antoniocampos13.github.io/how-to-query-ensembl-biomart-with-python.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently, me and my colleagues wrote a manuscript involving meta-analysis of &lt;span class="caps"&gt;RNA&lt;/span&gt;-Seq studies. One of my tasks of this project was to perform a Gene Ontology (&lt;span class="caps"&gt;GO&lt;/span&gt;) enrichment analysis: &lt;a href="http://geneontology.org/docs/go-enrichment-analysis/"&gt;&amp;#8220;[G]iven a set of genes that are up-regulated under certain conditions, an enrichment analysis will find which &lt;span class="caps"&gt;GO&lt;/span&gt; terms are over-represented (or under-represented) using annotations for that gene set&amp;#8221;&lt;/a&gt;. In other words, I could verify which cellular pathways were in action during the experimental&amp;nbsp;conditions.&lt;/p&gt;
&lt;p&gt;After I finished the &lt;span class="caps"&gt;GO&lt;/span&gt; analysis, I got a spreadsheet with a list of &lt;span class="caps"&gt;GO&lt;/span&gt; terms &amp;mdash; brief descriptions of the cellular process performed by each pathway. When I was thinking about the pathways, I started to wonder: &amp;#8220;which proteins participate into each pathway?&amp;#8221; So I decided to go data mining the &lt;a href="https://m.ensembl.org/biomart/martview"&gt;Ensembl BioMart&lt;/a&gt; to find out those protein&amp;nbsp;genes.&lt;/p&gt;
&lt;p&gt;Ensembl is a huge project by the European Bioinformatics Institute and the Wellcome Trust Sanger Institute to provide databases of annotated genomes for several (mainly vertebrate) species. BioMart is one of their data mining tools. In this post, I will describe how I used Python to query BioMart. I will introduce a simple function to generate &lt;a href="https://www.gnu.org/software/wget/"&gt;&lt;code&gt;GNU Wget&lt;/code&gt;&lt;/a&gt; commands to retrieve query results via &lt;a href="https://en.wikipedia.org/wiki/Representational_state_transfer"&gt;RESTful&lt;/a&gt; access. Then, I will show how I aggregated the data to met my objective (i.e. list all genes participating in a biological pathway represented by a &lt;span class="caps"&gt;GO&lt;/span&gt;&amp;nbsp;term).&lt;/p&gt;
&lt;p&gt;The code and example files presented here are available in my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2021_01_18_How_to_Query_BioMart_Python"&gt;portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prepare&amp;nbsp;identifiers&lt;/h2&gt;
&lt;p&gt;I created a project folder where I saved the Python script with this demonstration&amp;#8217;s code (&lt;code&gt;ensembl_rest.py&lt;/code&gt;) and two subfolders. The &lt;code&gt;data&lt;/code&gt; folder holds the example data: a spreadsheet named &lt;code&gt;go_demo.xlsx&lt;/code&gt;. The &lt;code&gt;src&lt;/code&gt; folder contains the functions&amp;#8217; code. I will write about them&amp;nbsp;later.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
âââ data
â   âââ go_demo.xlsx
âââ src
â   âââ files_to_pandas.py
â   âââ lists_ensembl.R
â   âââ query_biomart.py
âââ ensembl_rest.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The spreadsheet contains just two rows of data (so that the computation can be completed quickly). They are derived from a &lt;span class="caps"&gt;GO&lt;/span&gt; enrichment analysis output performed by &lt;a href="https://www.rdocumentation.org/packages/limma/versions/3.28.14/topics/goana"&gt;&lt;code&gt;limma&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s package &lt;code&gt;goana&lt;/code&gt; function&lt;/a&gt; available for R software. The &lt;span class="caps"&gt;GO&lt;/span&gt; ids were the &lt;strong&gt;identifiers&lt;/strong&gt; (search keywords) I used to query BioMart. You may use whatever identifier recognized by BioMart (more on that later). Just be sure they are on a nicely-named column so you can read it into&amp;nbsp;Python.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Print screen of identifiers file" src="https://antoniocampos13.github.io/images/go_demo_xlsx.png"&gt;&lt;/p&gt;
&lt;h2&gt;Load modules and set file&amp;nbsp;paths&lt;/h2&gt;
&lt;p&gt;These are the modules I&amp;nbsp;used:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;glob&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;subprocess&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;collections&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;dask.dataframe&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;dd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;dask&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;delayed&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;src.files_to_pandas&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;files_to_pandas&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;src.query_biomart&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;query_biomart&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first four modules are from Python&amp;#8217;s standard library. I already used &lt;code&gt;dask&lt;/code&gt; &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-2.html"&gt;before&lt;/a&gt;. If you do not have &lt;code&gt;dask&lt;/code&gt; or &lt;code&gt;pandas&lt;/code&gt; installed, do it now with &lt;code&gt;pip&lt;/code&gt;. I installed &lt;a href="https://openpyxl.readthedocs.io/en/stable/"&gt;&lt;code&gt;openpyxl&lt;/code&gt;&lt;/a&gt; as well, since it serves to read/write Excel&amp;nbsp;spreadsheets.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;pip install &lt;span class="s2"&gt;&amp;quot;dask[complete]&amp;quot;&lt;/span&gt; pandas openpyxl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Observe that I am explicitly loading the functions by prefixing the modules names with &lt;code&gt;src.&lt;/code&gt; (remember that we can call every Python script a&amp;nbsp;module).&lt;/p&gt;
&lt;p&gt;Using &lt;a href="https://docs.python.org/3/library/pathlib.html"&gt;&lt;code&gt;pathlib.Path&lt;/code&gt;&lt;/a&gt; I nicely set the folders and files paths, using the project folder as the&amp;nbsp;root.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resolve&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="c1"&gt;# project root&lt;/span&gt;
&lt;span class="n"&gt;data_folder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;go_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;go_demo.xlsx&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I loaded the identifiers file into a &lt;code&gt;pandas.DataFrame&lt;/code&gt; with the help of &lt;code&gt;openpyxl&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_excel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;go_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;engine&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;openpyxl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Configure the queries: datasets, filters, values and&amp;nbsp;attributes&lt;/h2&gt;
&lt;p&gt;Now let&amp;#8217;s examine an excerpt from the &lt;code&gt;query_biomart()&lt;/code&gt; function code. It contains the necessary modules, the function&amp;#8217;s arguments and some of their types hinted with the help of &lt;code&gt;typing&lt;/code&gt; module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;re&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;typing&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;query_biomart&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="n"&gt;filter_names&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="n"&gt;attribute_names&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="n"&gt;dataset_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;hsapiens_gene_ensembl&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;formatter&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;TSV&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;bool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;uniqueRows&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;bool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

&lt;span class="c1"&gt;# ...Function here ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There is a description of each argument&amp;nbsp;below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;filter_names&lt;/code&gt; (List[str]): A list of strings that define restrictions on the&amp;nbsp;query.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;values&lt;/code&gt; (List[str]): A list of strings containing the values that will be used for the filters. Must have same length of &lt;code&gt;filter_names&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;attribute_names&lt;/code&gt; (List[str]): A list of strings that define the data characteristics that must be retrieved for the filtered&amp;nbsp;data.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dataset_name&lt;/code&gt; (str, optional): A string indicating which dataset will be queried. Each species has their respective dataset. Defaults to &amp;#8220;hsapiens_gene_ensembl&amp;#8221;&amp;nbsp;(humans).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;formatter&lt;/code&gt; (str, optional): A string to indicate how output must be formatted. Options: &amp;#8220;&lt;code&gt;HTML&lt;/code&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;, &amp;#8220;&lt;code&gt;CSV&lt;/code&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;,&amp;#8221;&lt;code&gt;TSV&lt;/code&gt;&amp;#8221; and &amp;#8220;&lt;code&gt;XLS&lt;/code&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;. Defaults to &amp;#8220;&lt;code&gt;TSV&lt;/code&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;header&lt;/code&gt; (bool, optional): A Boolean indicating if the output must have a header. Defaults to &lt;code&gt;False&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;uniqueRows&lt;/code&gt; (bool, optional): A Boolean indicating if the output must have unique rows (deduplicate data). Defaults to &lt;code&gt;False&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Thus I searched&amp;nbsp;BioMart:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Into &lt;em&gt;Homo sapiens&lt;/em&gt; &lt;strong&gt;dataset&lt;/strong&gt;&amp;nbsp;(&amp;#8220;hsapiens_gene_ensembl&amp;#8221;),&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Filtering&lt;/strong&gt; by &lt;span class="caps"&gt;GO&lt;/span&gt; terms&amp;nbsp;(&amp;#8220;go_parent_term&amp;#8221;),&lt;/li&gt;
&lt;li&gt;Using &lt;span class="caps"&gt;GO&lt;/span&gt; ids (for example, &lt;span class="caps"&gt;GO&lt;/span&gt;:0002790) as search keywords (&lt;strong&gt;values&lt;/strong&gt;),&lt;/li&gt;
&lt;li&gt;And wanted to retrieve the gene name &lt;strong&gt;attribute&lt;/strong&gt;&amp;nbsp;(&amp;#8220;external_gene_name&amp;#8221;).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here is an example of the function with the inputs above. All defaults were maintained, except for &lt;code&gt;uniqueRows&lt;/code&gt;, which I set to &lt;code&gt;True&lt;/code&gt; to remove duplicate&amp;nbsp;data:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;query_biomart&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filter_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go_parent_term&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;GO:0002790&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;attribute_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;external_gene_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;uniqueRows&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;See the Appendix to help you see which information can be searched in&amp;nbsp;BioMart.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The output of the function is a string &amp;dash; a ready-to-use &lt;code&gt;GNU Wget&lt;/code&gt; Unix program command. The query string inside the command has a &lt;a href="https://m.ensembl.org/info/data/biomart/biomart_restful.html"&gt;&lt;code&gt;XML&lt;/code&gt; format&lt;/a&gt; that is sent to Ensembl&amp;#8217;s servers. The query result will then be saved on a file named based on the values (search keywords) of the query. The extension of the file depends on the &lt;code&gt;formatter&lt;/code&gt; argument.&lt;/p&gt;
&lt;p&gt;I created a &lt;code&gt;Bash&lt;/code&gt; script named &lt;code&gt;commands.sh&lt;/code&gt; inside the &lt;code&gt;data&lt;/code&gt; folder to backup and document my work. I did this with a &lt;code&gt;for&lt;/code&gt; loop. In other words, Python wrote for me a series of &lt;code&gt;Wget&lt;/code&gt; commands, one for each keyword in the identifiers data frame (&lt;code&gt;go_ids&lt;/code&gt; column &amp;dash; &lt;code&gt;df["go_ids"]&lt;/code&gt;) alongside the desired filter and&amp;nbsp;attribute:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;commands.sh&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;w&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;newline&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;commands&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;

&lt;span class="n"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;#!/bin/bash&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;go_id&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go_ids&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
   &lt;span class="n"&gt;commands&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query_biomart&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filter_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go_parent_term&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
   &lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;go_id&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;attribute_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;external_gene_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
   &lt;span class="n"&gt;uniqueRows&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;See a print screen of the &lt;code&gt;commands.sh&lt;/code&gt; file below. Notice the shebang (&lt;code&gt;#!&lt;/code&gt;) line: it ensures the file can be executed by &lt;code&gt;Bash&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Print screen commands.sh file" src="https://antoniocampos13.github.io/images/commands_print.png"&gt;&lt;/p&gt;
&lt;h2&gt;Execute the&amp;nbsp;queries&lt;/h2&gt;
&lt;p&gt;I executed the &lt;code&gt;commands.sh&lt;/code&gt; file with the help of Python&amp;#8217;s &lt;code&gt;subprocess&lt;/code&gt; library. Notice I indicated the path of the file by converting the &lt;code&gt;libpath.Path()&lt;/code&gt; address to a string with &lt;code&gt;str()&lt;/code&gt; and changed directories with the &lt;code&gt;cwd&lt;/code&gt; argument so that the queries results would be downloaded into the &lt;code&gt;data&lt;/code&gt; folder as&amp;nbsp;well.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;call&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;commands.sh&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;cwd&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;data_folder&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I waited for the download completion. The print screen below shows part of the &lt;code&gt;wget&lt;/code&gt; progress.&lt;/p&gt;
&lt;p&gt;&lt;img alt="wget download progress" src="https://antoniocampos13.github.io/images/wget_download.png"&gt;&lt;/p&gt;
&lt;p&gt;After a while, two files, named &lt;code&gt;GO0002790.txt&lt;/code&gt; and &lt;code&gt;GO0019932.txt&lt;/code&gt; were created, each containing a list of several genes related with the &lt;span class="caps"&gt;GO&lt;/span&gt;&amp;nbsp;ids:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
âââ data
â   âââ commands.sh
â   âââ go_demo.xlsx
â   âââ GO0002790.txt
â   âââ GO0019932.txt
â   âââ lists_ensembl.xlsx
âââ src
â   âââ files_to_pandas.py
â   âââ lists_ensembl.R
â   âââ query_biomart.py
âââ ensembl_rest.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="Result excerpt" src="https://antoniocampos13.github.io/images/go0002790.png"&gt;&lt;/p&gt;
&lt;h2&gt;Label and unify data into a &lt;code&gt;pandas.DataFrame&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Similarly to my &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-2.html"&gt;previous post&lt;/a&gt; I needed to make a relation of &lt;span class="caps"&gt;GO&lt;/span&gt; id &amp;#8212;&amp;gt; genes, so I adapted and renamed the old function I used on that occasion. The new function is named &lt;code&gt;files_to_pandas&lt;/code&gt;. It takes a file and converts it into a &lt;code&gt;pandas.DataFrame&lt;/code&gt; while creating a new column to accommodate the file name into a new column (in our case, the files were named after the values &amp;dash; &lt;span class="caps"&gt;GO&lt;/span&gt;&amp;nbsp;ids).&lt;/p&gt;
&lt;p&gt;The idea is that I needed to create one data frame for each file. Thus, I used the &lt;code&gt;glob&lt;/code&gt; method to get an iterable of file names and used &lt;code&gt;dask&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;a href="https://docs.dask.org/en/latest/delayed.html"&gt;&lt;code&gt;delayed&lt;/code&gt;&lt;/a&gt; method to assemble the several &lt;code&gt;pandas.DataFrames&lt;/code&gt; generated by the &lt;code&gt;files_to_pandas&lt;/code&gt; function loop into a unified &lt;code&gt;dask.DataFrame&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;file_list&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data_folder&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;glob&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;*.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dfs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;delayed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;files_to_pandas&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;file_list&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;ddf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_delayed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dfs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Reorganize data with &lt;code&gt;defaultdict&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;I could have ended in the previous step, but I surmised that I could condense the data frame into fewer rows, by aggregating all genes into the same row of the equivalent &lt;span class="caps"&gt;GO&lt;/span&gt; id. So I had the idea to use Python&amp;#8217;s &lt;a href="https://docs.python.org/3/library/collections.html#collections.defaultdict"&gt;&lt;code&gt;defaultdict&lt;/code&gt;&lt;/a&gt;. It is a subclass of the &lt;code&gt;dictionary&lt;/code&gt; class. First, I initialize an empty &lt;code&gt;defaultdict&lt;/code&gt;. It will have &lt;span class="caps"&gt;GO&lt;/span&gt; ids as keys and a list of gene names as&amp;nbsp;values:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;go_to_genes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;defaultdict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I simply had to loop through the rows of the &lt;code&gt;dask.DataFrame&lt;/code&gt;, insert the keys into the &lt;code&gt;defaultdict&lt;/code&gt; and appending the gene names into the value-list one by&amp;nbsp;one.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;go_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gene&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ddf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;filename&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;ddf&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gene_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="n"&gt;go_to_genes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;go_id&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Save the &lt;code&gt;defaultdict&lt;/code&gt; into a new &lt;code&gt;pandas.DataFrame&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Finally, I created another data frame to store the contents of the &lt;code&gt;defaultdict&lt;/code&gt;. Notice how I used the &lt;code&gt;.items()&lt;/code&gt; method. It is the correct way to access the contents of a &lt;code&gt;defaultdict&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;go_genes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;go_to_genes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;items&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go_ids&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;gene_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;Error&amp;nbsp;checking&lt;/h3&gt;
&lt;p&gt;Just to be sure that all queries worked correctly, a used a simple loop to check the contents of the gene names&amp;nbsp;column:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;go_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gene&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;go_genes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go_ids&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;&lt;span class="n"&gt;go_genes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gene_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Query&amp;quot;&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Error in &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;go_id&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;continue&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Error check done.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I am using &amp;#8220;Query&amp;#8221; is used as an example, because BioMart errors may contain this word in their error messages, such&amp;nbsp;as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&amp;quot;Query ERROR: caught BioMart::Exception::Database: Could not connect to mysql database ...&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Of course, other strings can be used. In retrospect, &amp;#8220;&lt;span class="caps"&gt;ERROR&lt;/span&gt;&amp;#8221; would have been an even better option than &amp;#8220;Query&amp;#8221;. If some error were detected, the loop would print the &lt;span class="caps"&gt;GO&lt;/span&gt; id that failed the query (due to a network error, for example), so I would have to retry the&amp;nbsp;query.&lt;/p&gt;
&lt;h2&gt;Annotate original data&amp;nbsp;frame&lt;/h2&gt;
&lt;p&gt;No errors were found, so I finally could annotate my original data frame by joining them by the &lt;code&gt;"go_ids"&lt;/code&gt; column. I even created a new column (&lt;code&gt;"gene_string"&lt;/code&gt;) to convert the Python lists into a nicely formatted comma-delimited string of gene&amp;nbsp;names:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df_annotated&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go_ids&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;go_genes&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go_ids&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="n"&gt;df_annotated&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gene_string&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;, &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;join&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;element&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;element&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;df_annotated&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gene_name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I then saved the result into a new tab (named &lt;code&gt;go annotated&lt;/code&gt;) inside my &lt;code&gt;go_demo.xlsx&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ExcelWriter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;go_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;a&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;df_annotated&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_excel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;writer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sheet_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;go annotated&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post,&amp;nbsp;I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introduced a function to create customized ready-to-use query strings via &lt;span class="caps"&gt;REST&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;&amp;nbsp;access;&lt;/li&gt;
&lt;li&gt;Provided a file with the descriptors of data deposited in BioMart (&lt;code&gt;list_ensembl.xlsx&lt;/code&gt;);&lt;/li&gt;
&lt;li&gt;Demonstrated how to use Python to invoke &lt;code&gt;GNU Wget&lt;/code&gt; to download the the queries&amp;#8217;&amp;nbsp;results;&lt;/li&gt;
&lt;li&gt;Demonstrated how to aggregate and manipulate the queries results using &lt;code&gt;pandas&lt;/code&gt;, &lt;code&gt;dask&lt;/code&gt; and &lt;code&gt;defaultdict&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Feel free to use the &lt;code&gt;query_biomart()&lt;/code&gt; function to data mine BioMart as you&amp;nbsp;wish!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Appendix&lt;/h2&gt;
&lt;p&gt;I prepared a spreadsheet named &lt;code&gt;lists_ensembl.xlsx&lt;/code&gt; also stored into the &lt;code&gt;data&lt;/code&gt; folder. Ensembl has a lot of datasets, filters and attributes, so examine the other rows if you are interested. I produced the spreadsheet with the help of &lt;a href="https://bioconductor.org/packages/release/bioc/vignettes/biomaRt/inst/doc/biomaRt.html"&gt;&lt;code&gt;biomaRt&lt;/code&gt; R package&lt;/a&gt;. If you prefer R over Python, be sure to try querying BioMart with it. The &lt;code&gt;lists_ensembl.R&lt;/code&gt; file contains the R code that generated the&amp;nbsp;spreadsheet.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://geneontology.org/docs/go-enrichment-analysis/"&gt;&lt;span class="caps"&gt;GO&lt;/span&gt; enrichment&amp;nbsp;analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://m.ensembl.org/biomart/martview"&gt;Ensembl&amp;nbsp;BioMart&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://m.ensembl.org/info/data/biomart/biomart_restful.html"&gt;BioMart RESTful&amp;nbsp;access&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.gnu.org/software/wget/"&gt;Wget - &lt;span class="caps"&gt;GNU&lt;/span&gt; Project - Free Software&amp;nbsp;Foundation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Representational_state_transfer"&gt;Representational state transfer -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.rdocumentation.org/packages/limma/versions/3.28.14/topics/goana"&gt;goana function | R&amp;nbsp;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-2.html"&gt;Working with Cancer Genomics Cloud datasets in a PostgreSQL database (Part&amp;nbsp;2)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/pathlib.html"&gt;pathlib â Object-oriented filesystem paths &amp;#8212; Python 3.9.1&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://openpyxl.readthedocs.io/en/stable/"&gt;openpyxl - A Python library to read/write Excel 2010 xlsx/xlsm files &amp;mdash; openpyxl 3.0.6&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bioconductor.org/packages/release/bioc/vignettes/biomaRt/inst/doc/biomaRt.html"&gt;The biomaRt users&amp;nbsp;guide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.dask.org/en/latest/delayed.html"&gt;Dask  documentation -&amp;nbsp;Delayed&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/collections.html#collections.defaultdict"&gt;collections â Container datatypes | Python 3.9.1&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Tue, 19 Jan 2021 10:20:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2021-01-19:/how-to-query-ensembl-biomart-with-python.html</guid><category>Python</category><category>Bioinformatics</category><category>Ensembl</category><category>BioMart</category><category>omics</category><category>data mining</category></item><item><title>Machine Learning with Python: Supervised Classification of TCGA Prostate Cancer Data (Part 1 - Making FeaturesÂ Datasets)</title><link>https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-1-making-features-datasets.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In a &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html"&gt;previous post&lt;/a&gt;, I showed how to retrieve &lt;a href="https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga"&gt;The Cancer Genome Atlas (&lt;span class="caps"&gt;TCGA&lt;/span&gt;)&lt;/a&gt; data from the &lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics Cloud (&lt;span class="caps"&gt;CGC&lt;/span&gt;) platform&lt;/a&gt;. I downloaded gene expression quantification data, created a relational database with PostgreSQL, and created a dataset uniting the raw quantification data for 675 differentially expressed genes &lt;a href="https://antoniocampos13.github.io/differential-expression-analysis-with-edger-in-r.html#differential-expression-analysis-with-edger-in-r"&gt;identified by edgeR&lt;/a&gt;, race, age at diagnosis and tumor size at&amp;nbsp;diagnosis.&lt;/p&gt;
&lt;p&gt;In this post, I will use Python to prepare features datasets to use them to produce a classification model using machine learning tools, especially the &lt;code&gt;scikit-learn&lt;/code&gt; module. Check its documentation &lt;a href="https://scikit-learn.org/stable/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The dataset and code presented here are available in my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2020_11_05_Supervised_Machine_Leaning_TCGA_Data"&gt;portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prepare&amp;nbsp;workspace&lt;/h2&gt;
&lt;p&gt;Since I will generate a statistical model, it is important to test it in never-seen before data, to assess its prediction validity. Thus, I will use a customized script, &lt;code&gt;make_features.py&lt;/code&gt;, to transform and split the dataset into separate train and test datasets. I will fit the model and then use it to predict the risk of prostate cancer of the subjects included in the test dataset and then compare with actual status (control: prostate cancer in remission/cases: prostate cancer progressing) to see how well it&amp;nbsp;predicted.&lt;/p&gt;
&lt;p&gt;Check the &lt;code&gt;make_features.py&lt;/code&gt;. Let&amp;#8217;s examine it. First, I import the necessary&amp;nbsp;modules:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;janitor&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;jn&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;preprocessing&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_selection&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;code&gt;pathlib&lt;/code&gt; is a module from the Python standard library. It helps file path management. &lt;code&gt;janitor&lt;/code&gt; is a module for data clean-up and manipulation. &lt;code&gt;pandas&lt;/code&gt; is also used for data manipulation and transformation, especially if it is contained on data frames. &lt;code&gt;sklearn&lt;/code&gt; is an alias for &lt;code&gt;scikit-learn&lt;/code&gt;. It is a powerhouse of several machine learning functions and utilities. Install all non-standard library modules using your Python package manager, usually &lt;code&gt;pip&lt;/code&gt; or through anaconda, if you are using a conda environment with&amp;nbsp;Python.&lt;/p&gt;
&lt;p&gt;Next, I set up some constants that I will use&amp;nbsp;later:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;RANDOM_SEED&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;
&lt;span class="n"&gt;TEST_SIZE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.30&lt;/span&gt;
&lt;span class="n"&gt;INDEX&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;676&lt;/span&gt;
&lt;span class="n"&gt;CORR_THRESHOLD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;RANDOM_SEED&lt;/code&gt;: An integer used to initialize the pseudo-random number generator. It helps generate reproducible outputs in different&amp;nbsp;sessions/computers;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TEST_SIZE&lt;/code&gt;: A decimal (float) number indicating the size of the test dataset. Currently, it is 30% of the samples in the complete&amp;nbsp;dataset;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;INDEX&lt;/code&gt;: An integer indicating a slicing index. I will explain it&amp;nbsp;later.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CORR_THRESHOLD&lt;/code&gt;: A float number indicating a threshold for eliminating correlated variables in the dataset; it helps overfitting by reducing the &lt;a href="https://en.wikipedia.org/wiki/Multicollinearity"&gt;multicollinearity issue&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next, I will indicate the path of the dataset, which i saved as a &lt;span class="caps"&gt;CSV&lt;/span&gt;&amp;nbsp;file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resolve&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt;
&lt;span class="n"&gt;CSV_FILE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;interim&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;prostate_cancer_dataset.csv&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;See below the structure of the current working&amp;nbsp;directory:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
âââ data
â   âââ interim
â   â   âââ prostate_cancer_dataset.csv
â   âââ processed
âââ models
âââ src
    âââ features
    â   âââ make_features.py
    âââ models
        âââ make_model.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Load data into&amp;nbsp;Pandas&lt;/h2&gt;
&lt;p&gt;Now, I create a &lt;code&gt;pandas.DataFrame&lt;/code&gt; using the &lt;code&gt;read_csv&lt;/code&gt; function, and assign it to &lt;code&gt;df&lt;/code&gt; object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;CSV_FILE&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Check the column names with the &lt;code&gt;columns&lt;/code&gt; method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columms&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;status&lt;/code&gt; column contains the sample labels: &lt;code&gt;0&lt;/code&gt; means &amp;#8220;control&amp;#8221; and &lt;code&gt;1&lt;/code&gt; means &amp;#8220;case&amp;#8221;. The other columns are the variables (or features) of each&amp;nbsp;sample.&lt;/p&gt;
&lt;h2&gt;Dummy-encode categorical&amp;nbsp;variables&lt;/h2&gt;
&lt;p&gt;I can now transform the data, making it suitable for machine learning modeling. Luckily, the &lt;span class="caps"&gt;TCGA&lt;/span&gt; dataset has no missing values and no gross errors are present. Otherwise, I would have to impute missing values and correct the errors. Now I will recode the categorical variables in the dataset. To check which variables are categorical, use the &lt;code&gt;pandas&lt;/code&gt; &lt;code&gt;dtypes&lt;/code&gt; method. The variables labeled with &lt;code&gt;object&lt;/code&gt; are usually categorical. If you see &lt;code&gt;object&lt;/code&gt; beside the name of a variable you know it is not categorical, then it is possible that there are missing data or some other&amp;nbsp;error.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtypes&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_dummies&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;drop_first&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Split&amp;nbsp;datasets&lt;/h2&gt;
&lt;p&gt;With the transformed dataset, let&amp;#8217;s go ahead and split the dataset into training and testing&amp;nbsp;datasets.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;jn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_features_targets&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;target_columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;status&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_selection&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;TEST_SIZE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stratify&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;RANDOM_SEED&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;get_features_targets&lt;/code&gt; from &lt;code&gt;janitor&lt;/code&gt; module will get the &lt;code&gt;df&lt;/code&gt; object and separate the sample labels from the variables (features). The features will then be assigned to object &lt;code&gt;X&lt;/code&gt;, which will be instanced as a &lt;code&gt;pandas.DataFrame&lt;/code&gt; and the &lt;code&gt;status&lt;/code&gt; column (see the &lt;code&gt;target_columns&lt;/code&gt; argument) will be put in the &lt;code&gt;y&lt;/code&gt; object, which will be instanced as a &lt;code&gt;pandas.Series&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The second function, &lt;code&gt;model_selection.train_test_split&lt;/code&gt; from &lt;code&gt;sklearn&lt;/code&gt; will split the &lt;code&gt;X&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; objects into its training and testing counterparts &amp;#8212; therefore, four objects: &lt;code&gt;X_train&lt;/code&gt;, &lt;code&gt;X_test&lt;/code&gt;, &lt;code&gt;y_train&lt;/code&gt; and &lt;code&gt;y_test&lt;/code&gt;. Check the &lt;code&gt;TEST_SIZE&lt;/code&gt; and &lt;code&gt;RANDOM_SEED&lt;/code&gt; constants I set up in the beginning of the script being used here. They indicate that 30% of the dataset must be included as a test dataset (therefore 70% in the training dataset) and setting a integer into the &lt;code&gt;random_state&lt;/code&gt; argument ensures that the splitting outputs can be&amp;nbsp;reproduced.&lt;/p&gt;
&lt;p&gt;Also note the &lt;code&gt;stratify&lt;/code&gt; argument. It ensures that the same proportion of cases and controls are drawn for training and testing datasets. For this, I indicate the object containing the labels, which is &lt;code&gt;y&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The commands below print the number of cases and controls for each&amp;nbsp;dataset:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Training Set has &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; Positive Labels (cases) and &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; Negative Labels (controls)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Test Set has &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; Positive Labels (cases) and &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="nb"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; Negative Labels (controls)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The&amp;nbsp;output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Training Set has 38 Positive Labels (cases) and 127 Negative Labels (controls)
Test Set has 16 Positive Labels (cases) and 55 Negative Labels (controls)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Normalize&amp;nbsp;data&lt;/h2&gt;
&lt;p&gt;Now I will standardize (scale or &lt;a href="https://en.wikipedia.org/wiki/Standard_score"&gt;Z-score normalize&lt;/a&gt;) the numerical columns. For each numerical feature, I will calculate its mean and standard deviation. Then, for each observed value of the variable, I will subtract the mean and divide by the standard&amp;nbsp;deviation.&lt;/p&gt;
&lt;p&gt;After the dummy coding of the categorical variables, they were transposed to the end of the data frame. I manually checked the column names and identified the column index. I then sliced the list of column names and stored in the &lt;code&gt;num_cols&lt;/code&gt; object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;num_cols&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)[:&lt;/span&gt;&lt;span class="n"&gt;INDEX&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That&amp;#8217;s why I conveniently saved the index number into the &lt;code&gt;INDEX&lt;/code&gt; variable at the beginning of the script. It helps code reusability with different data. It is one of the advantages of avoiding the so called &lt;a href="https://www.pluralsight.com/tech-blog/avoiding-magic-numbers/"&gt;&amp;#8220;magic numbers&amp;#8221;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With the numeric columns identified, I then used the &lt;code&gt;StandardScaler()&lt;/code&gt; function from &lt;code&gt;sklearn&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;preprocessing&lt;/code&gt; module:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;sca&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;preprocessing&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StandardScaler&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;num_cols&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;num_cols&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;num_cols&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sca&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;transform&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;num_cols&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I used the function &lt;code&gt;fit_transform()&lt;/code&gt; function in the &lt;code&gt;X_train&lt;/code&gt; dataset, passing the &lt;code&gt;num_cols&lt;/code&gt; list as the indication of which columns need to be normalized (using the brackets &lt;code&gt;[]&lt;/code&gt; notation) and saving the resulting transformation with the same names into the &lt;code&gt;X_train&lt;/code&gt; object. Thus, for all effects and purposes, I am replacing the old columns with the normalized&amp;nbsp;columns.&lt;/p&gt;
&lt;p&gt;Note that for &lt;code&gt;X_test&lt;/code&gt; dataset I used a different formula, &lt;code&gt;transform()&lt;/code&gt;. This is because I am using just the coefficients fitted by &lt;code&gt;fit_transform()&lt;/code&gt; in the train dataset to generate the normalization in the test dataset. This way I am sure that the scaling is not &amp;#8220;contaminated&amp;#8221; by the test data, that is supposed to not seem before the classification model&amp;nbsp;fitting.&lt;/p&gt;
&lt;h2&gt;Remove correlated&amp;nbsp;features&lt;/h2&gt;
&lt;p&gt;Now I will filter out correlated features. Please note that I would not normally do this with genomic data, but since here is just an exercise, I will show how to do it so you can apply to your projects when necessary. Check below the code (hat tip to &lt;a href="https://towardsdatascience.com/feature-selection-in-python-recursive-feature-elimination-19f1c39b8d15"&gt;Dario RadeÄiÄ&lt;/a&gt; for the&amp;nbsp;snippet):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;correlated_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;set&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;correlation_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;corr&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correlation_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;abs&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correlation_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;CORR_THRESHOLD&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;colname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;correlation_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="n"&gt;correlated_features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;colname&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The commands above will calculate the pairwise correlation between all features in the dataset, creating a &lt;code&gt;correlation_matrix&lt;/code&gt;. If the correlation between two features is above &lt;code&gt;CORR_THRESHOLD&lt;/code&gt; (currently 0.80), the loop will store the name of one of them into the &lt;code&gt;correlated_features&lt;/code&gt; set, ensuring that no names are repeated. If you want to know how may pairs of correlated features were present in the dataset, run the command below to print to the&amp;nbsp;console:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Number of correlated feature pairs: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correlated_features&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I can convert the set as list and pass it to &lt;code&gt;pandas&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt; &lt;code&gt;drop()&lt;/code&gt; method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correlated_features&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;correlated_features&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I dropped the columns and saved the data frames with the same name for convenience. Note the &lt;code&gt;axis=1&lt;/code&gt; argument, it tells &lt;code&gt;drop()&lt;/code&gt; to look the &lt;em&gt;columns&lt;/em&gt; for the list elements and then remove&amp;nbsp;them.&lt;/p&gt;
&lt;h2&gt;Serializing datasets for future&amp;nbsp;use&lt;/h2&gt;
&lt;p&gt;Finally, the train and test datasets are ready for use! To save them into disk for later use, I will use &lt;code&gt;pandas&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt; &lt;code&gt;to_pickle&lt;/code&gt; method. A &amp;#8220;pickled&amp;#8221; Python object is &lt;em&gt;serialized&lt;/em&gt;: it was converted into a series of bytes (a byte stream). This byte stream therefore can be &amp;#8220;unpickled&amp;#8221; to restore the object exactly as it was when was &amp;#8220;pickled&amp;#8221;. This is useful to backup data, share with others, and so on. Using &lt;code&gt;pathlib.Path&lt;/code&gt; notation, I saved the objects into the &amp;#8220;data/processed/&amp;#8221;&amp;nbsp;folder:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;processed&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X_train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;processed&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X_test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;processed&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y_train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;processed&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y_test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note: pickled objects are Python-specific only &amp;#8212; non-Python programs may not be able to reconstruct pickled Python objects. &lt;span class="caps"&gt;WARNING&lt;/span&gt;: never, &lt;span class="caps"&gt;NEVER&lt;/span&gt;, unpickle data you do not trust. As it says in the Python documentation: &lt;a href="https://docs.python.org/3/library/pickle.html"&gt;&amp;#8220;It is possible to construct malicious pickle data which will execute arbitrary code during unpickling&amp;#8221;&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Now, my folders are like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
âââ data
â   âââ interim
â   â   âââ prostate_cancer_dataset.csv
â   âââ processed
â       âââ X_train
â       âââ X_test
â       âââ y_train
â       âââ y_test
âââ models
âââ src
    âââ features
    â   âââ make_features.py
    âââ models
        âââ make_model.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I showed how&amp;nbsp;to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Import a &lt;span class="caps"&gt;CSV&lt;/span&gt; dataset into &lt;code&gt;pandas&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Dummy-encode categorical&amp;nbsp;data;&lt;/li&gt;
&lt;li&gt;Split the dataset into train/test&amp;nbsp;datasets;&lt;/li&gt;
&lt;li&gt;Normalize data&amp;nbsp;(Z-scores);&lt;/li&gt;
&lt;li&gt;Serialize (pickle) the datasets for future&amp;nbsp;use.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Go to the &lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-2-making-a-model.html"&gt;Part 2&lt;/a&gt;, where I show how to use the datasets to generate a classification model for predicting risk of prostate cancer disease progression with the &lt;code&gt;make_model.py&lt;/code&gt; script.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga"&gt;The Cancer Genome Atlas&amp;nbsp;Program&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics&amp;nbsp;Cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://scikit-learn.org/stable/"&gt;scikit-learn: machine learning in Python - scikit-learn 0.23.2&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Multicollinearity"&gt;Multicollinearity -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Standard_score"&gt;Standard score -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.pluralsight.com/tech-blog/avoiding-magic-numbers/"&gt;Avoiding Magic&amp;nbsp;Numbers&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://towardsdatascience.com/feature-selection-in-python-recursive-feature-elimination-19f1c39b8d15"&gt;Feature Selection in Python â Recursive Feature&amp;nbsp;Elimination&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/pickle.html"&gt;pickle â Python object serialization | Python 3.9.0&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Thu, 05 Nov 2020 14:50:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2020-11-05:/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-1-making-features-datasets.html</guid><category>Python</category><category>Bioinformatics</category><category>gene expression</category><category>machine learning</category><category>supervised classification</category></item><item><title>Machine Learning with Python: Supervised Classification of TCGA Prostate Cancer Data (Part 2 - Making aÂ Model)</title><link>https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-2-making-a-model.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;In a &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html"&gt;previous post&lt;/a&gt;, I showed how to retrieve &lt;a href="https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga"&gt;The Cancer Genome Atlas (&lt;span class="caps"&gt;TCGA&lt;/span&gt;)&lt;/a&gt; data from the &lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics Cloud (&lt;span class="caps"&gt;CGC&lt;/span&gt;) platform&lt;/a&gt;. I downloaded gene expression quantification data, created a relational database with PostgreSQL, and created a dataset uniting the raw quantification data for 675 differentially expressed genes &lt;a href="https://antoniocampos13.github.io/differential-expression-analysis-with-edger-in-r.html#differential-expression-analysis-with-edger-in-r"&gt;identified by edgeR&lt;/a&gt;, race, age at diagnosis and tumor size at&amp;nbsp;diagnosis.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In &lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-1-making-features-datasets.html"&gt;Part 1&lt;/a&gt;, I used Python to prepare features datasets to use them to produce a classification model using machine learning tools, especially the &lt;code&gt;scikit-learn&lt;/code&gt; module. Check its documentation &lt;a href="https://scikit-learn.org/stable/"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here in Part 2, I develop an illustrative model. If it were a serious model, its objective would be to predict if a person is in risk of developing prostate cancer based on personal characteristics and the expression of differentially expressed&amp;nbsp;genes.&lt;/p&gt;
&lt;p&gt;The dataset and code presented here are available in my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2020_11_05_Supervised_Machine_Leaning_TCGA_Data"&gt;portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Prepare&amp;nbsp;workspace&lt;/h2&gt;
&lt;p&gt;As in Part 1, I will import the modules needed to make the model (check the &lt;code&gt;make_model.py&lt;/code&gt; script located on &lt;code&gt;src/models&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pickle&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pathlib&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;xgboost&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f1_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;roc_auc_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;roc_curve&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;precision_recall_curve&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.dummy&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DummyClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.linear_model&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;StratifiedKFold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;KFold&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cross_val_score&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.naive_bayes&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;GaussianNB&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.neighbors&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KNeighborsClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.svm&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SVC&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Install all non-standard library modules using your Python package manager, usually &lt;code&gt;pip&lt;/code&gt; or through anaconda, if you are using a conda environment with&amp;nbsp;Python.&lt;/p&gt;
&lt;p&gt;Again, I will set up some constants to use&amp;nbsp;later:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;RANDOM_SEED&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;
&lt;span class="n"&gt;SPLITS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Set up the file&amp;nbsp;paths:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resolve&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parent&lt;/span&gt;
&lt;span class="n"&gt;features_folder&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;data&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;processed&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Importing&amp;nbsp;features&lt;/h2&gt;
&lt;p&gt;If you still have the features datasets loaded on memory, the commands below are not necessary. They serve to de-serialize the pickled datasets I saved in Part 1 using &lt;code&gt;pandas&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt; &lt;code&gt;read_pickle()&lt;/code&gt; method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;X_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;X_train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;X_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;X_test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_train&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;y_train&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_pickle&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;features_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;y_test&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Select classification&amp;nbsp;model&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;scikit-learn&lt;/code&gt; module contains code for several classification models. If you are in doubt, you can select from a list of models using a scoring metric, and the choose the best-performing model. See below a loop command that do just that (hat tip to &lt;a href="https://hairysun.com/"&gt;Matt Harrison&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;models&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
&lt;span class="n"&gt;DummyClassifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;KNeighborsClassifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;GaussianNB&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;SVC&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="n"&gt;xgboost&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;XGBClassifier&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;models&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="bp"&gt;cls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

    &lt;span class="n"&gt;kfold&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KFold&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_splits&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;SPLITS&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;random_state&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;RANDOM_SEED&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cross_val_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;cls&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scoring&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;f1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;kfold&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;22&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;  F1 Score: &amp;quot;&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; STD: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I will now explain what the loop does. First, I create a list named &lt;code&gt;models&lt;/code&gt; with the name of some &lt;code&gt;sklearn&lt;/code&gt; models. Note the way their names are written: they are the names of the corresponding &lt;code&gt;sklearn&lt;/code&gt; modules.&lt;/p&gt;
&lt;p&gt;I then loop this list creating a classifier &lt;code&gt;cls&lt;/code&gt; object by calling each model. Then, I create a K-Folds cross-validator. In other words, I randomly split the training dataset into &lt;code&gt;K&lt;/code&gt; datasets (folds). Each fold is then used once as a validation while the &lt;code&gt;K - 1&lt;/code&gt; remaining folds form the training set. The &lt;code&gt;n_splits&lt;/code&gt; argument indicates &lt;code&gt;K&lt;/code&gt;, which I set up using the &lt;code&gt;SPLITS&lt;/code&gt; constant (currently 10, so &lt;code&gt;K=10&lt;/code&gt; folds). Setting a integer (&lt;code&gt;RANDOM_SEED&lt;/code&gt; constant) into the &lt;code&gt;random_state&lt;/code&gt; argument ensures that the splitting outputs can be&amp;nbsp;reproduced.&lt;/p&gt;
&lt;p&gt;Next, I calculate the &lt;a href="https://en.wikipedia.org/wiki/F-score"&gt;F1 Score&lt;/a&gt; by comparing the predictions with their actual labels (&lt;code&gt;y_train&lt;/code&gt; series). As the text in the linked page states, &amp;#8220;it is calculated from the precision and recall of the test, where the precision is the number of correctly identified positive results divided by the number of all positive results, including those not identified correctly, and the recall is the number of correctly identified positive results divided by the number of all samples that should have been identified as positive&amp;#8221;. Recall is also named true positive rate (&lt;span class="caps"&gt;TPR&lt;/span&gt;) and sensitivity. The F1 Score is the harmonic mean of the precision and recall. This calculation is performed by &lt;code&gt;cross_val_score()&lt;/code&gt; method from &lt;code&gt;sklearn.model_selection&lt;/code&gt; module for each comparison among the folds. Then, the final result is an average of all measurements, which is stored at the &lt;code&gt;s&lt;/code&gt; object. Finally, the loop ends printing the mean F1 Score and its standard deviation (&lt;span class="caps"&gt;STD&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Why I chose the F1 Score? Because there is class imbalance in the data: there is much more control than cases in the dataset. Thus, scores such as accuracy and precision may be &lt;a href="https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28"&gt;misleading when considered alone&lt;/a&gt;. The &lt;a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity"&gt;Wikipedia article about sensitivity and specificity&lt;/a&gt; is a great summary of these&amp;nbsp;concepts.&lt;/p&gt;
&lt;p&gt;After a while, the output will be printed to your console. It will be something like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;DummyClassifier         F1 Score: 0.248 STD: 0.16
LogisticRegression      F1 Score: 0.578 STD: 0.28
DecisionTreeClassifier  F1 Score: 0.291 STD: 0.19
KNeighborsClassifier    F1 Score: 0.069 STD: 0.14
GaussianNB              F1 Score: 0.399 STD: 0.27
SVC                     F1 Score: 0.165 STD: 0.31
RandomForestClassifier  F1 Score: 0.337 STD: 0.26
XGBClassifier           F1 Score: 0.370 STD: 0.34
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The dataset folds will be identical, but you may have different values in some of these models due to stochasticity during calculation. Nevertheless, the values should be close enough. It seems that &lt;code&gt;LogisticRegression&lt;/code&gt; had the best performance among the models, with a rounded up F1 Score = 0.58. So I will continue with this model and try to improve the F1 Score by optimizing (tuning) the&amp;nbsp;model.&lt;/p&gt;
&lt;h2&gt;Model&amp;nbsp;optimization&lt;/h2&gt;
&lt;p&gt;I will now setup the model and produce a grid of &lt;strong&gt;hyperparameters&lt;/strong&gt;. A hyperparameter is a parameter of the model whose value affects the learning/classification process. A grid is therefore a collection of some of those hyperparameters that I will give to the model so it can choose the best candidates. I now set up the&amp;nbsp;model:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;estimator&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And this is the&amp;nbsp;grid:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;grid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;penalty&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;l1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;l2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;elasticnet&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="s2"&gt;&amp;quot;dual&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:[&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="s2"&gt;&amp;quot;C&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;logspace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that each model has different hyperparameters; those above are logistic regression-exclusive. Check the &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression"&gt;sklearn.linear_model.LogisticRegression module documentation&lt;/a&gt; to know more about these&amp;nbsp;hyperparameters.&lt;/p&gt;
&lt;p&gt;Now, I pass the model (assigned to &lt;code&gt;estimator&lt;/code&gt; object) to the &lt;code&gt;GridSearchCV&lt;/code&gt; function so that it returns the optimal parameters during fitting to the train data with 10-fold&amp;nbsp;cross-validation:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;logreg_cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;GridSearchCV&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;cv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;SPLITS&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;logreg_cv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let&amp;#8217;s see a summary of the&amp;nbsp;candidates:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tuned hyperparameters :(best parameters) &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;logreg_cv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_params_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output will&amp;nbsp;be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;tuned hyperparameters :(best parameters)  {&amp;#39;C&amp;#39;: 100.0, &amp;#39;dual&amp;#39;: False, &amp;#39;penalty&amp;#39;: &amp;#39;l2&amp;#39;}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I pass the best parameters as keywords to the optimized model, which I will assign to &lt;code&gt;logreg&lt;/code&gt; object, and then fit the training data once&amp;nbsp;again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;logreg&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;LogisticRegression&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;logreg_cv&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;best_params_&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;logreg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;With the fitted model, I can now generate predictions with the test&amp;nbsp;dataset:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logreg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;predicted_proba&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;logreg&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict_proba&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Evaluate model&amp;nbsp;performance&lt;/h2&gt;
&lt;p&gt;Let&amp;#8217;s compare the predict labels with actual classification using a confusion&amp;nbsp;matrix:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;conf_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Prediction: controls&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Prediction: cases&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Actual: controls&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Actual: cases&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf_matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output will&amp;nbsp;be:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#                  Actual: controls  Actual: cases
Prediction: controls                47             11
Prediction: cases                    8              5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now remember the labels summary I got in &lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-1-making-features-datasets.html"&gt;Part 1&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Training Set has 38 Positive Labels (cases) and 127 Negative Labels (controls)
Test Set has 16 Positive Labels (cases) and 55 Negative Labels (controls)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Thus, the model erroneously classified 11 (of 16) cases as controls (false negatives) and correctly classified 5 (of 16) cases as cases (true positives). If this model were to be used in a real world scenario, this would be a problem, because the model would miss people at risk of disease&amp;nbsp;progression.&lt;/p&gt;
&lt;p&gt;Imagine that detecting prostate cancer disease progression risk will trigger further analysis (gather second opinion, ask the patients more examinations etc.) whereas if you don&amp;#8217;t detect this risk, the patient would be sent home with the disease actively progressing. In this situation, therefore, I would prefer false positives (type I error) over false negatives (type &lt;span class="caps"&gt;II&lt;/span&gt;&amp;nbsp;error).&lt;/p&gt;
&lt;p&gt;Please note that there is not a single response for every classification problem; the researcher must evaluate the consequences of the errors and make a decision, since reducing one type of error means increasing the other type of error. In every situation, one type of error is more preferable than the other one &amp;#8212; it is always a&amp;nbsp;trade-off.&lt;/p&gt;
&lt;p&gt;But why the model incorrectly classified some patients? One of the factors is because logistic regression models calculate probabilities to make these predictions. If the predicted probability from a sample is &amp;gt;= 0.50, it labels the sample as case, otherwise, it labels as a control. I can change this probability cutoff to try to reduce the number of false negatives, especially since there is class imbalance in the&amp;nbsp;dataset.&lt;/p&gt;
&lt;p&gt;Using the data in this confusion matrix, I can calculate and print some&amp;nbsp;metrics:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Accuracy: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Precision: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Recall: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;F1 Score: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;f1_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The&amp;nbsp;output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Accuracy: 0.732
Precision: 0.385
Recall: 0.312
F1 Score: 0.345
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I can also print a receiver operating characteristic (&lt;span class="caps"&gt;ROC&lt;/span&gt;) curve. As stated in the &lt;a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic"&gt;Wikipedia article&lt;/a&gt;, it &amp;#8220;illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied&amp;#8221;. The area under the &lt;span class="caps"&gt;ROC&lt;/span&gt; curve (&lt;span class="caps"&gt;AUC&lt;/span&gt;) summarizes the predictive power of the&amp;nbsp;model.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;false_pos_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;true_pos_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;proba&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;roc_curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicted_proba&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# plot random curve&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;false_pos_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;true_pos_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;marker&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;AUC = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;roc_auc_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicted_proba&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.2f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ROC Curve&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;True Positive Rate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;False Positive Rate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lower right&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The&amp;nbsp;output:&lt;/p&gt;
&lt;p&gt;&lt;img alt="ROC curve from logistic regression model classifying TCGA prostate cancer dataset" src="https://antoniocampos13.github.io/images/logreg_roc.png"&gt;&lt;/p&gt;
&lt;p&gt;The &lt;span class="caps"&gt;AUC&lt;/span&gt; is equal to the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative one. It ranges from 0 to 1 (perfect classifier). A random, thus useless, classifier, has a &lt;span class="caps"&gt;AUC&lt;/span&gt; = 0.5. Since the &lt;span class="caps"&gt;AUC&lt;/span&gt; of my model is about 0.65, means that it has some predictive power, although not perfect. I cannot improve the &lt;span class="caps"&gt;AUC&lt;/span&gt;, but I can change the classification probability threshold (as I mentioned above) to try to better utilize the potential of the&amp;nbsp;model.&lt;/p&gt;
&lt;h2&gt;Change classification&amp;nbsp;threshold&lt;/h2&gt;
&lt;p&gt;Looking at the metrics I printed above, I can see the model has fairly low sensitivity (recall) as well as low F1 score. Since there is class imbalance, I will change the classification threshold based on F1&amp;nbsp;score.&lt;/p&gt;
&lt;p&gt;I now calculate the range of F1 scores with several classification&amp;nbsp;thresholds:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;precision&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;recall&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;thresholds&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;precision_recall_curve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predicted_proba&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="n"&gt;f1_scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;recall&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;precision&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;recall&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;precision&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;precision_recall_curve()&lt;/code&gt; function will select a classification threshold (&lt;code&gt;thresholds&lt;/code&gt; object), label the samples according to their predicted probabilities given by the model (&lt;code&gt;predicted_proba[:, -1]&lt;/code&gt;) and compare with the actual labels (&lt;code&gt;y_test&lt;/code&gt;), calculating the precision and recall (&lt;code&gt;precision&lt;/code&gt; and &lt;code&gt;recall&lt;/code&gt; objects).&lt;/p&gt;
&lt;p&gt;All three objects are &lt;code&gt;NumPy&lt;/code&gt; arrays. Thus, I can obtain the probability cutoff value associated with maximum F1 score, and create a list of prediction labels and assign to &lt;code&gt;f1_predictions&lt;/code&gt; object using a list comprehension with a &lt;code&gt;if else&lt;/code&gt; statement:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;optimal_proba_cutoff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;thresholds&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmax&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f1_scores&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="n"&gt;f1_predictions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="n"&gt;optimal_proba_cutoff&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;predicted_proba&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let&amp;#8217;s examine the confusion matrix using this new&amp;nbsp;cutoff:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;conf_matrix_th&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f1_predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;T&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Prediction: controls&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Prediction: cases&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Actual: controls&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Actual: cases&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;conf_matrix_th&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The&amp;nbsp;output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;#                  Actual: controls  Actual: cases
Prediction: controls                34              4
Prediction: cases                   21             12
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I can see that I reduced the number of false negatives at the cost of having more false positives, as discussed above. Now the model erroneously classified just 4 cases as controls, compared to 11 before thresholding. Let&amp;#8217;s calculate the model&amp;#8217;s metrics&amp;nbsp;again:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Accuracy Before: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; --&amp;gt; Now: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f1_predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Precision Before: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; --&amp;gt; Now: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;precision_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f1_predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Recall Before: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; --&amp;gt; Now: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;recall_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f1_predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;F1 Score Before: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;f1_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; --&amp;gt; Now: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;f1_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;f1_predictions&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The&amp;nbsp;output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Accuracy Before: 0.732 --&amp;gt; Now: 0.648
Precision Before: 0.385 --&amp;gt; Now: 0.364
Recall Before: 0.312 --&amp;gt; Now: 0.750
F1 Score Before: 0.345 --&amp;gt; Now: 0.490
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I see that the sensitivity (recall) improved greatly, with a consequential F1 score improvement. With the current data, this is the maximum improvement I can achieve. If this model would to be used in a real world scenario, I would have to gather more information to try to further improve the classification power of the model, always assessing the optimal probability cutoff to account for class&amp;nbsp;imbalance.&lt;/p&gt;
&lt;h2&gt;Save (Serialize)&amp;nbsp;model&lt;/h2&gt;
&lt;p&gt;I can save the model to disk as a pickled object to use in the future or share with someone. Remember that the same modules must be installed and loaded in the system that will receive the pickled model so it can be unpickled and work correctly. See below the commands to save the model and the chosen probability cutoff for&amp;nbsp;classification:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;pickle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;logreg&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;models&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;logreg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;wb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;pickle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dump&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimal_proba_cutoff&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;project_folder&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;models&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;logreg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;wb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note: pickled objects are Python-specific only &amp;#8212; non-Python programs may not be able to reconstruct pickled Python objects. &lt;span class="caps"&gt;WARNING&lt;/span&gt;: never, &lt;span class="caps"&gt;NEVER&lt;/span&gt;, unpickle data you do not trust. As it says in the Python documentation: &lt;a href="https://docs.python.org/3/library/pickle.html"&gt;&amp;#8220;It is possible to construct malicious pickle data which will execute arbitrary code during unpickling&amp;#8221;&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Below is a schematics of my working directory. Check it on &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Python/2020_11_05_Supervised_Machine_Leaning_TCGA_Data/data/interim"&gt;my porfolio&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
âââ data
â   âââ interim
â   â   âââ prostate_cancer_dataset.csv
â   âââ processed
â       âââ X_train
â       âââ X_test
â       âââ y_train
â       âââ y_test
âââ models
â   âââ logreg
â   âââ optimal_proba_cutoff
âââ src
    âââ features
    â   âââ make_features.py
    âââ models
        âââ make_model.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this part I demonstrated how&amp;nbsp;to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Import pickled datasets to be used as training and test data for the&amp;nbsp;model;&lt;/li&gt;
&lt;li&gt;Check metrics among selected models to support model&amp;nbsp;choice;&lt;/li&gt;
&lt;li&gt;Optimize (tune) model hyperparameters via grid&amp;nbsp;search;&lt;/li&gt;
&lt;li&gt;Evaluate model performance via confusion matrix, metrics and &lt;span class="caps"&gt;ROC&lt;/span&gt; &lt;span class="caps"&gt;AUC&lt;/span&gt;&amp;nbsp;plotting;&lt;/li&gt;
&lt;li&gt;Select a classification&amp;nbsp;cutoff;&lt;/li&gt;
&lt;li&gt;Save the model to disk for backup and future&amp;nbsp;use.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Go back to &lt;a href="https://antoniocampos13.github.io/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-1-making-features-datasets.html"&gt;Part 1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga"&gt;The Cancer Genome Atlas&amp;nbsp;Program&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics&amp;nbsp;Cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://scikit-learn.org/stable/"&gt;scikit-learn: machine learning in Python - scikit-learn 0.23.2&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://hairysun.com/"&gt;Matt Harrison&amp;#8217;s&amp;nbsp;Blog&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/F-score"&gt;F-score -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28"&gt;Handling Imbalanced Datasets in Machine&amp;nbsp;Learning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity"&gt;Sensitivity and specificity -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression"&gt;sklearn.linear_model.LogisticRegression - scikit-learn 0.23.2&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic"&gt;Receiver operating characteristic -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.python.org/3/library/pickle.html"&gt;pickle â Python object serialization | Python 3.9.0&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Thu, 05 Nov 2020 14:50:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2020-11-05:/machine-learning-with-python-supervised-classification-of-tcga-prostate-cancer-data-part-2-making-a-model.html</guid><category>Python</category><category>Bioinformatics</category><category>gene expression</category><category>machine learning</category><category>supervised classification</category></item><item><title>Differential Expression Analysis with edgeR inÂ R</title><link>https://antoniocampos13.github.io/differential-expression-analysis-with-edger-in-r.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In my &lt;a href="https://antoniocampos13.github.io/data-manipulation-with-r.html#data-manipulation-with-r"&gt;previous post&lt;/a&gt; I demonstrated how to organize the &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html"&gt;&lt;span class="caps"&gt;CGC&lt;/span&gt; prostate cancer data&lt;/a&gt; to a format suited to differential expression analysis (&lt;span class="caps"&gt;DEA&lt;/span&gt;).&lt;/p&gt;
&lt;p&gt;Nowadays, &lt;span class="caps"&gt;DEA&lt;/span&gt; usually arises from high-throughput sequencing of a collection (library) of &lt;span class="caps"&gt;RNA&lt;/span&gt; molecules expressed by single cells or tissue given their conditions upon collection and &lt;span class="caps"&gt;RNA&lt;/span&gt;&amp;nbsp;extraction.&lt;/p&gt;
&lt;p&gt;In terms of statistical analysis, &lt;span class="caps"&gt;DEA&lt;/span&gt; &lt;a href="https://www.ebi.ac.uk/training/online/course/functional-genomics-ii-common-technologies-and-data-analysis-methods/differential-gene"&gt;&amp;#8220;means taking the normalized read count data and performing statistical analysis to discover quantitative changes in expression levels between experimental groups&amp;#8221;&lt;/a&gt;. What are experimental groups? Consider for example, diseased versus healthy cells, treated cells versus non-treated cells (when someone is testing new drugs for example), and so&amp;nbsp;on.&lt;/p&gt;
&lt;h2&gt;The edgeR&amp;nbsp;package&lt;/h2&gt;
&lt;p&gt;There are some statistical packages in R that deal with &lt;span class="caps"&gt;DEA&lt;/span&gt;, such as &lt;code&gt;edgeR&lt;/code&gt;, &lt;code&gt;DESeq2&lt;/code&gt; and &lt;code&gt;limma&lt;/code&gt;. Here I will demonstrate a custom script to perform &lt;span class="caps"&gt;DEA&lt;/span&gt; with &lt;a href="https://bioconductor.org/packages/release/bioc/html/edgeR.html"&gt;&lt;code&gt;edgeR&lt;/code&gt;&lt;/a&gt;. The demonstration here is on Windows 10, but the same steps can be performed on Unix&amp;nbsp;systems.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;edgeR&lt;/code&gt; performs &lt;span class="caps"&gt;DEA&lt;/span&gt; for pre-defined genomic features, which can be genes, transcripts or exons, for example. In the present demonstration, we will quantify transcripts. Remember that genes can produce several transcripts through alternative splicing. The &lt;code&gt;edgeR&lt;/code&gt; statistical model is based on negative binomial distribution. Prior to statistical analysis, &lt;code&gt;edgeR&lt;/code&gt; normalizes gene/transcript expression counts via the Trimmed Mean of M-values (&lt;span class="caps"&gt;TMM&lt;/span&gt;) method. See &lt;a href="https://www.biostars.org/p/284775/#284893"&gt;Dr. Kevin Blighe&amp;#8217;s comment in a Biostars forum topic&lt;/a&gt; for a brief discussion of &lt;code&gt;edgeR&lt;/code&gt; and other &lt;span class="caps"&gt;DEA&lt;/span&gt;&amp;nbsp;packages.&lt;/p&gt;
&lt;p&gt;Without further ado, I will show how to set up a R session to run a &lt;span class="caps"&gt;DEA&lt;/span&gt; with &lt;code&gt;edgeR&lt;/code&gt;, and how to interpret results. As usual, the code presented here is deposited on my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/R/2020_10_22_DEA_with_edgeR"&gt;portfolio at GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Install and load&amp;nbsp;packages&lt;/h2&gt;
&lt;p&gt;First, I will install some new packages that I have not talked about. The first one is &lt;a href="https://www.rdocumentation.org/packages/openxlsx/versions/4.2.2"&gt;&lt;code&gt;openxlsx&lt;/code&gt;&lt;/a&gt;, which is a package used to read/write Microsoft Office Excel spreadsheets. I will use it to conveniently save the output of the &lt;span class="caps"&gt;DEA&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The second is &lt;a href="https://www.rdocumentation.org/packages/BiocManager/versions/1.30.10"&gt;&lt;code&gt;BiocManager&lt;/code&gt;&lt;/a&gt;. It is needed to install packages from &lt;a href="https://www.bioconductor.org/"&gt;Bioconductor project&lt;/a&gt;, which hosts Bioinformatics analysis packages that are not on the default R package&amp;nbsp;repository.&lt;/p&gt;
&lt;p&gt;The command below contains other packages I have used before, edit the comment if you already installed&amp;nbsp;them:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Run only once&lt;/span&gt;
&lt;span class="nf"&gt;install.packages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;here&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;tidyverse&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;openxlsx&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;BiocManager&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I install &lt;code&gt;edgeR&lt;/code&gt; and some more packages from Bioconductor. I will use them to annotate and convert the transcript/gene IDs to a gene symbol. Check their documentation: &lt;a href="https://www.bioconductor.org/packages/release/bioc/html/AnnotationDbi.html"&gt;&lt;code&gt;AnnotationDbi&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://www.rdocumentation.org/packages/annotate/versions/1.50.0"&gt;&lt;code&gt;annotate&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://bioconductor.org/packages/release/data/annotation/html/org.Hs.eg.db.html"&gt;&lt;code&gt;org.Hs.eg.db&lt;/code&gt;&lt;/a&gt;, &lt;a href="https://bioconductor.org/packages/release/data/annotation/html/EnsDb.Hsapiens.v79.html"&gt;&lt;code&gt;EnsDb.Hsapiens.v79&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://www.rdocumentation.org/packages/ensembldb/versions/1.4.7"&gt;&lt;code&gt;ensembldb&lt;/code&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Run only once&lt;/span&gt;
&lt;span class="n"&gt;BiocManager&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;install&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;edgeR&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;AnnotationDbi&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;annotate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;org.Hs.eg.db&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;EnsDb.Hsapiens.v79&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ensembldb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;::&lt;/code&gt; is used when we wish to invoke the mentioned package directly (&lt;code&gt;BiocManager&lt;/code&gt; in this case), without loading it into the&amp;nbsp;memory.&lt;/p&gt;
&lt;p&gt;Now, I will load just the &lt;code&gt;here&lt;/code&gt; package to handle file paths for now, the rest will be loaded into R&amp;nbsp;later.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;here&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I will use the &lt;code&gt;counts&lt;/code&gt; data frame I produced &lt;a href="https://antoniocampos13.github.io/data-manipulation-with-r.html"&gt;last time&lt;/a&gt;. Since I have saved it to my disk, I load it into the current R session. If you already have the &lt;code&gt;counts&lt;/code&gt; data frame loaded in the session from the previous demonstration, this step is not&amp;nbsp;necessary.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;counts.RData&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I load the custom &lt;code&gt;edgeR_setup()&lt;/code&gt; function I use to perform &lt;span class="caps"&gt;DEA&lt;/span&gt; with &lt;code&gt;edgeR&lt;/code&gt;. I wrote the function in a R script with the same name and saved it on my &lt;code&gt;src&lt;/code&gt; folder:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;source&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;src&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;edgeR_setup.R&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Careful to not confuse the &lt;code&gt;load()&lt;/code&gt; with &lt;code&gt;source()&lt;/code&gt; functions. The &lt;a href="https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/load"&gt;former&lt;/a&gt; is used with R objects (&lt;code&gt;*.RData&lt;/code&gt;) as input, whereas the &lt;a href="https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/source"&gt;latter&lt;/a&gt; takes a R script (&lt;code&gt;*.R&lt;/code&gt;) as input and parses the commands contained in the&amp;nbsp;script.&lt;/p&gt;
&lt;p&gt;Check the &lt;code&gt;edgeR_setup.R&lt;/code&gt; script. First, it loads the packages I installed&amp;nbsp;before:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tidyverse&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;openxlsx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Install trough BiocManager&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;edgeR&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;annotate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;org.Hs.eg.db&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;EnsDb.Hsapiens.v79&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ensembldb&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AnnotationDbi&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now, check the function&amp;nbsp;arguments:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;edger_setup&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;replicates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gene_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;NCBI&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ENSEMBL&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;SYMBOL&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;output_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;

    &lt;span class="c1"&gt;# ... the function goes here ...&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;name&lt;/code&gt;: A string. An identifier for the&amp;nbsp;experiment.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;counts&lt;/code&gt;: The data frame containing the transcript&amp;nbsp;counts.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;replicates&lt;/code&gt;: A Boolean indicating if the samples are biological replicates. Defaults to &lt;code&gt;TRUE&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;filter&lt;/code&gt;: A Boolean indicating if lowly expressed transcripts should be filter out. Defaults to &lt;code&gt;TRUE&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;gene_id&lt;/code&gt;: A string indicating how transcripts are identified in the data frame. There are three options:&lt;ul&gt;
&lt;li&gt;&lt;code&gt;NCBI&lt;/code&gt;: &lt;a href="https://www.ncbi.nlm.nih.gov/Class/MLACourse/Modules/Genes/sample_entrez_gene_record.html"&gt;Entrez Gene Record&amp;nbsp;ids&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ENSEMBL&lt;/code&gt;: &lt;a href="https://www.ebi.ac.uk/training-beta/online/courses/ensembl-browsing-genomes/navigating-ensembl/investigating-a-gene/"&gt;&lt;span class="caps"&gt;ENSEMBL&lt;/span&gt; ids (&lt;span class="caps"&gt;ENS&lt;/span&gt;#)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SYMBOL&lt;/code&gt;: &lt;a href="https://www.genenames.org/"&gt;Official &lt;span class="caps"&gt;HGNC&lt;/span&gt; gene&amp;nbsp;symbol&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;output_path&lt;/code&gt;: A path and filename string where the results will be saved in Excel spreadsheet format. Example: &lt;code&gt;"\some\path\results.xlsx"&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The use of &lt;code&gt;edgeR&lt;/code&gt; to analyze datasets with no biological replicates (&lt;code&gt;replicates = FALSE&lt;/code&gt;) is discouraged. However, I prepared a special dataset of housekeeping genes based on the work by &lt;a href="https://www.cell.com/trends/genetics/fulltext/S0168-9525(13)00089-9?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0168952513000899%3Fshowall%3Dtrue"&gt;Eisenberg and Levanon (2013)&lt;/a&gt;. I downloaded the &lt;code&gt;HK_genes.txt&lt;/code&gt; supplementary file &lt;a href="https://www.tau.ac.il/~elieis/HKG/"&gt;here&lt;/a&gt; and placed it in the &lt;code&gt;data&lt;/code&gt; folder at my current work directory. I also wrote an auxiliary script named &lt;code&gt;hk_genes.R&lt;/code&gt; and placed it in the &lt;code&gt;src&lt;/code&gt; folder. Check below a representation of my current work directory, where &lt;code&gt;main_dea_edgeR.R&lt;/code&gt; contains the commands of this&amp;nbsp;demonstration.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
âââ data
â   âââ counts.RData
â   âââ HK_genes.txt
âââ output
âââ src
â   âââ edgeR_setup.R
â   âââ hk_genes.R
âââ main_dea_edgeR.R
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For now, I will use default values and indicate that the transcripts in the data frame are identified by &lt;span class="caps"&gt;ENSEMBL&lt;/span&gt; ids. Before running the function, I assign the output path string to the &lt;code&gt;out_path&lt;/code&gt; object, which I include in the function call. Note that I gave the name &lt;code&gt;prostate_cancer&lt;/code&gt; to identify the experiment, and it will also be the name of the sheet in the&amp;nbsp;spreadsheet.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;out_path&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;output&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;prostate_cancer.xlsx&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;edger_setup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;prostate_cancer&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;replicates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;filter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;gene_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ENSEMBL&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;out_path&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The function will organize the data into groups based on the sample labels I applied previously (&amp;#8220;case&amp;#8221; and &amp;#8220;control&amp;#8221;), filter out genes with negligible expression and calculate the expression metrics, such as the logarithm of the fold-change (logFC) and counts per million transcripts (logCPM), as well as fit a statistical generalized linear model (&lt;span class="caps"&gt;GLM&lt;/span&gt;), calculating &lt;span class="caps"&gt;GLM&lt;/span&gt; coefficients (&amp;beta;) for each gene. The &lt;span class="caps"&gt;DEA&lt;/span&gt; then consists in perform a hypothesis test (quasi-likelihood F-test in this case), to test the null hypothesis that the coefficients are equal (or that &amp;beta;&lt;sub&gt;&lt;em&gt;control&lt;/em&gt;&lt;/sub&gt; - &amp;beta;&lt;sub&gt;&lt;em&gt;case&lt;/em&gt;&lt;/sub&gt; = 0). From the F-test statistics is then derived a p-value, which is adjusted by false discovery rate (&lt;span class="caps"&gt;FDR&lt;/span&gt;) to account for multiple&amp;nbsp;comparisons.&lt;/p&gt;
&lt;p&gt;After a while, the function will generate a spreadsheet with the &lt;span class="caps"&gt;DEA&lt;/span&gt; results. See below an excerpt of the spreadsheet (&lt;em&gt;with commas as decimal separators&lt;/em&gt;):&lt;/p&gt;
&lt;p&gt;&lt;img alt="edgeR differential expression analysis in a prostate cancer dataset" src="https://antoniocampos13.github.io/images/prostate_cancer_edger_result.PNG"&gt;&lt;/p&gt;
&lt;h2&gt;Interpretation of the&amp;nbsp;results&lt;/h2&gt;
&lt;p&gt;Note that there are seven&amp;nbsp;columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;symbol&lt;/code&gt; and &lt;code&gt;geneid&lt;/code&gt;: transcript&amp;nbsp;identifiers;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;logfc&lt;/code&gt;: the base 2 logarithm of the &lt;strong&gt;fold-change&lt;/strong&gt; (logFC), which is how much a quantity changes between two measurements &amp;#8212; it is a ratio of two quantities. A logFC = 1 means that the expression of a certain gene was double in one condition than the other, a logFC = 2 means four-times higher expression, and so on. A logFC = -1 means half of the expression, a logFC = -2 means a quarter, and so&amp;nbsp;on.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;logcpm&lt;/code&gt;: the logarithm of the counts per million&amp;nbsp;transcripts.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;f&lt;/code&gt;: the quasi-likelihood F-test&amp;nbsp;statistic.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pvalue&lt;/code&gt; and &lt;code&gt;adjpvalue&lt;/code&gt;: the quasi-likelihood F-test statistic raw and &lt;span class="caps"&gt;FDR&lt;/span&gt;-adjusted p-values,&amp;nbsp;respectively.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that the &lt;code&gt;logfc&lt;/code&gt; column is the expression in cases group relative to control group. The &lt;code&gt;edger_setup()&lt;/code&gt; custom function automatically organizes data to this&amp;nbsp;end.&lt;/p&gt;
&lt;p&gt;Usually, the researcher may want to further filter these results. For example, I like to consider not only the adjusted p-value, but also check which genes presented |logFC &amp;gt;= 1| (note the absolute value symbols here). Thus, if a gene passes these two criteria, I usually assume that it may have biological relevance for the disease/characteristic in&amp;nbsp;study.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I demonstrated a custom function that uses &lt;code&gt;edgeR&lt;/code&gt; package to perform differential expression analysis. Here is a summary of the requirements of the&amp;nbsp;function:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A R data frame: rows are the transcripts, columns are the&amp;nbsp;samples;&lt;/li&gt;
&lt;li&gt;The samples must be labeled as &amp;#8220;case&amp;#8221; or &amp;#8220;control&amp;#8221; (in the column&amp;nbsp;names);&lt;/li&gt;
&lt;li&gt;The function outputs a spreadsheet with logFC and&amp;nbsp;p-values;&lt;/li&gt;
&lt;li&gt;The reported logFC are relative to the control group (control group is the&amp;nbsp;reference);&lt;/li&gt;
&lt;li&gt;The result spreadsheets can be filtered as the researcher&amp;nbsp;wishes.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.ebi.ac.uk/training/online/course/functional-genomics-ii-common-technologies-and-data-analysis-methods/differential-gene"&gt;Differential gene expression&amp;nbsp;analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bioconductor.org/packages/release/bioc/html/edgeR.html"&gt;edgeR&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.biostars.org/p/284775/#284893"&gt;How do I explain the difference between edgeR, &lt;span class="caps"&gt;LIMMA&lt;/span&gt;, DESeq etc. to experimental&amp;nbsp;Biologist/non-bioinformatician&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.rdocumentation.org/packages/openxlsx/versions/4.2.2"&gt;openxlsx package | R&amp;nbsp;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.rdocumentation.org/packages/BiocManager/versions/1.30.10"&gt;BiocManager package | R&amp;nbsp;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.bioconductor.org/"&gt;Bioconductor -&amp;nbsp;Home&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.bioconductor.org/packages/release/bioc/html/AnnotationDbi.html"&gt;AnnotationDbi&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.rdocumentation.org/packages/annotate/versions/1.50.0"&gt;annotate package | R&amp;nbsp;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bioconductor.org/packages/release/data/annotation/html/org.Hs.eg.db.html"&gt;org.Hs.eg.db&amp;nbsp;package&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bioconductor.org/packages/release/data/annotation/html/EnsDb.Hsapiens.v79.html"&gt;EnsDb.Hsapiens.v79&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.rdocumentation.org/packages/ensembldb/versions/1.4.7"&gt;ensembldb package | R&amp;nbsp;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/load"&gt;load function | R&amp;nbsp;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/source"&gt;source function | R&amp;nbsp;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/Class/MLACourse/Modules/Genes/sample_entrez_gene_record.html"&gt;Entrez Gene&amp;nbsp;Records&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ebi.ac.uk/training-beta/online/courses/ensembl-browsing-genomes/navigating-ensembl/investigating-a-gene/"&gt;Investigating a gene |&amp;nbsp;Ensembl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.genenames.org/"&gt;&lt;span class="caps"&gt;HUGO&lt;/span&gt; Gene Nomenclature&amp;nbsp;Committee&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.cell.com/trends/genetics/fulltext/S0168-9525(13)00089-9?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS0168952513000899%3Fshowall%3Dtrue"&gt;Human housekeeping genes,&amp;nbsp;revisited&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.tau.ac.il/~elieis/HKG/"&gt;Human housekeeping genes, revisited - Supplementary&amp;nbsp;material&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Mon, 26 Oct 2020 09:50:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2020-10-26:/differential-expression-analysis-with-edger-in-r.html</guid><category>R</category><category>Bioinformatics</category><category>gene expression</category><category>edgeR</category></item><item><title>Data manipulation withÂ R</title><link>https://antoniocampos13.github.io/data-manipulation-with-r.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In my &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html#working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1"&gt;previous post&lt;/a&gt; I demonstrated how to obtain a prostate cancer dataset with genomic information in the form of gene expression quantification and created a local PostgreSQL database to hold the&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;Here, I will use R to connect to the PostgreSQL database, retrieve and then prepare the &lt;span class="caps"&gt;CGC&lt;/span&gt; data to perform a Differential Expression Analysis for sequence count data in the &lt;span class="caps"&gt;CGC&lt;/span&gt; dataset. This demonstration is on a RStudio project running in Windows 10, but the same steps can be followed on an Unix&amp;nbsp;system.&lt;/p&gt;
&lt;p&gt;As always, the code demonstrated here is available on my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/R/2020_10_19_Data_manipulation_with_R"&gt;portfolio on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Setting up .Renviron&amp;nbsp;file&lt;/h2&gt;
&lt;p&gt;In my working directory, I create a text file named &lt;code&gt;.Renviron&lt;/code&gt; to store the credentials of the PostgreSQL database with the following&amp;nbsp;information:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;userid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;lt;USER_NAME&amp;gt;&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;pwd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&amp;lt;PASSWORD&amp;gt;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Replace &lt;code&gt;&amp;lt;USER_NAME&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;PASSWORD&amp;gt;&lt;/code&gt; with your credentials used to access the PostgreSQL database. Usually, the default username is &lt;code&gt;postgres&lt;/code&gt; and the password is defined during PostgreSQL&amp;nbsp;installation.&lt;/p&gt;
&lt;h2&gt;Install/Load&amp;nbsp;packages&lt;/h2&gt;
&lt;p&gt;Then I open a RStudio session and create a project in the folder containing the &lt;code&gt;.Renviron&lt;/code&gt; file. Now, I need to load the packages I will use today. You can install them using &lt;code&gt;install.packages()&lt;/code&gt; function if you do not have them installed&amp;nbsp;yet:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Run only once&lt;/span&gt;
&lt;span class="nf"&gt;install.packages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;RPostgres&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;here&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;tidyverse&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that to install more than one package at once we must use the concatenate &lt;code&gt;c()&lt;/code&gt; command to pass the packages names and they must be quoted and separated by commas &amp;#8212; a R &lt;em&gt;vector&lt;/em&gt;. The package dependencies will be installed as&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;Now I load the packages into the R&amp;nbsp;session:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;RPostgres&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;here&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tidyverse&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;RPostgres&lt;/code&gt; is needed to connect to the PostgreSQL database I created before. The &lt;a href="https://github.com/jennybc/here_here"&gt;&lt;code&gt;here&lt;/code&gt; package&lt;/a&gt; handles file paths. &lt;code&gt;tidyverse&lt;/code&gt; is a powerful collection of packages for data manipulation. By using it, it will actually load several other packages, such as &lt;code&gt;dplyr&lt;/code&gt;, &lt;code&gt;stringr&lt;/code&gt; and &lt;code&gt;tidyr&lt;/code&gt;. It is one of the packages I use the most. Check &lt;code&gt;RPostgres&lt;/code&gt; documentation &lt;a href="https://www.rdocumentation.org/packages/RPostgres/versions/1.2.1"&gt;here&lt;/a&gt; and &lt;code&gt;tidyverse&lt;/code&gt; documentation &lt;a href="https://www.tidyverse.org/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Set up connection to tcga&amp;nbsp;database&lt;/h2&gt;
&lt;p&gt;I now set up the connection to the&amp;nbsp;database:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;con&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;dbConnect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;RPostgres&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;Postgres&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
  &lt;span class="n"&gt;dbname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;tcga&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;host&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;localhost&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;port&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;5432&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="n"&gt;user&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;Sys.getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;userid&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
  &lt;span class="n"&gt;password&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;Sys.getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;pwd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;Sys.getenv()&lt;/code&gt; will retrieve the information in the &lt;code&gt;.Renviron&lt;/code&gt; file. Remember to not share this file so your credentials remain secret. The connection credentials are now stored in the &lt;code&gt;con&lt;/code&gt; object.&lt;/p&gt;
&lt;h2&gt;Retrieve and pivot gene_counts_cases&amp;nbsp;table&lt;/h2&gt;
&lt;p&gt;With the command below, I retrieve the table containing the case-identified, raw gene counts I created last&amp;nbsp;time:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;cases&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;dbGetQuery&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;con&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;SELECT * FROM gene_counts_cases&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;dbGetQuery()&lt;/code&gt; is one of the functions from &lt;code&gt;DBI&lt;/code&gt; package, a dependency of &lt;code&gt;RPostgres&lt;/code&gt;. Check &lt;code&gt;DBI&lt;/code&gt; documentation &lt;a href="https://www.rdocumentation.org/packages/DBI/versions/0.5-1"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The information is now stored in the &lt;code&gt;cases&lt;/code&gt; object, which has three columns: &lt;code&gt;case_id&lt;/code&gt;, &lt;code&gt;gene_id&lt;/code&gt; and &lt;code&gt;gene_counts&lt;/code&gt;. Therefore, each row is a combination of a case, a gene and a gene count &amp;#8212; it is a &amp;#8220;long&amp;#8221;&amp;nbsp;format.&lt;/p&gt;
&lt;p&gt;I now will reformat the table to a &amp;#8220;wide&amp;#8221; format (pivot) because it is a requirement for the differential expression analysis. The pivoted table will have &lt;em&gt;G&lt;/em&gt;x&lt;em&gt;N&lt;/em&gt; dimensions, where &lt;em&gt;G&lt;/em&gt; are the number of rows (the number of gene transcripts quantified) and &lt;em&gt;N&lt;/em&gt; the number of columns (the number of&amp;nbsp;cases).&lt;/p&gt;
&lt;p&gt;I will use the &lt;code&gt;pivot_wider()&lt;/code&gt; function of the &lt;code&gt;tidyr&lt;/code&gt; package to do the job (&lt;strong&gt;&lt;span class="caps"&gt;WARNING&lt;/span&gt;: &lt;span class="caps"&gt;COMPUTATION&lt;/span&gt; &lt;span class="caps"&gt;INTENSIVE&lt;/span&gt; &lt;span class="caps"&gt;STEP&lt;/span&gt;&lt;/strong&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;cases_pivoted&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt;
  &lt;span class="n"&gt;cases&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="nf"&gt;pivot_wider&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;names_from&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;case_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;values_from&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;gene_count&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;values_fill&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;names_repair&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;check_unique&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;values_fn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;
  &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;code&gt;cases_pivoted&lt;/code&gt; is the name of the object that will hold the pivoted table. The &lt;code&gt;%&amp;gt;%&lt;/code&gt; is &lt;code&gt;dplyr&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s syntax. It means that we are piping the contents of &lt;code&gt;cases&lt;/code&gt; object into the &lt;code&gt;pivot_wider()&lt;/code&gt; function and its&amp;nbsp;arguments.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;names_from&lt;/code&gt; argument tells which column will be pivoted to generate new columns. The &lt;code&gt;values_from&lt;/code&gt; argument tells which column hold the values that will fill the new pivoted table. The &lt;code&gt;values_fill&lt;/code&gt; argument will substitute any missing data for a zero. The &lt;code&gt;names_repair&lt;/code&gt; argument checks that each new column has a unique name. Finally, the &lt;code&gt;values_fn&lt;/code&gt; argument indicates the function that must be applied to the values filling the new pivoted table. Note that I used the &lt;code&gt;mean&lt;/code&gt; function because during this step I noticed that some cases were associated with more than one gene expression quantification file. Therefore, I had to take the mean of these extra gene counts to correctly generate the pivoted&amp;nbsp;table.&lt;/p&gt;
&lt;p&gt;Since I calculated means for the values, I then rounded to the next integer all numerical data in the table with the help of &lt;code&gt;dplyr&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;mutate()&lt;/code&gt;, &lt;code&gt;across()&lt;/code&gt; and &lt;code&gt;where()&lt;/code&gt; functions and &lt;code&gt;round()&lt;/code&gt;, which is one of R&amp;#8217;s standard (base)&amp;nbsp;functions:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;cases_pivoted&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;across&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;is.numeric&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;round&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;where(is.numeric)&lt;/code&gt; ensures that I only manipulated numeric data in the table. The &lt;code&gt;across()&lt;/code&gt; function applies the same transformation (rounding in this case) to multiple columns. Finally, &lt;code&gt;mutate()&lt;/code&gt; adds the new variables (rounded columns), replacing the existing&amp;nbsp;ones.&lt;/p&gt;
&lt;h2&gt;Retrieve and de-duplicate follow_up&amp;nbsp;table&lt;/h2&gt;
&lt;p&gt;Now I set aside the &lt;code&gt;counts&lt;/code&gt; table for a moment to prepare the sample classifications. I realized that the &lt;code&gt;follow_up&lt;/code&gt; table had duplicate data for some reason. Thus I connected to the database and de-duplicate the &lt;code&gt;followup_primarytherapyoutcomesuccess_1&lt;/code&gt; column by using string aggregation (PostgreSQL&amp;#8217;s &lt;a href="https://www.postgresqltutorial.com/postgresql-aggregate-functions/postgresql-string_agg-function/"&gt;&lt;code&gt;STRING_AGG&lt;/code&gt; function&lt;/a&gt;). I also changed its name to &lt;code&gt;outcome&lt;/code&gt; for simplicity, and saved the results as the &lt;code&gt;followup_dedup&lt;/code&gt; table. This is the command I&amp;nbsp;used:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;dbExecute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;con&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;&amp;quot;CREATE TABLE followup_dedup AS SELECT case_id, STRING_AGG(followup_primarytherapyoutcomesuccess_1, &amp;#39;,&amp;#39;) AS outcome FROM follow_up GROUP BY case_id&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note how I used &lt;code&gt;DBI&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;dbExecute()&lt;/code&gt; instead of &lt;code&gt;dbGetQuery()&lt;/code&gt;, since I will not retrieve the table into the R&amp;nbsp;session.&lt;/p&gt;
&lt;p&gt;Now I create other table to link the cases &lt;span class="caps"&gt;ID&lt;/span&gt; numbers with the de-duplicated outcomes, saving the result as the &lt;code&gt;outcomes&lt;/code&gt; table:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;dbExecute&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="n"&gt;con&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s"&gt;&amp;quot;CREATE TABLE outcomes AS SELECT case_id, outcome FROM allcases INNER JOIN followup_dedup USING(case_id)&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I now retrieve the &lt;code&gt;outcomes&lt;/code&gt; table into the R&amp;nbsp;session:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;outcomes&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;dbGetQuery&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;con&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;SELECT * FROM outcomes&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Create sample classification and new&amp;nbsp;labels&lt;/h2&gt;
&lt;p&gt;I will now create a new column named &lt;code&gt;class&lt;/code&gt; in the outcomes table with a simplified case/control classification based on the &lt;code&gt;outcome&lt;/code&gt; column with the help of &lt;code&gt;dplyr&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;mutate()&lt;/code&gt; and &lt;code&gt;case_when()&lt;/code&gt;, which vectorizes multiple if/else statements (it is an R equivalent of the &lt;span class="caps"&gt;SQL&lt;/span&gt; &lt;code&gt;CASE WHEN&lt;/code&gt; statement):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;outcomes&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;outcomes&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;class&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;case_when&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
  &lt;span class="nf"&gt;str_detect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outcome&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Complete&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;control&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nf"&gt;str_detect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outcome&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Partial&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;case&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="nf"&gt;str_detect&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outcome&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Disease&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;case&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I will create a new label for the cases for simplification. I created a column named &lt;code&gt;new_names&lt;/code&gt; in the &lt;code&gt;outcomes&lt;/code&gt; table. The new names were created by joining the classification created in the previous step with the 12 last characters of the &lt;code&gt;case_id&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;outcomes&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;outcomes&lt;/span&gt; &lt;span class="o"&gt;%&amp;gt;%&lt;/span&gt; &lt;span class="nf"&gt;mutate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;str_sub&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;case_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;-12&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;_&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;class&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Finally, let&amp;#8217;s apply the new case labels, substituting the old ones with the help of &lt;code&gt;dplyr&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;recode()&lt;/code&gt;  function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;colnames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;dplyr&lt;/span&gt;&lt;span class="o"&gt;::&lt;/span&gt;&lt;span class="nf"&gt;recode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;colnames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="o"&gt;!!!&lt;/span&gt;&lt;span class="nf"&gt;setNames&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;as.character&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;outcomes&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;new_names&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;outcomes&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;case_id&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Convert gene_ids into row names, then delete gene_id&amp;nbsp;column&lt;/h2&gt;
&lt;p&gt;The table is almost in the state required for differential expression analysis. I just need to convert the &lt;code&gt;gene_id&lt;/code&gt; column into row names of the data&amp;nbsp;frame:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;as.data.frame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nf"&gt;row.names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="o"&gt;$&lt;/span&gt;&lt;span class="n"&gt;gene_id&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;gene_id&lt;/code&gt; is not needed anymore. I can delete&amp;nbsp;it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;counts&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;subset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;select&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;gene_id&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The data frame is now ready for differential expression analysis for sequence count data. The features (gene IDs) are embedded on the R object row names, whereas each column corresponds to a individual sample. The row/column intersection are therefore, the raw counts of gene&amp;nbsp;expression.&lt;/p&gt;
&lt;p&gt;Check the dimensions (&lt;em&gt;G&lt;/em&gt;x&lt;em&gt;N&lt;/em&gt;) of the data frame with the &lt;code&gt;dim()&lt;/code&gt; function and see that there 60483 rows and 236 columns, meaning that 60483 transcripts where quantified in samples obtained from 236 individuals with prostate&amp;nbsp;cancer.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;dim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;counts&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Output:&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="m"&gt;60483&lt;/span&gt;   &lt;span class="m"&gt;236&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;See below first few rows and columns of the finalized &lt;code&gt;counts&lt;/code&gt; data&amp;nbsp;frame:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Final gene counts table" src="https://antoniocampos13.github.io/images/counts_final.PNG"&gt;&lt;/p&gt;
&lt;p&gt;In a future post I will demonstrate the differential expression analysis &lt;em&gt;per se&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I demonstrated how to connect to a local PostgreSQL database with &lt;code&gt;Rpostgres&lt;/code&gt; and &lt;code&gt;DBI&lt;/code&gt; packages;&lt;/li&gt;
&lt;li&gt;Reorganized data by pivoting a long data frame to a wider data frame with gene IDs in rows and samples in columns with &lt;code&gt;dplyr&lt;/code&gt; package&amp;nbsp;functions;&lt;/li&gt;
&lt;li&gt;Labeled columns as cases or controls for the differential expression analysis, also with &lt;code&gt;dplyr&lt;/code&gt; package.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Go back to &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html#working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1"&gt;Working with Cancer Genomics Cloud datasets in a PostgreSQL database PartÂ 1&lt;/a&gt; and &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-2.html#working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-2"&gt;Part 2&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/jennybc/here_here"&gt;Ode to the here&amp;nbsp;package&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.rdocumentation.org/packages/RPostgres/versions/1.2.1"&gt;RPostgres package | R&amp;nbsp;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.tidyverse.org/"&gt;Tidyverse&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.rdocumentation.org/packages/DBI/versions/0.5-1"&gt;&lt;span class="caps"&gt;DBI&lt;/span&gt; package | R&amp;nbsp;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.postgresqltutorial.com/postgresql-aggregate-functions/postgresql-string_agg-function/"&gt;PostgreSQL STRING_AGG() Function By Practical&amp;nbsp;Examples&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Mon, 19 Oct 2020 13:30:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2020-10-19:/data-manipulation-with-r.html</guid><category>R</category><category>Bioinformatics</category><category>gene expression</category><category>SQL</category><category>PostgreSQL</category></item><item><title>Meta-analysis and Meta-regression withÂ R</title><link>https://antoniocampos13.github.io/meta-analysis-and-meta-regression-with-r.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;On December 2019, reports from severe acute respiratory syndrome in Wuhan, China, were linked to a novel coronavirus, now known as &lt;span class="caps"&gt;SARS&lt;/span&gt;-CoV-2, and the disease it causes was termed coronavirus disease 2019 (&lt;span class="caps"&gt;COVID&lt;/span&gt;-19).&lt;/p&gt;
&lt;p&gt;The World Health Organization declared the &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 outbreak a Public Health Emergency of International Concern on 30 January 2020, and a pandemic on 11 March&amp;nbsp;2020.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;SARS&lt;/span&gt;-CoV-2 causes an interstitial and alveolar pneumonia, but in nearly 26% of patients it can lead to a severe disease, with multiple organ failure (references: &lt;a href="https://www.bmj.com/content/369/bmj.m1985"&gt;1&lt;/a&gt;, &lt;a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2763184"&gt;2&lt;/a&gt;, &lt;a href="https://linkinghub.elsevier.com/retrieve/pii/S0140673620311892"&gt;3&lt;/a&gt;, &lt;a href="https://linkinghub.elsevier.com/retrieve/pii/S0163445320301705"&gt;4&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Thus, infection by &lt;span class="caps"&gt;SARS&lt;/span&gt;-CoV-2 can cause kidney injury, perhaps by direct impact of  virulence, or via other renal insults such as volume depletion, hypoxia and cytokine storm. This has posed pressure to healthcare systems due to a shortage of dialysis staff, equipment and consumables throughout the&amp;nbsp;world.&lt;/p&gt;
&lt;p&gt;I recently &lt;a href="https://jcp.bmj.com/content/early/2020/10/06/jclinpath-2020-207023.long"&gt;co-authored an article&lt;/a&gt; regarding acute kidney injury (&lt;span class="caps"&gt;AKI&lt;/span&gt;) among patients infected with &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 patients. The objective of the study was to systematically review the incidence of &lt;span class="caps"&gt;COVID&lt;/span&gt;-19-associated &lt;span class="caps"&gt;AKI&lt;/span&gt;, its related risks factors, need for renal replacement therapy (&lt;span class="caps"&gt;RRT&lt;/span&gt;) among patients with &lt;span class="caps"&gt;AKI&lt;/span&gt; and mortality according to current illness&amp;nbsp;severity.&lt;/p&gt;
&lt;p&gt;In this post I will show the code I used to meta-analyze the &lt;span class="caps"&gt;AKI&lt;/span&gt; incidence data among &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 patients. As usual, the codes presented here are available on the corresponding folder on my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/R/2020_10_13_Meta-analysis_with_R"&gt;GitHub portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Methods&lt;/h2&gt;
&lt;h3&gt;Systematic Review&amp;nbsp;methodology&lt;/h3&gt;
&lt;p&gt;My colleagues systematically searched PubMed, &lt;span class="caps"&gt;SCOPUS&lt;/span&gt; and Web of Science databases with the search terms &amp;#8220;&lt;span class="caps"&gt;COVID&lt;/span&gt;-19&amp;#8221; &lt;span class="caps"&gt;OR&lt;/span&gt; &amp;#8220;&lt;span class="caps"&gt;SARS&lt;/span&gt;- CoV-2&amp;#8221; &lt;span class="caps"&gt;OR&lt;/span&gt; &amp;#8220;Coronavirus 2019&amp;#8221; &lt;span class="caps"&gt;OR&lt;/span&gt; &amp;#8220;2019- nCoV&amp;#8221; &lt;span class="caps"&gt;AND&lt;/span&gt; &amp;#8220;Acute Kidney Injury&amp;#8221; &lt;span class="caps"&gt;OR&lt;/span&gt; &amp;#8220;Kidney&amp;#8221; &lt;span class="caps"&gt;OR&lt;/span&gt; &amp;#8220;Nephrology&amp;#8221; &lt;span class="caps"&gt;OR&lt;/span&gt; &amp;#8220;Renal Disease&amp;#8221; &lt;span class="caps"&gt;OR&lt;/span&gt; &amp;#8220;Clinical Characteristics&amp;#8221; &lt;span class="caps"&gt;OR&lt;/span&gt; &amp;#8220;Clinical&amp;nbsp;Features&amp;#8221;.&lt;/p&gt;
&lt;h3&gt;Meta-analysis and&amp;nbsp;meta-regression&lt;/h3&gt;
&lt;p&gt;The heterogeneity between studies sample sizes was expressed through I&lt;sup&gt;2&lt;/sup&gt; measure and &amp;tau;&lt;sup&gt;2&lt;/sup&gt; statistic (estimated via maximum- likelihood). A Cochran&amp;#8221;s Q test with kâ1 df (in which k is the number of studies included) and with significance level &amp;alpha;=0.10 (exclusively for this test) was conducted to assess if heterogeneity was significantly different from zero. If so, the heterogeneity was classified according to the observed I&lt;sup&gt;2&lt;/sup&gt; measure: â¤25%, between 25% and 50%, between 50% and 75% and between 75% and 100% were considered as low, moderate, high and very high heterogeneity, respectively. In case of significant heterogeneity, the meta-analyses were conducted assuming a random- effects model with the Hartung-Knapp adjustment (References: &lt;a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.791"&gt;6&lt;/a&gt;, &lt;a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1009"&gt;7&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The independent variables included in a multivariate meta-regression were: study location (China, Poland or the &lt;span class="caps"&gt;USA&lt;/span&gt;), study design (case series, prospective or retrospective cohorts), study setting (single or multicenter), &lt;span class="caps"&gt;AKI&lt;/span&gt; definition criteria, median age of patients in years, proportion of males patients in the sample size and proportion of patients with certain comorbidities, namely hypertension, other cardio-vascular diseases, diabetes, chronic pulmonary disease, chronic kidney disease (&lt;span class="caps"&gt;CKD&lt;/span&gt;) and&amp;nbsp;cancer.&lt;/p&gt;
&lt;h2&gt;Results&lt;/h2&gt;
&lt;h3&gt;Results of the systematic&amp;nbsp;review&lt;/h3&gt;
&lt;p&gt;My colleagues found 21 studies and I was responsible for perform the meta-analysis. Overall, 15,536 patients with &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 was the total sample size. Most studies (18) were from China, two were from the &lt;span class="caps"&gt;USA&lt;/span&gt; and one from&amp;nbsp;Poland.&lt;/p&gt;
&lt;h3&gt;Incidence of &lt;span class="caps"&gt;AKI&lt;/span&gt; among patients with &lt;span class="caps"&gt;COVID&lt;/span&gt;-19&lt;/h3&gt;
&lt;p&gt;The meta-analysis showed an incidence of &lt;span class="caps"&gt;AKI&lt;/span&gt; associated with &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 of 12.3% (95% &lt;span class="caps"&gt;CI&lt;/span&gt;=7.3% to 20.0%; Cochranâs Q=839.6 with 20 degrees of freedom, p&amp;lt;0.001; I&lt;sup&gt;2&lt;/sup&gt; = 97.6% and &amp;tau;&lt;sup&gt;2&lt;/sup&gt; =&amp;nbsp;1.42).&lt;/p&gt;
&lt;p&gt;Below I show the code that generated this&amp;nbsp;result.&lt;/p&gt;
&lt;h4&gt;Loading&amp;nbsp;packages&lt;/h4&gt;
&lt;p&gt;I used R software with &lt;a href="https://rstudio.com/"&gt;RStudio&lt;/a&gt; in Windows 10 to perform the meta-analysis (the same steps can be followed in Unix systems). I opened a R session through RStudio, &lt;a href="https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects"&gt;created a project&lt;/a&gt;, named it &lt;code&gt;aki_demo&lt;/code&gt; and loaded all packages needed for the meta-analysis. The command to install them&amp;nbsp;is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Run only once, if you do not have the packages installed yet&lt;/span&gt;
&lt;span class="nf"&gt;install.packages&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;meta&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;here&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;openxlsx&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The packages and their dependencies will be downloaded from the internet and installed. After install, I must load them in the R&amp;nbsp;session:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;meta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;here&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;library&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;openxlsx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4&gt;Load and inspect&amp;nbsp;data&lt;/h4&gt;
&lt;p&gt;Now I can load the data. I prepared an Excel spreadsheet with an excerpt of the data of our paper and put inside a subfolder named &lt;code&gt;data&lt;/code&gt; on my working directory. The current script I saved on a folder named &lt;code&gt;src&lt;/code&gt; (for &amp;#8220;source&amp;#8221;). My project folder now look like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
âââ data
â   âââ aki_demo.xlsx
âââ output
âââ src
â   âââ aki_demo.R
âââ .Rhistory
âââ aki_demo.RProj
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I load the spreadsheet into the R session memory with the&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;aki&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;read.xlsx&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;aki_demo.xlsx&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;&amp;lt;-&lt;/code&gt;, which is simply the &amp;#8220;less than&amp;#8221; &lt;code&gt;&amp;lt;&lt;/code&gt; symbol followed by a dash symbol &lt;code&gt;-&lt;/code&gt;, means &amp;#8220;assigning&amp;#8221;. I am assigning the data to the object named &lt;code&gt;aki&lt;/code&gt;. The &lt;code&gt;read.xlsx()&lt;/code&gt; function from &lt;code&gt;openxlsx&lt;/code&gt; package reads the content of the spreadsheet and loads into a &lt;strong&gt;data frame&lt;/strong&gt;, a tabular R format that holds&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;Note the usage of the &lt;code&gt;here()&lt;/code&gt; function. &lt;code&gt;here&lt;/code&gt; is a nice R package that helps file paths management. It ensures that my code will run smoothly on any computer, provided the directory structure is maintained. Thus, there is no need to hardcode file paths, improving code portability/reproducibility. The &lt;a href="https://github.com/jennybc/here_here"&gt;Dr. Jennifer Bryan&amp;#8217;s post&lt;/a&gt; is a nice introduction to the &lt;code&gt;here&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;To see the contents of the spreadsheet, type &lt;code&gt;aki&lt;/code&gt; in the RStudio console window (bottom left area of RStudio window) and press Enter/Return or select the or put the cursor at the start of the line in the script window (top left) and press &lt;code&gt;CTRL + Enter&lt;/code&gt; (&lt;code&gt;â Command + Enter&lt;/code&gt; on&amp;nbsp;MacOS).&lt;/p&gt;
&lt;p&gt;&lt;img alt="RStudio Script and Console windows" src="https://antoniocampos13.github.io/images/RStudio_console.PNG"&gt;&lt;/p&gt;
&lt;p&gt;The first five columns of the output are below (I manually omitted the remaining of the&amp;nbsp;table):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;aki&lt;/span&gt;
                          &lt;span class="n"&gt;authors&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="n"&gt;sample_size&lt;/span&gt; &lt;span class="n"&gt;control_aki&lt;/span&gt; &lt;span class="n"&gt;case_aki&lt;/span&gt;
&lt;span class="m"&gt;1&lt;/span&gt;           &lt;span class="n"&gt;Tao&lt;/span&gt; &lt;span class="n"&gt;Chen&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;         &lt;span class="m"&gt;274&lt;/span&gt;         &lt;span class="m"&gt;245&lt;/span&gt;       &lt;span class="m"&gt;29&lt;/span&gt;
&lt;span class="m"&gt;2&lt;/span&gt;            &lt;span class="n"&gt;W.&lt;/span&gt; &lt;span class="n"&gt;Guan&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;        &lt;span class="m"&gt;1099&lt;/span&gt;        &lt;span class="m"&gt;1093&lt;/span&gt;        &lt;span class="m"&gt;6&lt;/span&gt;
&lt;span class="m"&gt;3&lt;/span&gt;        &lt;span class="n"&gt;Xiaobo&lt;/span&gt; &lt;span class="n"&gt;Yang&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;          &lt;span class="m"&gt;52&lt;/span&gt;          &lt;span class="m"&gt;37&lt;/span&gt;       &lt;span class="m"&gt;15&lt;/span&gt;
&lt;span class="m"&gt;4&lt;/span&gt;           &lt;span class="n"&gt;Fei&lt;/span&gt; &lt;span class="n"&gt;Zhou&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;         &lt;span class="m"&gt;191&lt;/span&gt;         &lt;span class="m"&gt;163&lt;/span&gt;       &lt;span class="m"&gt;28&lt;/span&gt;
&lt;span class="m"&gt;5&lt;/span&gt;        &lt;span class="n"&gt;Yingzhen&lt;/span&gt; &lt;span class="n"&gt;Du&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;          &lt;span class="m"&gt;85&lt;/span&gt;          &lt;span class="m"&gt;37&lt;/span&gt;       &lt;span class="m"&gt;48&lt;/span&gt;
&lt;span class="m"&gt;6&lt;/span&gt;      &lt;span class="n"&gt;Chaolin&lt;/span&gt; &lt;span class="n"&gt;Huang&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;          &lt;span class="m"&gt;41&lt;/span&gt;          &lt;span class="m"&gt;38&lt;/span&gt;        &lt;span class="m"&gt;3&lt;/span&gt;
&lt;span class="m"&gt;7&lt;/span&gt;       &lt;span class="n"&gt;Qingxian&lt;/span&gt; &lt;span class="n"&gt;Cai&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;         &lt;span class="m"&gt;298&lt;/span&gt;         &lt;span class="m"&gt;281&lt;/span&gt;       &lt;span class="m"&gt;17&lt;/span&gt;
&lt;span class="m"&gt;8&lt;/span&gt;       &lt;span class="n"&gt;Yichun&lt;/span&gt; &lt;span class="n"&gt;Cheng&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;         &lt;span class="m"&gt;701&lt;/span&gt;         &lt;span class="m"&gt;665&lt;/span&gt;       &lt;span class="m"&gt;36&lt;/span&gt;
&lt;span class="m"&gt;9&lt;/span&gt;       &lt;span class="n"&gt;Shaoqing&lt;/span&gt; &lt;span class="n"&gt;Lei&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;          &lt;span class="m"&gt;34&lt;/span&gt;          &lt;span class="m"&gt;32&lt;/span&gt;        &lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="m"&gt;10&lt;/span&gt;        &lt;span class="n"&gt;Dawei&lt;/span&gt; &lt;span class="n"&gt;Wang&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;         &lt;span class="m"&gt;138&lt;/span&gt;         &lt;span class="m"&gt;133&lt;/span&gt;        &lt;span class="m"&gt;5&lt;/span&gt;
&lt;span class="m"&gt;11&lt;/span&gt;         &lt;span class="n"&gt;Lang&lt;/span&gt; &lt;span class="n"&gt;Wang&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;         &lt;span class="m"&gt;339&lt;/span&gt;         &lt;span class="m"&gt;312&lt;/span&gt;       &lt;span class="m"&gt;27&lt;/span&gt;
&lt;span class="m"&gt;12&lt;/span&gt;   &lt;span class="n"&gt;Jamie&lt;/span&gt; &lt;span class="n"&gt;S.&lt;/span&gt; &lt;span class="n"&gt;Hirsch&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;      &lt;span class="n"&gt;USA&lt;/span&gt;        &lt;span class="m"&gt;5449&lt;/span&gt;        &lt;span class="m"&gt;3456&lt;/span&gt;     &lt;span class="m"&gt;1993&lt;/span&gt;
&lt;span class="m"&gt;13&lt;/span&gt;      &lt;span class="n"&gt;Blazej&lt;/span&gt; &lt;span class="n"&gt;Nowak&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;   &lt;span class="n"&gt;Poland&lt;/span&gt;         &lt;span class="m"&gt;169&lt;/span&gt;         &lt;span class="m"&gt;152&lt;/span&gt;       &lt;span class="m"&gt;17&lt;/span&gt;
&lt;span class="m"&gt;14&lt;/span&gt;          &lt;span class="n"&gt;Yi&lt;/span&gt; &lt;span class="n"&gt;Zheng&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;          &lt;span class="m"&gt;34&lt;/span&gt;          &lt;span class="m"&gt;27&lt;/span&gt;        &lt;span class="m"&gt;7&lt;/span&gt;
&lt;span class="m"&gt;15&lt;/span&gt;         &lt;span class="n"&gt;Dawei&lt;/span&gt; &lt;span class="n"&gt;Wang&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;         &lt;span class="m"&gt;107&lt;/span&gt;          &lt;span class="m"&gt;93&lt;/span&gt;       &lt;span class="m"&gt;14&lt;/span&gt;
&lt;span class="m"&gt;16&lt;/span&gt;           &lt;span class="n"&gt;Yuan&lt;/span&gt; &lt;span class="n"&gt;Yu&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;         &lt;span class="m"&gt;226&lt;/span&gt;         &lt;span class="m"&gt;169&lt;/span&gt;       &lt;span class="m"&gt;57&lt;/span&gt;
&lt;span class="m"&gt;17&lt;/span&gt; &lt;span class="n"&gt;Safiya&lt;/span&gt; &lt;span class="n"&gt;Richardson&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;      &lt;span class="n"&gt;USA&lt;/span&gt;        &lt;span class="m"&gt;5700&lt;/span&gt;        &lt;span class="m"&gt;4330&lt;/span&gt;     &lt;span class="m"&gt;1370&lt;/span&gt;
&lt;span class="m"&gt;18&lt;/span&gt;    &lt;span class="n"&gt;Guangchang&lt;/span&gt; &lt;span class="n"&gt;Pei&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;         &lt;span class="m"&gt;333&lt;/span&gt;         &lt;span class="m"&gt;298&lt;/span&gt;       &lt;span class="m"&gt;35&lt;/span&gt;
&lt;span class="m"&gt;19&lt;/span&gt;         &lt;span class="n"&gt;Yanlei&lt;/span&gt; &lt;span class="n"&gt;Li&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;          &lt;span class="m"&gt;54&lt;/span&gt;          &lt;span class="m"&gt;25&lt;/span&gt;       &lt;span class="m"&gt;29&lt;/span&gt;
&lt;span class="m"&gt;20&lt;/span&gt;      &lt;span class="n"&gt;Qingchun&lt;/span&gt; &lt;span class="n"&gt;Yao&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;         &lt;span class="m"&gt;108&lt;/span&gt;          &lt;span class="m"&gt;92&lt;/span&gt;       &lt;span class="m"&gt;16&lt;/span&gt;
&lt;span class="m"&gt;21&lt;/span&gt;     &lt;span class="n"&gt;Chengfeng&lt;/span&gt; &lt;span class="n"&gt;Qiu&lt;/span&gt; &lt;span class="n"&gt;et&lt;/span&gt; &lt;span class="n"&gt;al.&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;2020&lt;/span&gt;    &lt;span class="n"&gt;China&lt;/span&gt;         &lt;span class="m"&gt;104&lt;/span&gt;         &lt;span class="m"&gt;102&lt;/span&gt;        &lt;span class="m"&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Check that are 21 studies (rows) and the first five columns are: &lt;code&gt;authors&lt;/code&gt;, &lt;code&gt;location&lt;/code&gt;, &lt;code&gt;sample_size&lt;/code&gt; (the number of &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 patients), &lt;code&gt;control_aki&lt;/code&gt; (the number of &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 patients without &lt;span class="caps"&gt;AKI&lt;/span&gt;) and &lt;code&gt;case_aki&lt;/code&gt; (the number of &lt;span class="caps"&gt;AKI&lt;/span&gt; cases among the &lt;span class="caps"&gt;COVID&lt;/span&gt;-19&amp;nbsp;patients).&lt;/p&gt;
&lt;p&gt;To check the name of the other columns,&amp;nbsp;type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;names&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;aki&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Output:&lt;/span&gt;
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;authors&amp;quot;&lt;/span&gt;        &lt;span class="s"&gt;&amp;quot;location&amp;quot;&lt;/span&gt;       &lt;span class="s"&gt;&amp;quot;sample_size&amp;quot;&lt;/span&gt;    &lt;span class="s"&gt;&amp;quot;control_aki&amp;quot;&lt;/span&gt;   
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;case_aki&amp;quot;&lt;/span&gt;       &lt;span class="s"&gt;&amp;quot;age&amp;quot;&lt;/span&gt;            &lt;span class="s"&gt;&amp;quot;design&amp;quot;&lt;/span&gt;         &lt;span class="s"&gt;&amp;quot;setting&amp;quot;&lt;/span&gt;       
 &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;9&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;aki_criteria&amp;quot;&lt;/span&gt;   &lt;span class="s"&gt;&amp;quot;males_p&amp;quot;&lt;/span&gt;        &lt;span class="s"&gt;&amp;quot;hypertension_p&amp;quot;&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;all_cardio_p&amp;quot;&lt;/span&gt;  
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;13&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;diabetes_p&amp;quot;&lt;/span&gt;     &lt;span class="s"&gt;&amp;quot;copd_p&amp;quot;&lt;/span&gt;         &lt;span class="s"&gt;&amp;quot;ckd_p&amp;quot;&lt;/span&gt;          &lt;span class="s"&gt;&amp;quot;cancer_p&amp;quot;&lt;/span&gt;   
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, it is possible to see that there are 16 columns in total. The bracketed number in the left side of the line is the index of the first element in the line (the count starts with &lt;code&gt;[1] authors&lt;/code&gt;; the fourth element is &lt;code&gt;[5] case_aki&lt;/code&gt; column, and so&amp;nbsp;on).&lt;/p&gt;
&lt;p&gt;To check the shape (dimensions) of the table I&amp;nbsp;type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;dim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;aki&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Output:&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="m"&gt;21&lt;/span&gt; &lt;span class="m"&gt;16&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output tells that there are 21 rows and 16 columns&amp;nbsp;indeed.&lt;/p&gt;
&lt;h4&gt;Calculating meta-analysis and inspecting&amp;nbsp;results&lt;/h4&gt;
&lt;p&gt;Now I will use the &lt;code&gt;metaprop()&lt;/code&gt; function from &lt;code&gt;meta&lt;/code&gt; package to calculate the pooled incidence (a &lt;strong&gt;proportion&lt;/strong&gt;) of &lt;span class="caps"&gt;AKI&lt;/span&gt; among &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 patients and assign the results to &lt;code&gt;aki_incidence_meta&lt;/code&gt; object:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;aki_incidence_meta&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;metaprop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;event&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;case_aki&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# numerator&lt;/span&gt;
                   &lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sample_size&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# denominator&lt;/span&gt;
                   &lt;span class="n"&gt;studlab&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;paste0&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;authors&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;, &amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;# makes label by joining authors and location&lt;/span&gt;
                   &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;aki&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# dataframe name&lt;/span&gt;
                   &lt;span class="n"&gt;sm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;PLO&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# pooling calculation method&lt;/span&gt;
                   &lt;span class="n"&gt;predict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# provides prediction confidence interval&lt;/span&gt;
                   &lt;span class="n"&gt;hakn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# Hartung-Knapp correction&lt;/span&gt;
                   &lt;span class="n"&gt;comb.fixed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# displays fixed-effect results&lt;/span&gt;
                   &lt;span class="n"&gt;comb.random&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# displays random-effect results&lt;/span&gt;
                   &lt;span class="n"&gt;level.comb&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.95&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# confidence interval&lt;/span&gt;
                   &lt;span class="n"&gt;method.tau&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ML&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# heterogeneity calculation method&lt;/span&gt;
                   &lt;span class="n"&gt;method.bias&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;linreg&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                   &lt;span class="n"&gt;warn&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note that I type the command in this &amp;#8220;list&amp;#8221; format to allow easier reading of the code, it is not mandatory. The &lt;code&gt;event&lt;/code&gt; argument is the numerator (thus &lt;code&gt;aki_case&lt;/code&gt; column) and &lt;code&gt;n&lt;/code&gt; is the denominator (&lt;code&gt;sample_size&lt;/code&gt; column). I summarized above what the principal arguments do. Check &lt;code&gt;metaprop()&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s documentation page &lt;a href="https://www.rdocumentation.org/packages/meta/versions/4.9-6/topics/metaprop"&gt;here&lt;/a&gt; for more details. The &lt;code&gt;meta&lt;/code&gt; package has other functions to perform meta-analysis with other kind of data. Check the documentation &lt;a href="https://www.rdocumentation.org/packages/meta/versions/4.9-6"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Retrieving a summary of the &lt;code&gt;aki_incidence_meta&lt;/code&gt; object produces this&amp;nbsp;output:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Command&lt;/span&gt;
&lt;span class="nf"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;aki_incidence_meta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Output:&lt;/span&gt;
&lt;span class="n"&gt;Number&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;studies&lt;/span&gt; &lt;span class="n"&gt;combined&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;21&lt;/span&gt;

                     &lt;span class="n"&gt;proportion&lt;/span&gt;           &lt;span class="m"&gt;95&lt;/span&gt;&lt;span class="o"&gt;%-CI&lt;/span&gt;
&lt;span class="o"&gt;Fixed effect model       0.2801 [0.2724; 0.2879]&lt;/span&gt;
&lt;span class="o"&gt;Random effects model     0.1227 [0.0725; 0.2003]&lt;/span&gt;
&lt;span class="o"&gt;Prediction interval             [0.0107; 0.6451]&lt;/span&gt;

&lt;span class="o"&gt;Quantifying heterogeneity:&lt;/span&gt;
&lt;span class="o"&gt; tau^2 = 1.4231 [0.8378; 3.3344]; tau = 1.1929 [0.9153; 1.8260];&lt;/span&gt;
&lt;span class="o"&gt; I^2 = 97.6%&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;97.1&lt;/span&gt;&lt;span class="o"&gt;%; 98.1%&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt; &lt;span class="n"&gt;H&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;6.48&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="m"&gt;5.83&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="m"&gt;7.20&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="n"&gt;Test&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;heterogeneity&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
      &lt;span class="n"&gt;Q&lt;/span&gt; &lt;span class="n"&gt;d.f.&lt;/span&gt;  &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;value&lt;/span&gt;
 &lt;span class="m"&gt;839.62&lt;/span&gt;   &lt;span class="m"&gt;20&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="m"&gt;0.0001&lt;/span&gt;

&lt;span class="n"&gt;Details&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt; &lt;span class="n"&gt;meta&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;analytical&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Inverse&lt;/span&gt; &lt;span class="n"&gt;variance&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Maximum&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="n"&gt;estimator&lt;/span&gt; &lt;span class="n"&gt;for&lt;/span&gt; &lt;span class="n"&gt;tau&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Q&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;profile&lt;/span&gt; &lt;span class="n"&gt;method&lt;/span&gt; &lt;span class="n"&gt;for&lt;/span&gt; &lt;span class="n"&gt;confidence&lt;/span&gt; &lt;span class="n"&gt;interval&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;tau&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="n"&gt;and&lt;/span&gt; &lt;span class="n"&gt;tau&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Hartung&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Knapp&lt;/span&gt; &lt;span class="n"&gt;adjustment&lt;/span&gt; &lt;span class="n"&gt;for&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt; &lt;span class="n"&gt;effects&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;
&lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;Logit&lt;/span&gt; &lt;span class="n"&gt;transformation&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Since there was high heterogeneity, I chose to interpret the results through a random-effects model, yielding the numbers I quoted above. Below the results are the heterogeneity reports, results of Cochran&amp;#8217;s Q test statistic, degrees-of-freedom and p-value confirming that the heterogeneity is not negligible and a summary of statistical methods used in the&amp;nbsp;meta-analysis.&lt;/p&gt;
&lt;p&gt;If you want to save this output directly to file, you could use the &lt;code&gt;sink()&lt;/code&gt; method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;sink&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;output&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;aki_incidence.txt&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nf"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;aki_incidence_meta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;sink&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It will save the raw output in text format in a file named &lt;code&gt;aki_incidence.txt&lt;/code&gt; into the &lt;code&gt;output&lt;/code&gt; folder.&lt;/p&gt;
&lt;h4&gt;Producing a Forest&amp;nbsp;plot&lt;/h4&gt;
&lt;p&gt;Now I will generate a Forest plot with &lt;code&gt;forest()&lt;/code&gt; function (also from &lt;code&gt;meta&lt;/code&gt; package) to visually represent the meta-analysis results and save it to a &lt;code&gt;TIFF&lt;/code&gt; file.  The commands below open the &lt;code&gt;TIFF&lt;/code&gt; graphical device, produce the plot and then closes the&amp;nbsp;device:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Open the image device&lt;/span&gt;
&lt;span class="nf"&gt;tiff&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filename&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;here&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;output&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;plots&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;aki_incidence.tiff&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
     &lt;span class="n"&gt;compression&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;lzw&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;300&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="n"&gt;width&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;3050&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
     &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;1750&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Produce the plot&lt;/span&gt;
&lt;span class="nf"&gt;forest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;aki_incidence_meta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
             &lt;span class="n"&gt;xlim&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;# set axis limits&lt;/span&gt;
             &lt;span class="n"&gt;comb.fixed&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt;  &lt;span class="kc"&gt;FALSE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="c1"&gt;# omit fixed-effect model results&lt;/span&gt;
             &lt;span class="n"&gt;leftlabs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;Study, Location&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;AKI cases&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Sample size&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;# set left-side column names&lt;/span&gt;
             &lt;span class="n"&gt;rightlabs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nf"&gt;c&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;AKI incidence&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;95% CI&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;Weight&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="c1"&gt;# set right-side column names&lt;/span&gt;
             &lt;span class="n"&gt;pooled.events&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;TRUE&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
             &lt;span class="n"&gt;col.predict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;black&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Close the device and save the plot&lt;/span&gt;
&lt;span class="nf"&gt;dev.off&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Check other R graphical devices commonly used to save plots as images &lt;a href="https://stat.ethz.ch/R-manual/R-devel/library/grDevices/html/png.html"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Calculating a&amp;nbsp;meta-regression&lt;/h4&gt;
&lt;p&gt;To calculate the meta-analysis, I needed just the sample size and number of events. I can use the remaining variables in the data frame to calculate a meta-regression to assess if aggregate measures of patients characteristics would be associated with increased risk of &lt;span class="caps"&gt;AKI&lt;/span&gt; occurrence. Below is the command I&amp;nbsp;used:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;aki_incidence_metareg&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;metareg&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;aki_incidence_meta&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;design&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;setting&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;location&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;age&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;males_p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;aki_criteria&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;hypertension_p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;all_cardio_p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;diabetes_p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;copd_p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;ckd_p&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;cancer_p&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I used the &lt;code&gt;aki_incidence_meta&lt;/code&gt; meta-analysis object as input to the &lt;code&gt;metareg&lt;/code&gt; function. After a comma, I write a tilde &lt;code&gt;~&lt;/code&gt; and the list all covariates separated by plus signs &lt;code&gt;+&lt;/code&gt;. In R notation, the tilde separates the dependent covariate from the independent variables. Remember they are all columns from the spreadsheet I imported in the beginning. I check the results using the &lt;code&gt;summary()&lt;/code&gt; function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nf"&gt;summary&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;aki_incidence_metareg&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Output:&lt;/span&gt;
&lt;span class="n"&gt;Mixed&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;Effects&lt;/span&gt; &lt;span class="nf"&gt;Model &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;21&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="n"&gt;tau&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="n"&gt;estimator&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="n"&gt;ML&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

  &lt;span class="n"&gt;logLik&lt;/span&gt;  &lt;span class="n"&gt;deviance&lt;/span&gt;       &lt;span class="n"&gt;AIC&lt;/span&gt;       &lt;span class="n"&gt;BIC&lt;/span&gt;      &lt;span class="n"&gt;AICc&lt;/span&gt; 
&lt;span class="m"&gt;-20.3487&lt;/span&gt;   &lt;span class="m"&gt;62.5815&lt;/span&gt;   &lt;span class="m"&gt;76.6975&lt;/span&gt;   &lt;span class="m"&gt;95.4989&lt;/span&gt;  &lt;span class="m"&gt;418.6975&lt;/span&gt;   

&lt;span class="n"&gt;tau&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;estimated&lt;/span&gt; &lt;span class="n"&gt;amount&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;residual&lt;/span&gt; &lt;span class="n"&gt;heterogeneity&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;     &lt;span class="m"&gt;0.2864&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.1130&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nf"&gt;tau &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;square&lt;/span&gt; &lt;span class="n"&gt;root&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="n"&gt;estimated&lt;/span&gt; &lt;span class="n"&gt;tau&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;             &lt;span class="m"&gt;0.5352&lt;/span&gt;
&lt;span class="n"&gt;I&lt;/span&gt;&lt;span class="o"&gt;^&lt;/span&gt;&lt;span class="m"&gt;2&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;residual&lt;/span&gt; &lt;span class="n"&gt;heterogeneity&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;unaccounted&lt;/span&gt; &lt;span class="n"&gt;variability&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="m"&gt;77.00&lt;/span&gt;&lt;span class="o"&gt;%&lt;/span&gt;
&lt;span class="o"&gt;H^2 (unaccounted variability / sampling variability):   4.35&lt;/span&gt;
&lt;span class="o"&gt;R^2 (amount of heterogeneity accounted for):            79.88%&lt;/span&gt;

&lt;span class="n"&gt;Test&lt;/span&gt; &lt;span class="n"&gt;for&lt;/span&gt; &lt;span class="n"&gt;Residual&lt;/span&gt; &lt;span class="n"&gt;Heterogeneity&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="nf"&gt;QE&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;90.3774&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;.&lt;/span&gt;&lt;span class="m"&gt;0001&lt;/span&gt;

&lt;span class="n"&gt;Test&lt;/span&gt; &lt;span class="n"&gt;of&lt;/span&gt; &lt;span class="nf"&gt;Moderators &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coefficients&lt;/span&gt; &lt;span class="m"&gt;2&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="m"&gt;17&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
&lt;span class="nf"&gt;F&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.6824&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;val&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;0.7411&lt;/span&gt;

&lt;span class="n"&gt;Model&lt;/span&gt; &lt;span class="n"&gt;Results&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;

                                     &lt;span class="n"&gt;estimate&lt;/span&gt;       &lt;span class="n"&gt;se&lt;/span&gt;     &lt;span class="n"&gt;tval&lt;/span&gt;    &lt;span class="n"&gt;pval&lt;/span&gt;      &lt;span class="n"&gt;ci.lb&lt;/span&gt;     &lt;span class="n"&gt;ci.ub&lt;/span&gt; 
&lt;span class="n"&gt;intrcpt&lt;/span&gt;                               &lt;span class="m"&gt;-7.1298&lt;/span&gt;   &lt;span class="m"&gt;7.4538&lt;/span&gt;  &lt;span class="m"&gt;-0.9565&lt;/span&gt;  &lt;span class="m"&gt;0.3930&lt;/span&gt;   &lt;span class="m"&gt;-27.8250&lt;/span&gt;   &lt;span class="m"&gt;13.5654&lt;/span&gt;    
&lt;span class="n"&gt;designprospective&lt;/span&gt;                      &lt;span class="m"&gt;3.9234&lt;/span&gt;   &lt;span class="m"&gt;4.6568&lt;/span&gt;   &lt;span class="m"&gt;0.8425&lt;/span&gt;  &lt;span class="m"&gt;0.4469&lt;/span&gt;    &lt;span class="m"&gt;-9.0058&lt;/span&gt;   &lt;span class="m"&gt;16.8527&lt;/span&gt;    
&lt;span class="n"&gt;designretrospective&lt;/span&gt;                    &lt;span class="m"&gt;4.8177&lt;/span&gt;   &lt;span class="m"&gt;4.3793&lt;/span&gt;   &lt;span class="m"&gt;1.1001&lt;/span&gt;  &lt;span class="m"&gt;0.3330&lt;/span&gt;    &lt;span class="m"&gt;-7.3411&lt;/span&gt;   &lt;span class="m"&gt;16.9764&lt;/span&gt;    
&lt;span class="n"&gt;settingsingle_center&lt;/span&gt;                  &lt;span class="m"&gt;-0.0597&lt;/span&gt;   &lt;span class="m"&gt;1.0959&lt;/span&gt;  &lt;span class="m"&gt;-0.0544&lt;/span&gt;  &lt;span class="m"&gt;0.9592&lt;/span&gt;    &lt;span class="m"&gt;-3.1023&lt;/span&gt;    &lt;span class="m"&gt;2.9830&lt;/span&gt;    
&lt;span class="n"&gt;locationPoland&lt;/span&gt;                        &lt;span class="m"&gt;-0.7740&lt;/span&gt;   &lt;span class="m"&gt;1.7608&lt;/span&gt;  &lt;span class="m"&gt;-0.4396&lt;/span&gt;  &lt;span class="m"&gt;0.6829&lt;/span&gt;    &lt;span class="m"&gt;-5.6628&lt;/span&gt;    &lt;span class="m"&gt;4.1147&lt;/span&gt;    
&lt;span class="n"&gt;locationUSA&lt;/span&gt;                            &lt;span class="m"&gt;1.5444&lt;/span&gt;   &lt;span class="m"&gt;2.4229&lt;/span&gt;   &lt;span class="m"&gt;0.6374&lt;/span&gt;  &lt;span class="m"&gt;0.5585&lt;/span&gt;    &lt;span class="m"&gt;-5.1826&lt;/span&gt;    &lt;span class="m"&gt;8.2714&lt;/span&gt;    
&lt;span class="n"&gt;age&lt;/span&gt;                                    &lt;span class="m"&gt;0.1119&lt;/span&gt;   &lt;span class="m"&gt;0.1032&lt;/span&gt;   &lt;span class="m"&gt;1.0844&lt;/span&gt;  &lt;span class="m"&gt;0.3392&lt;/span&gt;    &lt;span class="m"&gt;-0.1747&lt;/span&gt;    &lt;span class="m"&gt;0.3985&lt;/span&gt;    
&lt;span class="n"&gt;males_p&lt;/span&gt;                               &lt;span class="m"&gt;-5.7066&lt;/span&gt;   &lt;span class="m"&gt;8.9694&lt;/span&gt;  &lt;span class="m"&gt;-0.6362&lt;/span&gt;  &lt;span class="m"&gt;0.5592&lt;/span&gt;   &lt;span class="m"&gt;-30.6097&lt;/span&gt;   &lt;span class="m"&gt;19.1964&lt;/span&gt;    
&lt;span class="n"&gt;aki_criteriaKDIGO&lt;/span&gt;                     &lt;span class="m"&gt;-3.4725&lt;/span&gt;   &lt;span class="m"&gt;2.6281&lt;/span&gt;  &lt;span class="m"&gt;-1.3213&lt;/span&gt;  &lt;span class="m"&gt;0.2569&lt;/span&gt;   &lt;span class="m"&gt;-10.7691&lt;/span&gt;    &lt;span class="m"&gt;3.8242&lt;/span&gt;    
&lt;span class="n"&gt;aki_criteriaKDIGO&lt;/span&gt; &lt;span class="n"&gt;Expanded&lt;/span&gt; &lt;span class="n"&gt;Criteria&lt;/span&gt;   &lt;span class="m"&gt;-5.6275&lt;/span&gt;   &lt;span class="m"&gt;4.2522&lt;/span&gt;  &lt;span class="m"&gt;-1.3235&lt;/span&gt;  &lt;span class="m"&gt;0.2563&lt;/span&gt;   &lt;span class="m"&gt;-17.4334&lt;/span&gt;    &lt;span class="m"&gt;6.1783&lt;/span&gt;    
&lt;span class="n"&gt;aki_criteriaSerum&lt;/span&gt; &lt;span class="nf"&gt;creatinine &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Scr&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;    &lt;span class="m"&gt;-1.5829&lt;/span&gt;   &lt;span class="m"&gt;3.9282&lt;/span&gt;  &lt;span class="m"&gt;-0.4030&lt;/span&gt;  &lt;span class="m"&gt;0.7076&lt;/span&gt;   &lt;span class="m"&gt;-12.4894&lt;/span&gt;    &lt;span class="m"&gt;9.3236&lt;/span&gt;    
&lt;span class="n"&gt;hypertension_p&lt;/span&gt;                       &lt;span class="m"&gt;-12.7624&lt;/span&gt;  &lt;span class="m"&gt;10.2703&lt;/span&gt;  &lt;span class="m"&gt;-1.2426&lt;/span&gt;  &lt;span class="m"&gt;0.2819&lt;/span&gt;   &lt;span class="m"&gt;-41.2774&lt;/span&gt;   &lt;span class="m"&gt;15.7526&lt;/span&gt;    
&lt;span class="n"&gt;all_cardio_p&lt;/span&gt;                         &lt;span class="m"&gt;-12.5110&lt;/span&gt;  &lt;span class="m"&gt;25.5305&lt;/span&gt;  &lt;span class="m"&gt;-0.4900&lt;/span&gt;  &lt;span class="m"&gt;0.6498&lt;/span&gt;   &lt;span class="m"&gt;-83.3950&lt;/span&gt;   &lt;span class="m"&gt;58.3729&lt;/span&gt;    
&lt;span class="n"&gt;diabetes_p&lt;/span&gt;                            &lt;span class="m"&gt;29.8351&lt;/span&gt;  &lt;span class="m"&gt;25.7688&lt;/span&gt;   &lt;span class="m"&gt;1.1578&lt;/span&gt;  &lt;span class="m"&gt;0.3114&lt;/span&gt;   &lt;span class="m"&gt;-41.7105&lt;/span&gt;  &lt;span class="m"&gt;101.3807&lt;/span&gt;    
&lt;span class="n"&gt;copd_p&lt;/span&gt;                                 &lt;span class="m"&gt;7.8958&lt;/span&gt;  &lt;span class="m"&gt;38.3726&lt;/span&gt;   &lt;span class="m"&gt;0.2058&lt;/span&gt;  &lt;span class="m"&gt;0.8470&lt;/span&gt;   &lt;span class="m"&gt;-98.6438&lt;/span&gt;  &lt;span class="m"&gt;114.4353&lt;/span&gt;    
&lt;span class="n"&gt;ckd_p&lt;/span&gt;                                 &lt;span class="m"&gt;77.4441&lt;/span&gt;  &lt;span class="m"&gt;78.1549&lt;/span&gt;   &lt;span class="m"&gt;0.9909&lt;/span&gt;  &lt;span class="m"&gt;0.3778&lt;/span&gt;  &lt;span class="m"&gt;-139.5486&lt;/span&gt;  &lt;span class="m"&gt;294.4367&lt;/span&gt;    
&lt;span class="n"&gt;cancer_p&lt;/span&gt;                              &lt;span class="m"&gt;-5.5262&lt;/span&gt;  &lt;span class="m"&gt;20.2780&lt;/span&gt;  &lt;span class="m"&gt;-0.2725&lt;/span&gt;  &lt;span class="m"&gt;0.7987&lt;/span&gt;   &lt;span class="m"&gt;-61.8269&lt;/span&gt;   &lt;span class="m"&gt;50.7746&lt;/span&gt;    

&lt;span class="o"&gt;---&lt;/span&gt;
&lt;span class="n"&gt;Signif.&lt;/span&gt; &lt;span class="n"&gt;codes&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;  &lt;span class="m"&gt;0&lt;/span&gt; â&lt;span class="o"&gt;***&lt;/span&gt;â &lt;span class="m"&gt;0.001&lt;/span&gt; â&lt;span class="o"&gt;**&lt;/span&gt;â &lt;span class="m"&gt;0.01&lt;/span&gt; â&lt;span class="o"&gt;*&lt;/span&gt;â &lt;span class="m"&gt;0.05&lt;/span&gt; â&lt;span class="n"&gt;.&lt;/span&gt;â &lt;span class="m"&gt;0.1&lt;/span&gt; â â &lt;span class="m"&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;estimate&lt;/code&gt; column is the magnitude of the effect of the variable upon the dependent variable (&lt;span class="caps"&gt;AKI&lt;/span&gt; incidence among &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 patients). The bigger this number, the higher is the influence. &lt;strong&gt;Positive&lt;/strong&gt; numbers mean &lt;strong&gt;higher risk&lt;/strong&gt;. &lt;strong&gt;Negative&lt;/strong&gt; numbers mean &lt;strong&gt;less risk&lt;/strong&gt;. However, we must check the &lt;code&gt;tval&lt;/code&gt; and its corresponding &lt;code&gt;pval&lt;/code&gt; (p-value) columns. The &lt;code&gt;tval&lt;/code&gt; test assumes a null hypothesis that &lt;code&gt;estimate&lt;/code&gt; = 0. Therefore if &lt;code&gt;pval&lt;/code&gt; is &lt;strong&gt;less than&lt;/strong&gt; a pre-specified level of confidence (say, 5%), we can assume the &lt;code&gt;estimate&lt;/code&gt; is significantly different than zero. Thus, we would assume that the variable would have influence over the&amp;nbsp;outcome.&lt;/p&gt;
&lt;p&gt;However, as you can see in the output, no variables were associated with &lt;span class="caps"&gt;AKI&lt;/span&gt; incidence with statistical&amp;nbsp;significance.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;With this I conclude this demonstration. To summarize&amp;nbsp;I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demonstrated how to import data into a R&amp;nbsp;session;&lt;/li&gt;
&lt;li&gt;Introduced the &lt;code&gt;meta&lt;/code&gt; package, a widely-used R package to meta-analysis&amp;nbsp;calculation;&lt;/li&gt;
&lt;li&gt;Demonstrated the rationale, execution and interpretation of&amp;nbsp;meta-regression.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Check my published paper to see the results from a complete analysis of the systematically-reviwed data &lt;a href="https://www.bmj.com/content/369/bmj.m1985"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;&lt;span class="caps"&gt;RSS&lt;/span&gt; feed&lt;/a&gt;, &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; or &lt;a href="https://t.me/joinchat/AAAAAEYrNCLK80Fh1w8nAg"&gt;Telegram channel&lt;/a&gt; to keep you updated whenever I post new&amp;nbsp;content.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.bmj.com/content/369/bmj.m1985"&gt;Features of 20â133 &lt;span class="caps"&gt;UK&lt;/span&gt; patients in hospital with covid-19 using the &lt;span class="caps"&gt;ISARIC&lt;/span&gt; &lt;span class="caps"&gt;WHO&lt;/span&gt; Clinical Characterisation Protocol: prospective observational cohort&amp;nbsp;study&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://jamanetwork.com/journals/jamainternalmedicine/fullarticle/2763184"&gt;Acute Respiratory Distress Syndrome and Death in Patients With &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 in Wuhan,&amp;nbsp;China&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://linkinghub.elsevier.com/retrieve/pii/S0140673620311892"&gt;Epidemiology, clinical course, and outcomes of critically ill adults with &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 in New York City: a prospective cohort&amp;nbsp;study&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://linkinghub.elsevier.com/retrieve/pii/S0163445320301705"&gt;Clinical characteristics of coronavirus disease 2019 (&lt;span class="caps"&gt;COVID&lt;/span&gt;-19) in China: A systematic review and&amp;nbsp;meta-analysis&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://jcp.bmj.com/content/early/2020/10/06/jclinpath-2020-207023.long"&gt;High burden of acute kidney injury in &lt;span class="caps"&gt;COVID&lt;/span&gt;-19 pandemic: systematic review and&amp;nbsp;meta-analysis&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.791"&gt;On tests of the overall treatment effect in metaâanalysis with normally distributed&amp;nbsp;responses&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1009"&gt;A refined method for the metaâanalysis of controlled clinical trials with binary&amp;nbsp;outcome&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/jennybc/here_here"&gt;Ode to the here&amp;nbsp;package&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.rdocumentation.org/packages/meta/versions/4.9-6/topics/metaprop"&gt;metaprop function | R&amp;nbsp;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.rdocumentation.org/packages/meta/versions/4.9-6"&gt;meta package | R&amp;nbsp;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://stat.ethz.ch/R-manual/R-devel/library/grDevices/html/png.html"&gt;R: &lt;span class="caps"&gt;BMP&lt;/span&gt;, &lt;span class="caps"&gt;JPEG&lt;/span&gt;, &lt;span class="caps"&gt;PNG&lt;/span&gt; and &lt;span class="caps"&gt;TIFF&lt;/span&gt; graphics&amp;nbsp;devices&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Tue, 13 Oct 2020 14:00:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2020-10-13:/meta-analysis-and-meta-regression-with-r.html</guid><category>R</category><category>meta-analysis</category><category>statistical analysis</category><category>COVID-19</category><category>SARS-CoV-2</category><category>acute kidney injury</category></item><item><title>Working with Cancer Genomics Cloud datasets in a PostgreSQL database (PartÂ 1)</title><link>https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently I have been looking for publicly-available genomics datasets to test machine learning models in Python. During my searches for such a &amp;#8220;toy dataset&amp;#8221;, I came upon the &lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics Cloud (&lt;span class="caps"&gt;CGC&lt;/span&gt;)&lt;/a&gt;&amp;nbsp;initiative.&lt;/p&gt;
&lt;p&gt;Anyone can register in &lt;span class="caps"&gt;CGC&lt;/span&gt; and have access to open access massive public datasets, like &lt;a href="http://cancergenome.nih.gov/"&gt;The Cancer Genomics Atlas (&lt;span class="caps"&gt;TCGA&lt;/span&gt;)&lt;/a&gt;. Most individual-level genomic data can only be accessed following approval of a Data Access Request through the &lt;a href="https://www.ncbi.nlm.nih.gov/gap/"&gt;Database of Genotypes and Phenotypes (dbGaP)&lt;/a&gt;. For now, I guess the open data tier will suffice for this&amp;nbsp;exercise.&lt;/p&gt;
&lt;p&gt;This demonstration will be separated into two parts. Here in the first part I will provide a brief run-down of how I queried the &lt;span class="caps"&gt;CGC&lt;/span&gt; to obtain genomic data from cancer patients and the first steps into preparing a local PostgreSQL relational database in my&amp;nbsp;computer.&lt;/p&gt;
&lt;h2&gt;Querying the &lt;span class="caps"&gt;CGC&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;I registered at &lt;span class="caps"&gt;CGC&lt;/span&gt;, then I created a project in the &lt;span class="caps"&gt;CGC&lt;/span&gt; dashboard and went into the data browser&amp;nbsp;tool:&lt;/p&gt;
&lt;p&gt;&lt;img alt="CGC dashboard options" src="https://antoniocampos13.github.io/images/cgc_1.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Then, I chose the &lt;span class="caps"&gt;TCGA&lt;/span&gt; GRCh38 dataset and clicked on the &lt;code&gt;Explore selected&lt;/code&gt; button.&lt;/p&gt;
&lt;p&gt;&lt;img alt="TCGA dataset" src="https://antoniocampos13.github.io/images/cgc_2.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Inside the data browser, I see that there are several information&amp;nbsp;entities:&lt;/p&gt;
&lt;p&gt;&lt;img alt="TCGA information entities" src="https://antoniocampos13.github.io/images/cgc_3.PNG"&gt;&lt;/p&gt;
&lt;p&gt;I clicked on the first one, &lt;code&gt;Cases&lt;/code&gt; and then created a query with the following entities and&amp;nbsp;filters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Entity&lt;/em&gt;&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Filters&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Case&lt;ul&gt;
&lt;li&gt;Primary site: Prostate&amp;nbsp;Gland&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Diagnosis&lt;ul&gt;
&lt;li&gt;Age at&amp;nbsp;diagnosis&lt;/li&gt;
&lt;li&gt;Clinical T (&lt;span class="caps"&gt;TNM&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Demographic&lt;ul&gt;
&lt;li&gt;Ethnicity&lt;/li&gt;
&lt;li&gt;Race&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Follow up&lt;ul&gt;
&lt;li&gt;Primary therapy&amp;nbsp;outcome&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;File&lt;ul&gt;
&lt;li&gt;Access level:&amp;nbsp;Open&lt;/li&gt;
&lt;li&gt;Data type: Gene Level Copy Number, Gene Expression Quantification, Gene Level Copy Number&amp;nbsp;Scores&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final query ended up like&amp;nbsp;this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Prostate cancer query" src="https://antoniocampos13.github.io/images/cgc_4.PNG"&gt;&lt;/p&gt;
&lt;p&gt;In other words, the query resulted in individuals diagnosed with prostate cancer (n=237), their age at diagnosis, their demographic characteristics, their therapeutic outcomes, and their genomic data (n=1,065 files overall: 276 with raw counts of gene expression quantification, 552 with &lt;a href="https://rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/"&gt;&lt;span class="caps"&gt;FPKM&lt;/span&gt;&lt;/a&gt; information, 236 from &lt;a href="https://www.nature.com/scitable/topicpage/copy-number-variation-445/"&gt;copy number variation&lt;/a&gt; genotyping, and a single file containing what I believe is a prostate cancer diagnosis score stratified by&amp;nbsp;gene).&lt;/p&gt;
&lt;p&gt;Then, I clicked on the &lt;code&gt;Copy files to project&lt;/code&gt; and on the &lt;code&gt;Export&lt;/code&gt; button and chose &lt;code&gt;Export as TSV&lt;/code&gt; option. I went back to my project dashboard, clicked on the &lt;code&gt;Files&lt;/code&gt; tab and downloaded&amp;nbsp;everything.&lt;/p&gt;
&lt;p&gt;I realized that the four &lt;code&gt;TSV&lt;/code&gt; and the genomic data could be organized as tables on a &lt;a href="https://en.wikipedia.org/wiki/Relational_database"&gt;relational database&lt;/a&gt;. So I used my &lt;a href="https://www.postgresql.org/"&gt;PostgreSQL server&lt;/a&gt; that I have installed on computer. For this demonstration, I will use my Windows 10 &lt;span class="caps"&gt;OS&lt;/span&gt;, but PostgreSQL can be installed on Unix systems as well. In my portfolio I provide a Windows script and a Unix script as well containing the steps I followed to load all the data into a PostgreSQL&amp;nbsp;database.&lt;/p&gt;
&lt;h2&gt;Creating the &amp;#8216;tcga&amp;#8217; database into the local PostgreSQL&amp;nbsp;server&lt;/h2&gt;
&lt;p&gt;The official PostgreSQL installation instructions are &lt;a href="https://www.postgresql.org/download/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I created a folder named &lt;code&gt;TCGA&lt;/code&gt; for this project, and put the downloaded files inside a &lt;code&gt;data&lt;/code&gt; subfolder. Here is a representation of my directory&amp;nbsp;structure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
âââ TCGA
    âââ data
    â   âââ counts
    â   â   âââ &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;200&lt;/span&gt;+ *.counts.gz files&lt;span class="o"&gt;]&lt;/span&gt;
    â   âââ focal_score_by_genes
    â   âââ fpkm
    â   âââ gene_level_copy_numbers
    â   âââ cases.tsv
    â   âââ demographic.tsv
    â   âââ files.tsv
    â   âââ follow_up.tsv
    âââ main_tcga.ps1
    âââ main_tcga.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;TSV&lt;/code&gt; files are the query results and inside the folders are the files containing the genomic data (these are not their original names &amp;#8212; I renamed them to make easier to identify the contents of each one). For now, I will use just the four &lt;code&gt;TSV&lt;/code&gt; files and the &lt;code&gt;counts&lt;/code&gt; folders.&lt;/p&gt;
&lt;p&gt;This structure is replicated in the folder corresponding to this post in my &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/SQL/2020_10_12_Working_Data_CGC_PostgreSQL/TCGA"&gt;portfolio&lt;/a&gt;. The &lt;code&gt;main_tcga.ps1&lt;/code&gt; and &lt;code&gt;main_tcga.sh&lt;/code&gt; files contain the commands I used for this demonstration. The first is for Windows and the second for Unix&amp;nbsp;systems.&lt;/p&gt;
&lt;p&gt;Then, in the &lt;code&gt;TCGA&lt;/code&gt; folder I opened a Windows PowerShell terminal and using &lt;code&gt;psql&lt;/code&gt;, a terminal-based front-end to PostgreSQL, created a database named &lt;code&gt;tcga&lt;/code&gt; on my local&amp;nbsp;server:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;CREATE DATABASE tcga ENCODING &amp;#39;UTF-8&amp;#39; LC_COLLATE &amp;#39;English_United States&amp;#39; LC_CTYPE &amp;#39;English_United States&amp;#39; TEMPLATE template0&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;-U&lt;/code&gt; flag serves to indicate which user will connect to the local PostgreSQL server. &lt;code&gt;postgres&lt;/code&gt; is the default user created during PostgreSQL installation. The &lt;code&gt;-c&lt;/code&gt; flag means that we are sending a command to the server. Note that the command is inside double quotes and strings into the command are single-quoted.In summary, this command serves to connect the &lt;code&gt;postgres&lt;/code&gt; user into the server and pass a command to create the &lt;code&gt;tcga&lt;/code&gt; database with certain characteristics: use &lt;span class="caps"&gt;UTF&lt;/span&gt;-8 codification, with English locale using the &lt;code&gt;template0&lt;/code&gt; database as template, which is created by default during PostgreSQL server&amp;nbsp;installation.&lt;/p&gt;
&lt;p&gt;If during installation you provided a password to access the server, the terminal will ask for it after you press&amp;nbsp;Enter.&lt;/p&gt;
&lt;h2&gt;Creating tables in the &amp;#8216;tcga&amp;#8217;&amp;nbsp;database&lt;/h2&gt;
&lt;p&gt;Then, I created four tables, corresponding to each &lt;code&gt;TSV&lt;/code&gt; files with the following&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-a&lt;/span&gt; &lt;span class="o"&gt;-f&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;src/tcga_create_tables.sql&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The new friends here are &lt;code&gt;-d&lt;/code&gt; and &lt;code&gt;-a -f&lt;/code&gt;. &lt;code&gt;-d&lt;/code&gt; is the flag that indicates the &lt;em&gt;database&lt;/em&gt; I wished to connect; it is the &lt;code&gt;tcga&lt;/code&gt; I created above. The &lt;code&gt;-a&lt;/code&gt; serves to echo all information from the command to the terminal output so it is possible to check if the commands worked. The &lt;code&gt;-f&lt;/code&gt; flag mean &lt;em&gt;file&lt;/em&gt;: I am indicating that I want to pass the commands within the &lt;code&gt;tcga_create_tables.sql&lt;/code&gt; file inside the &lt;code&gt;src&lt;/code&gt; directory &amp;#8212; which I created as a subfolder of the &lt;code&gt;TCGA&lt;/code&gt; folder. If you are wondering how I created this file: wrote the commands in a text file and simply saved it with the &lt;code&gt;.sql&lt;/code&gt; extension.&lt;/p&gt;
&lt;p&gt;Below is one of the commands inside the &lt;code&gt;.sql&lt;/code&gt; file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;CREATE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;TABLE&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;allcases&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;case_id&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;case_primarysite&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;diagnosis&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;diagnosis_ageatdiagnosis_1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;INT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="o"&gt;#&lt;/span&gt;&lt;span class="n"&gt;diagnosis_clinicalt_1&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;TEXT&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="p"&gt;);&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The command above creates the table &lt;code&gt;allcases&lt;/code&gt; with five columns: &lt;code&gt;case_id&lt;/code&gt;, &lt;code&gt;case_primarysite&lt;/code&gt;, &lt;code&gt;diagnosis&lt;/code&gt;, &lt;code&gt;diagnosis_ageatdiagnosis_1&lt;/code&gt;, and &lt;code&gt;diagnosis_clinicalt_1&lt;/code&gt;. Notice the words beside each one: they indicate the &lt;strong&gt;data type&lt;/strong&gt; of the data that the column will hold. In this case I have four columns that will get text data (&lt;code&gt;TEXT&lt;/code&gt;) and one that will get numbers &amp;#8212; integers (&lt;code&gt;INT&lt;/code&gt;)&amp;nbsp;specifically.&lt;/p&gt;
&lt;p&gt;Note the semicolon &lt;code&gt;;&lt;/code&gt; at the end &amp;#8212; it is a PostgreSQL requirement. It indicates the end of a command (however, if we are passing arguments through the &lt;code&gt;-c&lt;/code&gt; flag the semicolon is not needed though, it is implicit within the&amp;nbsp;flag).&lt;/p&gt;
&lt;p&gt;The file have three more commands similar to the one above. The output of the second to last command should be &lt;code&gt;CREATE TABLE&lt;/code&gt; messages, meaning all went well &amp;#8212; I created tour tables inside the &lt;code&gt;tcga&lt;/code&gt; database.&lt;/p&gt;
&lt;h2&gt;Populating the&amp;nbsp;tables&lt;/h2&gt;
&lt;p&gt;However, they are still empty. To populate the tables, I used the four commands below, one for each table (&lt;code&gt;allcases&lt;/code&gt;, &lt;code&gt;demographic&lt;/code&gt;, &lt;code&gt;follow_up&lt;/code&gt; and &lt;code&gt;allfiles&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;\COPY allcases FROM &amp;#39;data/cases.tsv&amp;#39; DELIMITER E&amp;#39;\t&amp;#39; CSV HEADER&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;\COPY demographic FROM &amp;#39;data/demographic.tsv&amp;#39; DELIMITER E&amp;#39;\t&amp;#39; CSV HEADER&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;\COPY follow_up FROM &amp;#39;data/follow_up.tsv&amp;#39; DELIMITER E&amp;#39;\t&amp;#39; CSV HEADER&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;\COPY allfiles FROM &amp;#39;data/files.tsv&amp;#39; DELIMITER E&amp;#39;\t&amp;#39; CSV HEADER&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;(It is good practice to separate table-creating commands of table-populating ones). In summary, the commands tell the PostgreSQL server to copy the information contained in the &lt;code&gt;TSV&lt;/code&gt; files inside the &lt;code&gt;data&lt;/code&gt; directory into the specified&amp;nbsp;table.&lt;/p&gt;
&lt;p&gt;The argument &lt;code&gt;DELIMITER E'\t'&lt;/code&gt; means that the columns are tab-separated (delimited). This argument would be &lt;code&gt;DELIMITER ','&lt;/code&gt; if the file were comma-separated or omitted&amp;nbsp;altogether.  &lt;/p&gt;
&lt;p&gt;The &lt;code&gt;CSV&lt;/code&gt; indicates that we are importing a delimiter-separated file. &lt;code&gt;HEADER&lt;/code&gt; means that the copied file have a header &amp;#8212; the first line have the column titles, which &lt;strong&gt;must be equal&lt;/strong&gt; to the ones specified during table creation; an error will occur otherwise. This argument must be omitted if the file does not have a&amp;nbsp;header.&lt;/p&gt;
&lt;p&gt;The output &lt;code&gt;COPY&lt;/code&gt; followed by an integer (representing the number of rows copied) means that everything went well. Be careful: do not run the copy commands more than once, otherwise data duplication will&amp;nbsp;occur.&lt;/p&gt;
&lt;p&gt;With this I conclude the first part of this demonstration. In the next part I will use I will use a customized Python to help with the import of genomic data into the PostgreSQL&amp;nbsp;database.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-2.html"&gt;Go to Part 2&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion of Part&amp;nbsp;1&lt;/h2&gt;
&lt;p&gt;In this part&amp;nbsp;I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demonstrated how to query open access data in &lt;span class="caps"&gt;CGC&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;Showed basic commands for importing data into tables created in a local PostgreSQL&amp;nbsp;database.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics&amp;nbsp;Cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://cancergenome.nih.gov/"&gt;The Cancer Genome Atlas&amp;nbsp;Program&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/gap/"&gt;Home - dbGaP - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://rna-seqblog.com/rpkm-fpkm-and-tpm-clearly-explained/"&gt;&lt;span class="caps"&gt;RPKM&lt;/span&gt;, &lt;span class="caps"&gt;FPKM&lt;/span&gt; and &lt;span class="caps"&gt;TPM&lt;/span&gt;, clearly&amp;nbsp;explained&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.nature.com/scitable/topicpage/copy-number-variation-445/"&gt;Copy Number Variation | Scitable by Nature&amp;nbsp;Education&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Relational_database"&gt;Relational database -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.postgresql.org/"&gt;PostgreSQL: The world&amp;#8217;s most advanced open source&amp;nbsp;database&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.postgresql.org/download/"&gt;PostgreSQL:&amp;nbsp;Downloads&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Mon, 12 Oct 2020 12:42:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2020-10-12:/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html</guid><category>SQL</category><category>Bioinformatics</category><category>gene expression quantification</category><category>copy number variation</category><category>Windows</category></item><item><title>Working with Cancer Genomics Cloud datasets in a PostgreSQL database (PartÂ 2)</title><link>https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-2.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Recently I have been looking for publicly-available genomics datasets to test machine learning models in Python. During my searches for such a &amp;#8220;toy dataset&amp;#8221;, I came upon the &lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics Cloud (&lt;span class="caps"&gt;CGC&lt;/span&gt;)&lt;/a&gt;&amp;nbsp;initiative.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Anyone can register in &lt;span class="caps"&gt;CGC&lt;/span&gt; and have access to open access massive public datasets, like &lt;a href="http://cancergenome.nih.gov/"&gt;The Cancer Genomics Atlas (&lt;span class="caps"&gt;TCGA&lt;/span&gt;)&lt;/a&gt;. Most individual-level genomic data can only be accessed following approval of a Data Access Request through the &lt;a href="https://www.ncbi.nlm.nih.gov/gap/"&gt;Database of Genotypes and Phenotypes (dbGaP)&lt;/a&gt;. For now, I guess the open data tier will suffice for this&amp;nbsp;exercise.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This demonstration will be separated into two parts. In the &lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html"&gt;first part&lt;/a&gt; I provided a brief run-down of how I queried the &lt;span class="caps"&gt;CGC&lt;/span&gt; to obtain genomic data from cancer patients and the first steps into preparing a local PostgreSQL relational database in my&amp;nbsp;computer.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here in the second part I will use a customized Python to help with the import of genomic data into the PostgreSQL&amp;nbsp;database.&lt;/p&gt;
&lt;h2&gt;Why use Python to import the genomic data into the PostgreSQL&amp;nbsp;database&lt;/h2&gt;
&lt;p&gt;In the first part of this demonstration I mentioned that I got more than 200 files containing the raw counts of gene expression in the prostate cancer individuals, each corresponding to a individual with prostate gland cancer. Unfortunately, the counts files do not have the patient identification. This information is only available in the &lt;code&gt;files.tsv&lt;/code&gt; (and in my &lt;code&gt;allfiles&lt;/code&gt; table in the database consequently), which indicates which count file belongs to each patient. Therefore, I must include the count file name alongside the gene&amp;nbsp;counts.&lt;/p&gt;
&lt;p&gt;Below I have an illustration of the problem. I have two files, count_A and&amp;nbsp;count_B:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# count_A&lt;/span&gt;
ENSG00000000003.13 &lt;span class="m"&gt;4000&lt;/span&gt;
ENSG00000000005.5 &lt;span class="m"&gt;5&lt;/span&gt;
ENSG00000000419.11 &lt;span class="m"&gt;1800&lt;/span&gt;

&lt;span class="c1"&gt;# count_B&lt;/span&gt;
ENSG00000000003.13 &lt;span class="m"&gt;3000&lt;/span&gt;
ENSG00000000005.5 &lt;span class="m"&gt;25&lt;/span&gt;
ENSG00000000419.11 &lt;span class="m"&gt;500&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this state, I cannot know which patients provided the samples that generate count_A and count_B. But if I add a new column with the&amp;nbsp;filename:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# count_A&lt;/span&gt;
ENSG00000000003.13 &lt;span class="m"&gt;4000&lt;/span&gt;    count_A
ENSG00000000005.5 &lt;span class="m"&gt;5&lt;/span&gt;   count_A
ENSG00000000419.11 &lt;span class="m"&gt;1800&lt;/span&gt;    count_A

&lt;span class="c1"&gt;# count_B&lt;/span&gt;
ENSG00000000003.13 &lt;span class="m"&gt;3000&lt;/span&gt;    count_B
ENSG00000000005.5 &lt;span class="m"&gt;25&lt;/span&gt;  count_B
ENSG00000000419.11 &lt;span class="m"&gt;500&lt;/span&gt; count_B
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I can now cross-reference with the &lt;code&gt;allfiles&lt;/code&gt; table, and identify which file belong to each&amp;nbsp;patient:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;case_id file_name
case0001 count_A
case0002 count_B
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Thus, I created a &lt;strong&gt;relation&lt;/strong&gt; between the gene expression quantification and their patients of origin. Keep in mind that the gene counts file have &lt;strong&gt;thousands&lt;/strong&gt; of rows, each corresponding to one human gene/alternate transcript. Therefore, I&amp;nbsp;must:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Automate the creation of the third column containing the file name in all 200+ gene count&amp;nbsp;files;&lt;/li&gt;
&lt;li&gt;Join the modified files into a single, unified data&amp;nbsp;frame;&lt;/li&gt;
&lt;li&gt;Import the data frame into the &lt;code&gt;tcga&lt;/code&gt; database.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With only programming language &amp;#8212; Python &amp;#8212; I can do all three requirements above. So that&amp;#8217;s why I used Python: it is a very powerful, versatile&amp;nbsp;language!&lt;/p&gt;
&lt;h2&gt;Create Python virtual&amp;nbsp;environment&lt;/h2&gt;
&lt;p&gt;Follow instructions to install Python in Windows &lt;a href="https://www.python.org/downloads/"&gt;here&lt;/a&gt;. Ensure that Python &lt;a href="https://datatofish.com/add-python-to-windows-path/"&gt;is included in your Windows &lt;span class="caps"&gt;PATH&lt;/span&gt;&lt;/a&gt;. Python usually comes pre-installed in several Unix distros and already included in the &lt;span class="caps"&gt;PATH&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;First, I will create a virtual environment to hold the necessary Python modules for my customized Python script. This is good practice &amp;#8212; as I explained in my &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;previous post&lt;/a&gt; different environments isolate programs for different uses, ensuring compatibility. In the post I talked about miniconda, but the principle is the same for Python&amp;nbsp;here.&lt;/p&gt;
&lt;p&gt;Otherwise, you can create a miniconda environment with Python included, and install all Python packages via miniconda channels. Since I will not use any other software besides Python here, there is no need to use miniconda, in my opinion. I created a virtual environment using Python&amp;#8217;s &lt;code&gt;virtualenv&lt;/code&gt; tool. Currently, I am using Python version&amp;nbsp;3.8.&lt;/p&gt;
&lt;p&gt;In the &lt;code&gt;TCGA&lt;/code&gt; folder I open a PowerShell and issue the commands&amp;nbsp;below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="n"&gt;-m&lt;/span&gt; &lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="p"&gt;-&lt;/span&gt;&lt;span class="n"&gt;-user&lt;/span&gt; &lt;span class="n"&gt;virtualenv&lt;/span&gt;

&lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="n"&gt;-m&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The first command installs the program &lt;code&gt;virtualenv&lt;/code&gt; (&lt;code&gt;venv&lt;/code&gt;) via the Python package manager &lt;code&gt;pip&lt;/code&gt;. The second command uses &lt;code&gt;venv&lt;/code&gt; to create a virtual environment deposited in a folder named &lt;code&gt;venv&lt;/code&gt; in the current directory (&lt;code&gt;TCGA&lt;/code&gt; folder in this example). You can also provide a complete path like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="n"&gt;-m&lt;/span&gt; &lt;span class="n"&gt;venv&lt;/span&gt; &lt;span class="n"&gt;C&lt;/span&gt;&lt;span class="p"&gt;:\&lt;/span&gt;&lt;span class="n"&gt;Users&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;some_path&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;TCGA&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;venv&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Of course, you can call the virtual environment as you&amp;nbsp;wish.&lt;/p&gt;
&lt;h2&gt;Activate the virtual&amp;nbsp;environment&lt;/h2&gt;
&lt;p&gt;Still in the &lt;code&gt;TCGA&lt;/code&gt; folder, I type the&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;venv&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;Scripts&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;activate&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The virtual environment is ready to be used. I will install the necessary modules for the&amp;nbsp;work.&lt;/p&gt;
&lt;h2&gt;Install Python modules into the virtual&amp;nbsp;environment&lt;/h2&gt;
&lt;p&gt;The modules I will install&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://docs.dask.org/en/latest/why.html"&gt;&lt;code&gt;dask&lt;/code&gt;&lt;/a&gt;: to create the unified data frame with the gene&amp;nbsp;expression;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;psycopg2-binary&lt;/code&gt; and &lt;code&gt;sqlalchemy&lt;/code&gt;: to connect with the PostgreSQL database and push the dataframe into&amp;nbsp;it.&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;pip&lt;/span&gt; &lt;span class="n"&gt;install&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;dask[complete]&amp;quot;&lt;/span&gt; &lt;span class="n"&gt;psycopg2-binary&lt;/span&gt; &lt;span class="n"&gt;sqlalchemy&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The modules will be downloaded from the internet and installed at the &lt;code&gt;venv&lt;/code&gt; folder. Additional dependencies, such as &lt;a href="https://pandas.pydata.org/"&gt;&lt;code&gt;pandas&lt;/code&gt;&lt;/a&gt; (a widely-used data analysis and manipulation tool) and &lt;a href="https://numpy.org/"&gt;&lt;code&gt;NumPy&lt;/code&gt;&lt;/a&gt; (package for scientific computing), used by &lt;code&gt;dask&lt;/code&gt;, will be downloaded as&amp;nbsp;well.&lt;/p&gt;
&lt;h2&gt;Creating Python credentials to access PostgreSQL&amp;nbsp;database&lt;/h2&gt;
&lt;p&gt;To access the &lt;code&gt;tcga&lt;/code&gt; database through Python, we need to configure credentials for the&amp;nbsp;connection.&lt;/p&gt;
&lt;p&gt;In the terminal I&amp;nbsp;type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;CREATE USER &amp;lt;USER_NAME&amp;gt; with encrypted password &amp;#39;&amp;lt;PASSWORD&amp;gt;&amp;#39;&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;GRANT ALL PRIVILEGES ON DATABASE tcga TO &amp;lt;USER_NAME&amp;gt;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;code&gt;&amp;lt;USER_NAME&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;PASSWORD&amp;gt;&lt;/code&gt; are placeholders for my username and password, respectively, since it is good practice to &lt;strong&gt;&lt;span class="caps"&gt;NEVER&lt;/span&gt; share sensitive information&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Then, I created a file named &lt;code&gt;settings.py&lt;/code&gt; and put it in a &lt;code&gt;src&lt;/code&gt; folder with the following&amp;nbsp;content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;DB_FLAVOR&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;postgresql&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;DB_PYTHON_LIBRARY&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;psycopg2&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;USER&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;lt;USER_NAME&amp;gt;&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;PASSWORD&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;lt;PASSWORD&amp;gt;&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;DB_HOST&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;localhost&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;PORT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;5432&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;DB_NAME&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;tcga&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Create one yourself with the user name and password you specified on the previous step. The other parameters can be left as they are. The 5432 port is usually the default port configured during installation to connection to PostgreSQL. Change it if needed, of course. &lt;code&gt;localhost&lt;/code&gt; means that the PostgreSQL is running locally in my&amp;nbsp;computer.&lt;/p&gt;
&lt;p&gt;Then, to keep the organization of my folder, I added my &lt;code&gt;tcga_processing_counts.py&lt;/code&gt; customized script to the &lt;code&gt;src&lt;/code&gt; folder. The folder structure is now like&amp;nbsp;this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;.
âââ TCGA
    âââ data
    â   âââ counts
    â   â   âââ &lt;span class="o"&gt;[&lt;/span&gt;&lt;span class="m"&gt;200&lt;/span&gt;+ *.counts.gz files&lt;span class="o"&gt;]&lt;/span&gt;
    â   âââ focal_score_by_genes
    â   âââ fpkm
    â   âââ gene_level_copy_numbers
    â   âââ cases.tsv
    â   âââ demographic.tsv
    â   âââ files.tsv
    â   âââ follow_up.tsv
    âââ src
    â   âââ settings.py
    â   âââ tcga_processing_counts.py
    âââ main_tcga.ps1
    âââ main_tcga.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Running the&amp;nbsp;script&lt;/h2&gt;
&lt;p&gt;Back in the &lt;code&gt;TCGA&lt;/code&gt; folder, I type in the&amp;nbsp;PowerShell:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="n"&gt;src&lt;/span&gt;&lt;span class="p"&gt;\&lt;/span&gt;&lt;span class="n"&gt;tcga_processing_counts&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will start the script, which has eight&amp;nbsp;steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Set up PostgreSQL connection object: &lt;code&gt;psycopg2&lt;/code&gt; and &lt;code&gt;sqlalchemy&lt;/code&gt; modules use the credential of the &lt;code&gt;settings.py&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Set up project paths: locate the data&amp;nbsp;folders;&lt;/li&gt;
&lt;li&gt;Decompress the &lt;code&gt;.counts.gz&lt;/code&gt; files;&lt;/li&gt;
&lt;li&gt;Make a list of all uncompressed&amp;nbsp;files;&lt;/li&gt;
&lt;li&gt;Create a function ready to return a pandas.DataFrame: this is when I add the third column with the filename in the counts&amp;nbsp;files;&lt;/li&gt;
&lt;li&gt;Create a list of commands to apply the read_and_label_csv function to all&amp;nbsp;files;&lt;/li&gt;
&lt;li&gt;Using &lt;code&gt;dask&lt;/code&gt;&lt;span class="quo"&gt;&amp;#8216;&lt;/span&gt;s &lt;code&gt;delayed&lt;/code&gt; method, assemble the pandas.DataFrames into a &lt;code&gt;dask.DataFrame&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;Send the &lt;code&gt;dask.DataFrame&lt;/code&gt; to the&amp;nbsp;database.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There is an optional step before step 8 to export the &lt;code&gt;dask.DataFrame&lt;/code&gt; as &lt;span class="caps"&gt;HUGE&lt;/span&gt; &lt;span class="caps"&gt;CSV&lt;/span&gt; file that I disabled by default. &lt;strong&gt;&lt;span class="caps"&gt;WARNING&lt;/span&gt;: &lt;span class="caps"&gt;IT&lt;/span&gt; &lt;span class="caps"&gt;USES&lt;/span&gt; A &lt;span class="caps"&gt;LOT&lt;/span&gt; &lt;span class="caps"&gt;OF&lt;/span&gt; &lt;span class="caps"&gt;RAM&lt;/span&gt; &lt;span class="caps"&gt;AND&lt;/span&gt; &lt;span class="caps"&gt;CPU&lt;/span&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The use of &lt;code&gt;dask&lt;/code&gt; for this job is crucial. &lt;code&gt;pandas&lt;/code&gt; works by loading all data into the &lt;span class="caps"&gt;RAM&lt;/span&gt;. However, since there are several files of considerable size, it would overload my available &lt;span class="caps"&gt;RAM&lt;/span&gt;. &lt;code&gt;dask&lt;/code&gt; is suited for larger-than-memory datasets, since it operates by lazy evaluation: it break operations into blocks and specifies task chains and execute them only on demand, saving computing&amp;nbsp;resources.&lt;/p&gt;
&lt;p&gt;Go check the contents of my &lt;a href="https://github.com/antoniocampos13/portfolio/blob/master/SQL/2020_10_12_Working_Data_CGC_PostgreSQL/TCGA/src/tcga_processing_counts.py"&gt;&lt;code&gt;tcga_processing_counts.py&lt;/code&gt; in my portfolio&lt;/a&gt;. By default, it will create a table named &lt;code&gt;gene_counts&lt;/code&gt; in the &lt;code&gt;tcga&lt;/code&gt; database. See an excerpt of the final&amp;nbsp;result:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gene counts table in tcga database" src="https://antoniocampos13.github.io/images/tcga_gene_counts.PNG"&gt;&lt;/p&gt;
&lt;h2&gt;Finishing&amp;nbsp;touches&lt;/h2&gt;
&lt;p&gt;With the gene expression counts dataset imported in the database, it is time to create the filename (gene counts)/patient relation as I explained in the beginning of the post. In the terminal again, I&amp;nbsp;type:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;psql&lt;/span&gt; &lt;span class="n"&gt;-U&lt;/span&gt; &lt;span class="n"&gt;postgres&lt;/span&gt; &lt;span class="n"&gt;-d&lt;/span&gt; &lt;span class="n"&gt;tcga&lt;/span&gt; &lt;span class="n"&gt;-c&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;CREATE TABLE gene_counts_cases AS SELECT DISTINCT case_id, gene_id, gene_count FROM gene_counts LEFT JOIN allfiles ON gene_counts.filename = allfiles.file_uuid WHERE gene_id LIKE &amp;#39;%ENSG%&amp;#39;&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The command above links the two tables by their information in common: the filename of the gene counts, which is named &lt;code&gt;filename&lt;/code&gt; in the gene_counts table and &lt;code&gt;file_uuid&lt;/code&gt; in &lt;code&gt;allfiles&lt;/code&gt; table that we created&amp;nbsp;before.&lt;/p&gt;
&lt;p&gt;See an excerpt of the final&amp;nbsp;result:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gene counts table in tcga database: counts/patient relation" src="https://antoniocampos13.github.io/images/tcga_gene_counts_cases.PNG"&gt;&lt;/p&gt;
&lt;p&gt;With this I conclude the second and last part of this demonstration. There is still missing the outcome information, which is located in the &lt;code&gt;follow_up&lt;/code&gt; table in the database. However, the &lt;code&gt;gene_counts_cases&lt;/code&gt; table is not yet ready to be linked. I need to pivot this table, but PostgreSQL has a limit of 1600 columns. Perhaps if I import this table into a session in &lt;code&gt;R&lt;/code&gt;, it will be possible to transform the table. Additionally, I will perform differential expression analysis for sequence count&amp;nbsp;data.&lt;/p&gt;
&lt;h2&gt;Conclusion of Part&amp;nbsp;2&lt;/h2&gt;
&lt;p&gt;In this part&amp;nbsp;I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Demonstrated how Python can be used to create data frames larger-than-memory with &lt;code&gt;dask&lt;/code&gt; module;&lt;/li&gt;
&lt;li&gt;Demonstrated how to connect Python to PostgreSQL databases with &lt;code&gt;psycopg2&lt;/code&gt; and &lt;code&gt;sqlalchemy&lt;/code&gt; modules;&lt;/li&gt;
&lt;li&gt;Demonstrated simple &lt;code&gt;LEFT JOIN&lt;/code&gt; operation to link gene counts to individual cases of prostate&amp;nbsp;cancer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-1.html"&gt;Go back to Part 1&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="http://www.cancergenomicscloud.org/"&gt;Cancer Genomics&amp;nbsp;Cloud&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://cancergenome.nih.gov/"&gt;The Cancer Genome Atlas&amp;nbsp;Program&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/gap/"&gt;Home - dbGaP - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.python.org/downloads/"&gt;Download&amp;nbsp;Python&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://datatofish.com/add-python-to-windows-path/"&gt;How to add Python to Windows &lt;span class="caps"&gt;PATH&lt;/span&gt; - Data to&amp;nbsp;Fish&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;Setting Up Your Unix Computer for Bioinformatics&amp;nbsp;Analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.dask.org/en/latest/why.html"&gt;Dask  documentation - Why&amp;nbsp;Dask?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pandas.pydata.org/"&gt;pandas - Python Data Analysis&amp;nbsp;Library&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://numpy.org/"&gt;NumPy&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Mon, 12 Oct 2020 12:42:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2020-10-12:/working-with-cancer-genomics-cloud-datasets-in-a-postgresql-database-part-2.html</guid><category>SQL</category><category>Bioinformatics</category><category>gene expression quantification</category><category>copy number variation</category><category>Windows</category></item><item><title>FASTQ to Annotation (PartÂ 4)</title><link>https://antoniocampos13.github.io/fastq-to-annotation-part-4.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;In a &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;previous post&lt;/a&gt;, I showed how to configure an Ubuntu system to install Bioinformatics&amp;nbsp;programs.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Now, using the environment I created, I will demonstrate a bash script, &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; that takes next generation sequencing (&lt;span class="caps"&gt;NGS&lt;/span&gt;) raw reads from human whole genome sequencing as input and produces variant annotation as output. Variant annotation is the process of identifying genetic variants in some genomic &lt;span class="caps"&gt;DNA&lt;/span&gt; sample, and assess, for example, if any of the found variants have any effect on phenotype, such as increased susceptibility to certain&amp;nbsp;diseases.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In the &lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-1"&gt;first part&lt;/a&gt;, I showed how to search for &lt;span class="caps"&gt;NGS&lt;/span&gt; projects deposited in &lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology Information (&lt;span class="caps"&gt;NCBI&lt;/span&gt;) databases&lt;/a&gt; from which I can download sequencing reads later to use with the&amp;nbsp;script.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In the &lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-2"&gt;second part&lt;/a&gt;, I showed how to retrieve raw genome sequencing reads in the form of &lt;code&gt;FASTQ&lt;/code&gt; files, which are deposited in &lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;&lt;span class="caps"&gt;SRA&lt;/span&gt;&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In the &lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-3"&gt;third part&lt;/a&gt;, I made the final preparations for the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script demonstration using the &lt;code&gt;FASTQ&lt;/code&gt; files obtained in the second&amp;nbsp;part.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here in the fourth and final part, I finally can summarize the inner workings of the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script.&lt;/p&gt;
&lt;h2&gt;FastQ_to_Annotation.sh&amp;nbsp;parameters&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Activate&lt;/strong&gt; your miniconda environment if needed and go to your &lt;code&gt;demo&lt;/code&gt; folder. Make sure you have the &lt;code&gt;FASTQ&lt;/code&gt; files and a &lt;code&gt;refs&lt;/code&gt; folder with the human genome &lt;code&gt;FASTA&lt;/code&gt; files and the other various supporting&amp;nbsp;files.&lt;/p&gt;
&lt;p&gt;The script needs 10 command line parameters to work correctly. They&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mate-pair &lt;span class="caps"&gt;FASTQ&lt;/span&gt; files name root (without extension) (absolute file&amp;nbsp;path)&lt;/li&gt;
&lt;li&gt;Reference genome &lt;span class="caps"&gt;FASTA&lt;/span&gt; (absolute file&amp;nbsp;path)&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;BED&lt;/span&gt; or &lt;span class="caps"&gt;GFF&lt;/span&gt; file (absolute file&amp;nbsp;path)&lt;/li&gt;
&lt;li&gt;Minimum quality for bases at read ends, below which bases will be cut (integer - default:&amp;nbsp;20)&lt;/li&gt;
&lt;li&gt;Minimum allowed read length (integer - default:&amp;nbsp;20)&lt;/li&gt;
&lt;li&gt;Adaptor for trimming off read ends (&amp;#8216;illumina&amp;#8217; / &amp;#8216;nextera&amp;#8217; /&amp;nbsp;&amp;#8216;small_rna&amp;#8217;)&lt;/li&gt;
&lt;li&gt;Minimum read depth for calling a variant (integer - default:&amp;nbsp;3)&lt;/li&gt;
&lt;li&gt;Minimum allowed mapping quality (integer - default:&amp;nbsp;0)&lt;/li&gt;
&lt;li&gt;Stringency for calling variants (&amp;#8216;relaxed&amp;#8217; / &amp;#8216;normal&amp;#8217;) (relaxed uses &amp;#8212;pval-threshold 1.0 with BCFtools&amp;nbsp;call)&lt;/li&gt;
&lt;li&gt;User identification for logging&amp;nbsp;(alphanumeric)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For this example, use the following&amp;nbsp;values:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;SRR6784104&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;refs/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;refs/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.bed&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;20&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;20&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;illumina&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;normal&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Your name (do not use&amp;nbsp;spaces)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Since the names of the compressed human genome &lt;code&gt;FASTA&lt;/code&gt; file is big, you can rename it, or create an alias in the command line to simplify the&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nv"&gt;REF&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;refs/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz
&lt;span class="nv"&gt;BED&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;refs/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.bed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, joining everything&amp;nbsp;together:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;./FastQ_to_Annotation.sh SRR6784104 &lt;span class="nv"&gt;$REF&lt;/span&gt; &lt;span class="nv"&gt;$BED&lt;/span&gt; &lt;span class="m"&gt;20&lt;/span&gt; &lt;span class="m"&gt;20&lt;/span&gt; illumina &lt;span class="m"&gt;3&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt; normal antonio
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Pipeline&amp;nbsp;steps&lt;/h2&gt;
&lt;p&gt;The script will check if all parameters are adequate and then run the core pipeline, which proceeds in an 8-step&amp;nbsp;process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Adaptor and read quality trimming: uses &lt;a href="https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/"&gt;Trim Galore!&lt;/a&gt;, &lt;a href="http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"&gt;FastQC&lt;/a&gt; and &lt;a href="https://github.com/marcelm/cutadapt/"&gt;Cutadapt&lt;/a&gt; programs. They remove adaptor sequence from reads and discards low-quality reads so they do not interfere with the second step, alignment. Outputs the trimmed &lt;code&gt;FASTQ&lt;/code&gt; files, text and &lt;code&gt;HTML&lt;/code&gt; reports of the trimming&amp;nbsp;results.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Alignment: uses &lt;code&gt;bwa mem&lt;/code&gt; command (&lt;a href="https://academic.oup.com/bioinformatics/article/25/14/1754/225615"&gt;Li &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Durbin, 2009&lt;/a&gt;). &lt;code&gt;bwa&lt;/code&gt; is a widely-used program to align short reads into genomes, so we can pinpoint where in the genome the identified variants are located. Takes the trimmed &lt;code&gt;FASTQ&lt;/code&gt; files, the reference &lt;code&gt;FASTA&lt;/code&gt; file and produces an aligned &lt;span class="caps"&gt;SAM&lt;/span&gt;&amp;nbsp;file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Marking and removing &lt;span class="caps"&gt;PCR&lt;/span&gt; duplicates: uses Picard (Broad Institute of &lt;span class="caps"&gt;MIT&lt;/span&gt; and Harvard) and SAMtools &lt;a href="https://academic.oup.com/bioinformatics/article/25/16/2078/204688"&gt;(Li et al., 2009)&lt;/a&gt;. This is another cleanup step. It takes the aligned &lt;span class="caps"&gt;SAM&lt;/span&gt; file and produces an aligned sorted &lt;span class="caps"&gt;BAM&lt;/span&gt; file with duplicated reads removed. &lt;a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1097-3"&gt;Ebbert et al.&lt;/a&gt; define &lt;span class="caps"&gt;PCR&lt;/span&gt; duplicates as: &amp;#8220;&amp;#8230;sequence reads that result from sequencing two or more copies of the exact same &lt;span class="caps"&gt;DNA&lt;/span&gt; fragment, which, at worst, may contain erroneous mutations introduced during &lt;span class="caps"&gt;PCR&lt;/span&gt; amplification, or, at the very least, make the occurrence of the allele(s) sequenced in duplicates appear proportionately more often than it should compared to the other allele (assuming a non-haploid&amp;nbsp;organism)&amp;#8221;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remove low mapping quality reads: uses SAMtools (Li et al., 2009). Reads falling in repetitive regions usually get very low mapping quality, so we remove it to reduce noise during variant call. Takes the aligned sorted &lt;span class="caps"&gt;BAM&lt;/span&gt; file with duplicated reads removed and removes low mapping quality&amp;nbsp;reads.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Quality control (&lt;span class="caps"&gt;QC&lt;/span&gt;): uses SAMtools (Li et al., 2009), BEDTools &lt;a href="https://academic.oup.com/bioinformatics/article/26/6/841/244688"&gt;(Quinlan &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Hall, 2010)&lt;/a&gt;. Quantifies the removed off-target reads, the sequencing reads that do not align to the target genome and calculates the mean depth of read coverage in the genome. Takes in the &lt;span class="caps"&gt;BAM&lt;/span&gt; file generated in the previous&amp;nbsp;step.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Downsampling/random read sampling: uses &lt;a href="https://broadinstitute.github.io/picard/"&gt;Picard&lt;/a&gt; (Broad Institute of &lt;span class="caps"&gt;MIT&lt;/span&gt; and Harvard). This step takes the cleaned-up aligned sorted &lt;span class="caps"&gt;BAM&lt;/span&gt; file generated by the previous steps and splits into 3 &amp;#8216;sub-BAMs&amp;#8217; of random reads sorted with probabilities of 75%, 50%, and&amp;nbsp;25%.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Variant calling: uses SAMtools/BCFtools (Li et al., 2009). This step identifies genetic variation present in the sample reads. It takes on all 4 &lt;span class="caps"&gt;BAM&lt;/span&gt; files, after which a consensus &lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3137218/"&gt;Variant Call Format (&lt;span class="caps"&gt;VCF&lt;/span&gt;)&lt;/a&gt; file is&amp;nbsp;produced.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Annotation: uses Variant Effect Predictor &lt;a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0974-4"&gt;(McLaren et al., 2016)&lt;/a&gt;. It takes the list of variants compiled in the consensus &lt;span class="caps"&gt;VCF&lt;/span&gt; file and annotates them, identifying possible phenotypic effects. Outputs text and html summary files with the results. Check &lt;a href="https://www.ensembl.org/info/docs/tools/vep/script/vep_other.html"&gt;&lt;span class="caps"&gt;VEP&lt;/span&gt;&amp;#8217;s documentation&lt;/a&gt; if you want to customize the annotation options in the&amp;nbsp;script.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Output&lt;/h2&gt;
&lt;p&gt;Once the script is running you will see several files being generated. Once the script finishes, the files will be neatly organized in a folder &lt;code&gt;prefix_results&lt;/code&gt;, where &lt;code&gt;prefix&lt;/code&gt; is the name root of the &lt;code&gt;FASTQ&lt;/code&gt; files:&lt;/p&gt;
&lt;p&gt;&lt;img alt="demo results 1" src="https://antoniocampos13.github.io/images/demo_results_1.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Open the folder and check that there are some files and three subfolders. These subfolders hold all intermediate files generated by the script (&lt;code&gt;.sam&lt;/code&gt;, &lt;code&gt;.bam&lt;/code&gt; and many others). &lt;code&gt;trimmed_files&lt;/code&gt; folder hold the trimmed &lt;code&gt;FASTQ&lt;/code&gt; files alongside Trim Galore!&amp;#8217;s reports (step 1). &lt;code&gt;alignment_files&lt;/code&gt; hold intermediate files generated by steps 2 trough 5. &lt;code&gt;variant_call_files&lt;/code&gt; hold intermediate files generated by steps 7 through&amp;nbsp;8.&lt;/p&gt;
&lt;p&gt;&lt;img alt="demo results 2" src="https://antoniocampos13.github.io/images/demo_results_2.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s focus the attention on the other five&amp;nbsp;files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Master_Log.txt and Pipeline_Log.txt files: logs from the script operations. The first one has a copy of all commands issued by the script. The second one is more concise; it summarizes input parameters alongside date and time each step in the scripted started. Check these files if any errors occur to identify what went&amp;nbsp;wrong.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Final.vcf: a &lt;span class="caps"&gt;VCF&lt;/span&gt; file containing all variants identified in the sample. It contains chromosome position of the variants, alleles and other&amp;nbsp;information.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;AnnotationVEP.txt and AnnotationVEP.html: outputs of annotation by Ensembl&amp;#8217;s &lt;span class="caps"&gt;VEP&lt;/span&gt;. The text file is tab-separated file listing the called variants and their characteristics (more on that later). The &lt;code&gt;HTML&lt;/code&gt; file contains a summarized quantification of the variants&amp;nbsp;characteristics.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Open the &lt;code&gt;SRR6784104_AnnotationVEP.txt&lt;/code&gt; file into a spreadsheet to make the visualization easier. You will see there is a header with several definitions/abbreviations for the information contained in the file. Scroll down until you found a table-like&amp;nbsp;part.&lt;/p&gt;
&lt;p&gt;In this table part, there is several important information that is interesting to check. Some of the columns I like to&amp;nbsp;assess:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;#Uploaded_variation&lt;/code&gt;: an identifier of each&amp;nbsp;variation;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Location&lt;/code&gt;: chromosome and position of the&amp;nbsp;variation;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Allele&lt;/code&gt;: particular nucleotide configuration found in determined position in the&amp;nbsp;sample;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Gene&lt;/code&gt;: if the variant is located within a gene, its unique RefSeq gene &lt;span class="caps"&gt;ID&lt;/span&gt; (an integer) will be&amp;nbsp;there;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Feature&lt;/code&gt;: if the variant is located within a gene, a unique RefSeq accession code of the gene sequence will be&amp;nbsp;there;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Consequence&lt;/code&gt;: I found this column weirdly-named, because it reflects more the overall location of the variant than a molecular consequence as the name implies. For example, it will indicate that the variant is a &lt;code&gt;missense_variant&lt;/code&gt;, an &lt;code&gt;intron_variant&lt;/code&gt;, &lt;code&gt;regulatory_region_variant&lt;/code&gt; and so&amp;nbsp;on;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Protein_position&lt;/code&gt;, &lt;code&gt;Amino_acids&lt;/code&gt;, &lt;code&gt;Codons&lt;/code&gt;: if missense or synonym, information about amino acids changes and position on the protein will be in these&amp;nbsp;columns;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Existing_variation&lt;/code&gt;: if variation was already previously identified in other samples, the RefSeq (starting with &lt;code&gt;rs&lt;/code&gt;) or other identifier will be there. RefSeq-identified variants can be found in &lt;a href="https://www.ncbi.nlm.nih.gov/snp/"&gt;&lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;#8217;s dbSNP&lt;/a&gt;;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IMPACT&lt;/code&gt;: the variant&amp;#8217;s impact on phenotype (&lt;span class="caps"&gt;LOW&lt;/span&gt;, &lt;span class="caps"&gt;MODIFIER&lt;/span&gt;, &lt;span class="caps"&gt;HIGH&lt;/span&gt;);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;VARIANT_CLASS&lt;/code&gt;: the class of the variant. &lt;span class="caps"&gt;SNV&lt;/span&gt; (single nucleotide variation, the same as single nucleotied polymorphism &amp;#8212; &lt;span class="caps"&gt;SNP&lt;/span&gt;), insertions and deletions are the most&amp;nbsp;common;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SYMBOL&lt;/code&gt;: the official symbol (abbreviation) of the gene&amp;nbsp;name;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;BIOTYPE&lt;/code&gt;: if the variant is located within a gene, the gene function. For example: protein_coding, lncRNA, miRNA, and so&amp;nbsp;on;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;SIFT&lt;/code&gt; and &lt;code&gt;PolyPhen&lt;/code&gt;: named after the tools that predict whether an amino acid substitution affects protein function and structure of a human&amp;nbsp;protein;&lt;/li&gt;
&lt;li&gt;Columns prefixed with &lt;code&gt;AF&lt;/code&gt;: contain the allelic frequency of a given variant in some &lt;a href="https://www.internationalgenome.org/category/population/"&gt;global populations&lt;/a&gt;. For example, &lt;code&gt;AFR&lt;/code&gt;: African,&lt;code&gt;AMR&lt;/code&gt;: Ad Mixed American, &lt;code&gt;EAS&lt;/code&gt;: East Asian, &lt;code&gt;SAS&lt;/code&gt;: South&amp;nbsp;Asian;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CLIN_SIG&lt;/code&gt;: a short sentence stating the clinical significance (if available) of the&amp;nbsp;variant;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;PUBMED&lt;/code&gt;: a list of PubMed IDs of references citing the variation (if&amp;nbsp;available).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The &lt;code&gt;AnnotationVEP.html&lt;/code&gt; file contains a collection of graphical representations of several characteristics of the detected variants. See below some of them. Notice that your results will be different from these figures, since I used a different set of &lt;code&gt;FASTQ&lt;/code&gt; files and reference&amp;nbsp;files.&lt;/p&gt;
&lt;p&gt;&lt;img alt="demo results 3" src="https://antoniocampos13.github.io/images/demo_results_3.PNG"&gt;&lt;/p&gt;
&lt;h2&gt;Conclusion of Part&amp;nbsp;4&lt;/h2&gt;
&lt;p&gt;In this part I&amp;nbsp;showed:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How to use the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script;&lt;/li&gt;
&lt;li&gt;Summarized the steps performed by the&amp;nbsp;script;&lt;/li&gt;
&lt;li&gt;Summarized the principal results output by the&amp;nbsp;script.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Therefore, I finished all the steps I followed to prepare the system for Bioinformatics analysis, gather the necessary files and apply them to obtain annotations from human genome &lt;span class="caps"&gt;NGS&lt;/span&gt; reads&amp;nbsp;samples.&lt;/p&gt;
&lt;p&gt;Subscribe to my &lt;a href="https://antoniocampos13.github.io/feeds/all.rss.xml"&gt;rss feed&lt;/a&gt; or &lt;a href="https://antoniocampos13.github.io/feeds/all.atom.xml"&gt;Atom feed&lt;/a&gt; to keep updated whenever I post new&amp;nbsp;protocols.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-1"&gt;Go back to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;1)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-2"&gt;Go back to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;2)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-3"&gt;Go back to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;3)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;Setting Up Your Unix Computer for Bioinformatics&amp;nbsp;Analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology&amp;nbsp;Information&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;Home - &lt;span class="caps"&gt;SRA&lt;/span&gt; - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/"&gt;Babraham Bioinformatics - Trim&amp;nbsp;Galore!&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.bioinformatics.babraham.ac.uk/projects/fastqc/"&gt;Babraham Bioinformatics - FastQC A Quality Control tool for High Throughput Sequence&amp;nbsp;Data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/marcelm/cutadapt/"&gt;marcelm/cutadapt&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://academic.oup.com/bioinformatics/article/25/14/1754/225615"&gt;Fast and accurate short read alignment with BurrowsâWheeler&amp;nbsp;transform&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://academic.oup.com/bioinformatics/article/25/16/2078/204688"&gt;Sequence Alignment/Map format and&amp;nbsp;SAMtools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-1097-3"&gt;Evaluating the necessity of &lt;span class="caps"&gt;PCR&lt;/span&gt; duplicate removal from next-generation sequencing data and a comparison of&amp;nbsp;approaches&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://academic.oup.com/bioinformatics/article/26/6/841/244688"&gt;BEDTools: a flexible suite of utilities for comparing genomic&amp;nbsp;features&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://broadinstitute.github.io/picard/"&gt;Picard Tools - By Broad&amp;nbsp;Institute&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3137218/"&gt;The variant call format and&amp;nbsp;VCFtools&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0974-4"&gt;The Ensembl Variant Effect&amp;nbsp;Predictor&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ensembl.org/info/docs/tools/vep/script/vep_other.html"&gt;Other&amp;nbsp;information&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/snp/"&gt;Home - &lt;span class="caps"&gt;SNP&lt;/span&gt; - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.internationalgenome.org/category/population/"&gt;Population | 1000&amp;nbsp;Genomes&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Tue, 06 Oct 2020 18:00:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2020-10-06:/fastq-to-annotation-part-4.html</guid><category>Unix</category><category>Bioinformatics</category><category>genomic variation</category><category>entrez-direct</category><category>EDirect</category></item><item><title>FASTQ to Annotation (PartÂ 3)</title><link>https://antoniocampos13.github.io/fastq-to-annotation-part-3.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;In a &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;previous post&lt;/a&gt;, I showed how to configure an Ubuntu system to install Bioinformatics&amp;nbsp;programs.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Now, using the environment I created, I will demonstrate a bash script, &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; that takes next generation sequencing (&lt;span class="caps"&gt;NGS&lt;/span&gt;) raw reads from human whole genome sequencing as input and produces variant annotation as output. Variant annotation is the process of identifying genetic variants in some genomic &lt;span class="caps"&gt;DNA&lt;/span&gt; sample, and assess, for example, if any of the found variants have any effect on phenotype, such as increased susceptibility to certain&amp;nbsp;diseases.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In the &lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-1"&gt;first part&lt;/a&gt;, I showed how to search for &lt;span class="caps"&gt;NGS&lt;/span&gt; projects deposited in &lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology Information (&lt;span class="caps"&gt;NCBI&lt;/span&gt;) databases&lt;/a&gt; from which I can download sequencing reads later to use with the&amp;nbsp;script.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In the &lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-2"&gt;second part&lt;/a&gt;, I showed how to retrieve raw genome sequencing reads in the form of &lt;code&gt;FASTQ&lt;/code&gt; files, which are deposited in &lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;&lt;span class="caps"&gt;SRA&lt;/span&gt;&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here in the third part, I make the final preparations for the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script demonstration using the &lt;code&gt;FASTQ&lt;/code&gt; files obtained in the previous&amp;nbsp;part.&lt;/p&gt;
&lt;h2&gt;Final&amp;nbsp;preparations&lt;/h2&gt;
&lt;h3&gt;Installing local cache of Ensembl Variant Effect Predictor (&lt;span class="caps"&gt;VEP&lt;/span&gt;)&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://www.ensembl.org/info/docs/tools/vep/index.html"&gt;Ensembl Variant Effect Predictor (&lt;span class="caps"&gt;VEP&lt;/span&gt;)&lt;/a&gt; is the core tool used by the script for the annotation of the effects of any variants present in the sample. It may be used online, but Ensembl strongly recommends users to download and install a local cache of all data deposited in the tool to avoid server overload. I open a terminal, activate the &lt;code&gt;bioenv&lt;/code&gt; miniconda environment and execute the commands&amp;nbsp;below:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;WARNING&lt;/span&gt;: Several gigabytes of data will be downloaded from the internet and installed on your computer. Be sure that you have plenty or unlimited data allowances from your &lt;span class="caps"&gt;ISP&lt;/span&gt; and sufficient free space on your hard drive before continuing. It will take a while (several minutes to hours) until all the needed processes&amp;nbsp;finish.&lt;/strong&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;vep_install -a cf -s homo_sapiens_refseq -y GRCh38 -c . âCONVERT
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Unfortunately, the command above is prone to network and other esoteric errors. If you have any problem, you can try an alternative manner. See&amp;nbsp;below:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Alternative: manually download the compressed cache&lt;/span&gt;
wget ftp://ftp.ensembl.org/pub/release-101/variation/indexed_vep_cache/homo_sapiens_refseq_vep_101_GRCh38.tar.gz -P &lt;span class="nv"&gt;$HOME&lt;/span&gt;/.vep

&lt;span class="c1"&gt;# Uncompress the cache&lt;/span&gt;
tar -zxf &lt;span class="nv"&gt;$HOME&lt;/span&gt;/.vep/homo_sapiens_refseq_vep_101_GRCh38.tar.gz -C &lt;span class="nv"&gt;$HOME&lt;/span&gt;/.vep
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After some time (several minutes to some hours depending on the network and the capabilities of your computer) the &lt;span class="caps"&gt;VEP&lt;/span&gt; cache will be downloaded and installed into a hidden folder in your home folder (&lt;code&gt;$HOME/.vep&lt;/code&gt;). Therefore, notice that it is independent of miniconda environments. Thus, it is expected that once installed, this cache will work with any miniconda environment on your computer. You may backup the &lt;code&gt;.vep/&lt;/code&gt; folder to avoid downloading the whole thing again (but consider to download newer versions of the cache as they become available&amp;nbsp;though).&lt;/p&gt;
&lt;h3&gt;Obtaining human genome reference&amp;nbsp;files&lt;/h3&gt;
&lt;p&gt;The annotation process require a collection of reference files. These files will assist us to generate a &amp;#8220;list&amp;#8221; of genetic variants, alongside their possible effects on the phenotype (the &lt;strong&gt;annotation&lt;/strong&gt;).&lt;/p&gt;
&lt;p&gt;These files&amp;nbsp;are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A &lt;a href="https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;amp;PAGE_TYPE=BlastDocs&amp;amp;DOC_TYPE=BlastHelp"&gt;&lt;code&gt;FASTA&lt;/code&gt;&lt;/a&gt; file (extensions &lt;code&gt;.fasta&lt;/code&gt;, &lt;code&gt;.fa&lt;/code&gt; or &lt;code&gt;.fna&lt;/code&gt;). It must contain the complete nucleotide sequence of the human genome. We will compare our &lt;code&gt;FASTQ&lt;/code&gt; files against&amp;nbsp;it;&lt;/li&gt;
&lt;li&gt;A &lt;a href="http://www.htslib.org/doc/faidx.html"&gt;&lt;code&gt;FASTA index&lt;/code&gt;&lt;/a&gt; (&lt;code&gt;.fai&lt;/code&gt;). It stores genomic regions as coordinates. We will use it to generate a Browser Extensible Data (&lt;code&gt;.bed&lt;/code&gt;) file (see&amp;nbsp;below);&lt;/li&gt;
&lt;li&gt;A &lt;a href="https://en.wikipedia.org/wiki/BED_(file_format)"&gt;Browser Extensible Data (&lt;code&gt;.bed&lt;/code&gt;)&lt;/a&gt; file. It stores genomic regions as coordinates, indicating the start and end of chromosomes. It is most useful when its information is chromosome-ordered and&amp;nbsp;position-sorted;&lt;/li&gt;
&lt;li&gt;Alternatively, the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script can accept a &lt;a href="https://www.ensembl.org/info/website/upload/gff.html"&gt;General Feature Format (&lt;code&gt;.gff&lt;/code&gt;)&lt;/a&gt; instead of the &lt;code&gt;.bed&lt;/code&gt; file. It is used for describing genes and other features of &lt;span class="caps"&gt;DNA&lt;/span&gt;, &lt;span class="caps"&gt;RNA&lt;/span&gt; and protein&amp;nbsp;sequences;&lt;/li&gt;
&lt;li&gt;Burrows-Wheelers Aligner index files (&lt;code&gt;.amb&lt;/code&gt;, &lt;code&gt;.ann&lt;/code&gt;, &lt;code&gt;.bwt&lt;/code&gt;, &lt;code&gt;.pac&lt;/code&gt; and &lt;code&gt;.sa&lt;/code&gt;). The &lt;a href="http://bio-bwa.sourceforge.net/"&gt;&lt;code&gt;bwa&lt;/code&gt; program&lt;/a&gt; is a short read alignment tool. In other words, it identifies the location of the reads inside the &lt;code&gt;FASTQ&lt;/code&gt; files. The &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script uses &lt;code&gt;bwa&lt;/code&gt; at the second step of the pipeline. It works by efficiently using this collection of five files as a&amp;nbsp;index.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Why we need these files?&lt;/em&gt; Briefly, They serve to map the genomic location of any variant we identify in our samples, as well as the genetic mutation that occurred there, which allows us to predict the possible effect(s) over the phenotype in question (in our case, MAPKi-resistance in melanoma samples). If we compare the genetic variation profile of MAPKi-susceptible samples with MAPKi-resistant samples, we could identify genetic variants associated with the resistances, and perhaps point to new directions of prognosis and new&amp;nbsp;treatments.&lt;/p&gt;
&lt;p&gt;I will now show how to obtain all these files. Remember to &lt;strong&gt;activate&lt;/strong&gt; the &lt;code&gt;miniconda&lt;/code&gt; that you created before if&amp;nbsp;necessary.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class="caps"&gt;WARNING&lt;/span&gt;: Several gigabytes of data will be downloaded from the internet and installed on your computer. Be sure that you have plenty or unlimited data allowances from your &lt;span class="caps"&gt;ISP&lt;/span&gt; and sufficient free space on your hard drive before continuing. It will take a while (several minutes to hours) until all the needed processes&amp;nbsp;finish.&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;1. Human genome &lt;span class="caps"&gt;FASTA&lt;/span&gt;&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# First, create a subfolder into the demo folder to better organize our reference files&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt; demo
mkdir refs
&lt;span class="nb"&gt;cd&lt;/span&gt; refs

&lt;span class="c1"&gt;# Download GRCh38 major release without ALT contigs and with decoy genomes (EBV and hs38d1 contig) from NCBI&amp;#39;s FTP server&lt;/span&gt;
curl -O ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.28_GRCh38.p13/GRCh38_major_release_seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4&gt;2. Human genome &lt;span class="caps"&gt;FASTA&lt;/span&gt;&amp;nbsp;index&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Download from NCBI&amp;#39;s FTP server&lt;/span&gt;
curl -O ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.28_GRCh38.p13/GRCh38_major_release_seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.fai
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4&gt;3. Human genome &lt;span class="caps"&gt;BED&lt;/span&gt;&amp;nbsp;file&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Produce sorted BED file from reference genome index file obtained above&lt;/span&gt;
awk &lt;span class="s1"&gt;&amp;#39;{print $1 &amp;quot;\t0\t&amp;quot; $2}&amp;#39;&lt;/span&gt; GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.fai &lt;span class="p"&gt;|&lt;/span&gt; sort -k1,1V -k2,2n &amp;gt; GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.bed
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4&gt;4. Human genome &lt;span class="caps"&gt;GFF&lt;/span&gt; file (optional alternative to &lt;span class="caps"&gt;BED&lt;/span&gt;&amp;nbsp;file)&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Download from NCBI&amp;#39;s FTP server&lt;/span&gt;
curl -O ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.28_GRCh38.p13/GRCh38_major_release_seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_full_analysis_set.refseq_annotation.gff.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4&gt;5. bwa index&amp;nbsp;files&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;bwa index GCA_000001405.15_GRCh38_no_alt_plus_hs38d1_analysis_set.fna.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When you complete all of the steps in Part 1, Part 2 and in this part, your &lt;code&gt;demo&lt;/code&gt; folder should have the files showed&amp;nbsp;below&lt;/p&gt;
&lt;p&gt;&lt;img alt="demo folder contents until now" src="https://antoniocampos13.github.io/images/demo_folder.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Now, go to the folder &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Unix/2020-10-01_Fastq%20to%20Annotation"&gt;&lt;code&gt;FastQ_to_Annotation&lt;/code&gt; folder in my portfolio&lt;/a&gt;, take heed of the &lt;span class="caps"&gt;GPL&lt;/span&gt; License and Copyright Notice, download and copy the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script into your &lt;code&gt;demo&lt;/code&gt; folder.&lt;/p&gt;
&lt;p&gt;Thus, the only mandatory files are the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt;, the &lt;code&gt;FASTQ&lt;/code&gt; pair and the ones in the &lt;code&gt;refs&lt;/code&gt; folder. If you are missing any other file, do not&amp;nbsp;worry.&lt;/p&gt;
&lt;h2&gt;&lt;span class="caps"&gt;GPL&lt;/span&gt; License and Copyright&amp;nbsp;Notice&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script is a modified version from &lt;a href="https://github.com/kevinblighe/ClinicalGradeDNAseq"&gt;Dr. Kevin Blighe&amp;#8217;s original scripts&lt;/a&gt;. Both works are licensed under &lt;a href="https://www.gnu.org/licenses/licenses.en.html"&gt;&lt;span class="caps"&gt;GNU&lt;/span&gt; General Public License v3.0&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Conclusion of Part&amp;nbsp;3&lt;/h2&gt;
&lt;p&gt;In this part I showed how&amp;nbsp;to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set up &lt;span class="caps"&gt;VEP&lt;/span&gt; local&amp;nbsp;cache;&lt;/li&gt;
&lt;li&gt;Obtain human genome reference&amp;nbsp;files;&lt;/li&gt;
&lt;li&gt;Obtain auxiliary files needed for short read&amp;nbsp;alignment.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, everything is in place for the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-4"&gt;Go to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;4)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-1"&gt;Go back to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;1)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-2"&gt;Go back to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;2)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology&amp;nbsp;Information&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;Home - &lt;span class="caps"&gt;SRA&lt;/span&gt; - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ensembl.org/info/docs/tools/vep/index.html"&gt;Ensembl Variant Effect Predictor (&lt;span class="caps"&gt;VEP&lt;/span&gt;)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web&amp;amp;PAGE_TYPE=BlastDocs&amp;amp;DOC_TYPE=BlastHelp"&gt;&lt;span class="caps"&gt;BLAST&lt;/span&gt; Topics | Query Input and database&amp;nbsp;selection&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.htslib.org/doc/faidx.html"&gt;faidx(5) manual&amp;nbsp;page&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/BED_(file_format)"&gt;&lt;span class="caps"&gt;BED&lt;/span&gt; (file format) -&amp;nbsp;Wikipedia&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ensembl.org/info/website/upload/gff.html"&gt;&lt;span class="caps"&gt;GFF&lt;/span&gt;/&lt;span class="caps"&gt;GTF&lt;/span&gt; File&amp;nbsp;Format&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://bio-bwa.sourceforge.net/"&gt;Burrows-Wheeler&amp;nbsp;Aligner&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/kevinblighe/ClinicalGradeDNAseq"&gt;kevinblighe/ClinicalGradeDNAseq&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.gnu.org/licenses/licenses.en.html"&gt;&lt;span class="caps"&gt;GNU&lt;/span&gt; General Public&amp;nbsp;License&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.28_GRCh38.p13/GRCh38_major_release_seqs_for_alignment_pipelines/"&gt;&lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;#8217;s &lt;span class="caps"&gt;FTP&lt;/span&gt; Server | GRCh38 Major release sequences for alignment&amp;nbsp;pipelines&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://ftp://ftp.ensembl.org/pub/release-101/fasta/homo_sapiens/dna/"&gt;Ensembl&amp;#8217;s &lt;span class="caps"&gt;FTP&lt;/span&gt; Server | Homo sapiens &lt;span class="caps"&gt;DNA&lt;/span&gt; sequences release&amp;nbsp;101&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Mon, 05 Oct 2020 18:00:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2020-10-05:/fastq-to-annotation-part-3.html</guid><category>Unix</category><category>Bioinformatics</category><category>genomic variation</category><category>entrez-direct</category><category>EDirect</category></item><item><title>FASTQ to Annotation (PartÂ 2)</title><link>https://antoniocampos13.github.io/fastq-to-annotation-part-2.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;In a &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis"&gt;previous post&lt;/a&gt;, I showed how to configure an Ubuntu system to install Bioinformatics&amp;nbsp;programs.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Now, using the environment I created, I will demonstrate a bash script, &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; that takes next generation sequencing (&lt;span class="caps"&gt;NGS&lt;/span&gt;) raw reads from human whole genome sequencing as input and produces variant annotation as output. Variant annotation is the process of identifying genetic variants in some genomic &lt;span class="caps"&gt;DNA&lt;/span&gt; sample, and assess, for example, if any of the found variants have any effect on phenotype, such as increased susceptibility to certain&amp;nbsp;diseases.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;In the &lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-1"&gt;first part&lt;/a&gt;, I showed how to search for &lt;span class="caps"&gt;NGS&lt;/span&gt; projects deposited in &lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology Information (&lt;span class="caps"&gt;NCBI&lt;/span&gt;) databases&lt;/a&gt; from which I can download sequencing reads later to use with the&amp;nbsp;script.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Here in the second part, I will show how to retrieve raw genome sequencing reads in the form of &lt;code&gt;FASTQ&lt;/code&gt; files, which are deposited in &lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;&lt;span class="caps"&gt;SRA&lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;But first, let&amp;#8217;s review what &lt;code&gt;FASTQ&lt;/code&gt; files&amp;nbsp;are.&lt;/p&gt;
&lt;h2&gt;What is the the &lt;span class="caps"&gt;FASTQ&lt;/span&gt;&amp;nbsp;format&lt;/h2&gt;
&lt;p&gt;The &lt;a href="https://support.illumina.com/bulletins/2016/04/fastq-files-explained.html"&gt;&lt;code&gt;FASTQ&lt;/code&gt;&lt;/a&gt; file format is how we store the output of whole genome or transcriptomic sequencing (sequences of nucleotides). It inherits its name from the &lt;code&gt;FASTA&lt;/code&gt; format that stores and the word &lt;code&gt;Qualities&lt;/code&gt;, because a &lt;code&gt;FASTQ&lt;/code&gt; file not only contains the nucleotide sequence, but also contains the quality of the sequencing&amp;nbsp;procedure.&lt;/p&gt;
&lt;p&gt;The qualities are represented by Phred scores (&lt;code&gt;Q&lt;/code&gt;), which is used to calculate the probability of a nucleotide being incorrectly identified during sequencing using a formula (I will not go into details here). So, for example, if we check a &lt;code&gt;FASTQ&lt;/code&gt; file and found a nucleotide with &lt;code&gt;Q = 30&lt;/code&gt;, it means that there is a probability of 1 in 1000 that it was incorrectly assigned during sequencing &amp;#8212; in other words an accuracy of 99.9%. Therefore, &lt;code&gt;Q&lt;/code&gt; values around 30 and above are generally seem as very good&amp;nbsp;quality.&lt;/p&gt;
&lt;h3&gt;The reason &lt;code&gt;FASTQ&lt;/code&gt; files contain information about&amp;nbsp;quality&lt;/h3&gt;
&lt;p&gt;Because during use of these kind of files, it is important that we have confidence on the sequence assignment. During processing in Bioinformatics analysis pipelines, we can remove low-quality nucleotides to ensure that he have the &amp;#8220;cleanest&amp;#8221; information&amp;nbsp;possible.&lt;/p&gt;
&lt;h3&gt;Obtaining &lt;span class="caps"&gt;FASTQ&lt;/span&gt;&amp;nbsp;files&lt;/h3&gt;
&lt;p&gt;We obtain &lt;code&gt;FASTQ&lt;/code&gt; after sequencing of genomic samples in platforms such as &lt;a href="https://www.illumina.com"&gt;Illumina&lt;/a&gt;, which practically dominates the &lt;span class="caps"&gt;NGS&lt;/span&gt; market nowadays. Check the fundamentals of Illumina&amp;#8217;s &lt;span class="caps"&gt;NGS&lt;/span&gt; platform &lt;a href="https://www.illumina.com/science/technology/next-generation-sequencing/beginners.html"&gt;here&lt;/a&gt;. Normally, researchers deposit raw &lt;code&gt;FASTQ&lt;/code&gt; files on public databases to share their discoveries with other scientists. This is why I took note of the &lt;code&gt;BioProject&lt;/code&gt; accession &lt;span class="caps"&gt;ID&lt;/span&gt; during the demonstration of Part 1. With this &lt;span class="caps"&gt;ID&lt;/span&gt;, I can retrieve sequencing reads associated with the&amp;nbsp;project.&lt;/p&gt;
&lt;h3&gt;A&amp;nbsp;Warning&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;FASTQ&lt;/code&gt; files, especially from human samples, have very big sizes, in the gigabytes range. Therefore, considerable computing power and storage are needed to process these kind of&amp;nbsp;files.&lt;/p&gt;
&lt;h2&gt;Retrieving reads from a&amp;nbsp;BioProject&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Activate&lt;/strong&gt; the environment, if needed, and connect to &lt;code&gt;SRA&lt;/code&gt; database via &lt;code&gt;EDirect esearch&lt;/code&gt; command using the &lt;code&gt;PRJNA436005&lt;/code&gt; as keyword for query. Then, we pipe the results to the &lt;code&gt;efetch&lt;/code&gt; command. With the &lt;code&gt;-format&lt;/code&gt; flag, it will format the results into the &lt;code&gt;runinfo&lt;/code&gt; format (more on that later). Finally, will save it into the &lt;code&gt;PRJNA436005_runinfo.csv&lt;/code&gt; file. You can choose other name if you&amp;nbsp;wish.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Continuing into the folder I created in the previous part&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt; demo
conda activate bioenv

esearch -db sra -query &lt;span class="s1"&gt;&amp;#39;PRJNA436005&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; efetch -format runinfo &amp;gt; PRJNA436005_runinfo.csv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;runinfo&lt;/code&gt; format displays metadata of read sets. Reads are inferred sequences of base pairs corresponding to &lt;span class="caps"&gt;DNA&lt;/span&gt; fragments produced during procedures for &lt;span class="caps"&gt;NGS&lt;/span&gt;. The collection of &lt;span class="caps"&gt;DNA&lt;/span&gt; fragments from a given sample is called a &lt;strong&gt;library&lt;/strong&gt;, which are sequenced to produce the set of &lt;strong&gt;reads&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Checking the &lt;code&gt;CSV&lt;/code&gt; file, I see that there are seven read sets generated by the project, each displayed on a row, and are identified by the &lt;code&gt;SRR&lt;/code&gt; prefix followed by some numbers. With this &lt;span class="caps"&gt;ID&lt;/span&gt; is possible to retrieve &lt;code&gt;FASTQ&lt;/code&gt; files for each read set. Now I check the &lt;code&gt;LibraryLayout&lt;/code&gt; column to confirm they are all &lt;strong&gt;&lt;span class="caps"&gt;PAIRED&lt;/span&gt;&lt;/strong&gt; reads, meaning that the researchers sequenced both ends of a fragment. Thus, each read set will produce two &lt;code&gt;FASTQ&lt;/code&gt; files, containing the sequences and qualities from all reads obtained from the library of the original sample. This is important to check because the script requires paired&amp;nbsp;reads.&lt;/p&gt;
&lt;p&gt;Other interesting columns that I like to check&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;spots&lt;/code&gt;, which are the number of physical locations in the sequencing flow cells where the sequencing adaptors are fixed. A spot contains several nucleotide bases from several, possibly millions, of&amp;nbsp;reads;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;avgLength&lt;/code&gt;, which as the name implies, is the average length, in nucleotides, of reads in the&amp;nbsp;set;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;size_MB&lt;/code&gt;, the size in megabytes of the read&amp;nbsp;set;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;LibrarySource&lt;/code&gt;, which indicates if the sample source is &lt;span class="caps"&gt;GENOMIC&lt;/span&gt;, &lt;span class="caps"&gt;TRANSCRIPTOMIC&lt;/span&gt; and so&amp;nbsp;on;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Platform&lt;/code&gt;, the vendor of &lt;span class="caps"&gt;NGS&lt;/span&gt;&amp;nbsp;procedure;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Model&lt;/code&gt;, the model of the &lt;code&gt;Platform&lt;/code&gt;;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Sex&lt;/code&gt;, &lt;code&gt;Disease&lt;/code&gt; and &lt;code&gt;Tumor&lt;/code&gt;: descriptors of sample&amp;nbsp;phenotype.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For now, I will use only the first read, which has the &lt;code&gt;SRR6784104&lt;/code&gt; &lt;span class="caps"&gt;ID&lt;/span&gt;, since I will just demonstrate the script use. Finally, let&amp;#8217;s download the read set with the &lt;code&gt;EDirect fastq-dump&lt;/code&gt; command and split it into two files (&lt;code&gt;--split-files&lt;/code&gt; flag), one with reads from each end of &lt;span class="caps"&gt;DNA&lt;/span&gt; fragments in the original library, and compress them with &lt;code&gt;--gzip&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;fastq-dump --split-files SRR6784104 --gzip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After a moment, two &lt;code&gt;fastq.gz&lt;/code&gt; files will be downloaded to the current working directory and are ready to be used as the input for the &lt;code&gt;FastQ_to_VariantCall.sh&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Conclusion (Part&amp;nbsp;2)&lt;/h2&gt;
&lt;p&gt;In this part I showed how&amp;nbsp;to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;obtain and inspect metadata from projects via their &lt;code&gt;BioProjects&lt;/code&gt; accession &lt;span class="caps"&gt;ID&lt;/span&gt;;&lt;/li&gt;
&lt;li&gt;download read sets via their &lt;code&gt;SRR&lt;/code&gt; accession &lt;span class="caps"&gt;ID&lt;/span&gt; via &lt;code&gt;fastq-dump&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I need to do some final preparations before using the &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-3"&gt;Go to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;3)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-1"&gt;Go back to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;1)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis"&gt;Setting Up Your Unix Computer for Bioinformatics&amp;nbsp;Analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-1"&gt;&lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;1)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://support.illumina.com/bulletins/2016/04/fastq-files-explained.html"&gt;&lt;span class="caps"&gt;FASTQ&lt;/span&gt; files&amp;nbsp;explained&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology&amp;nbsp;Information&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;Home - &lt;span class="caps"&gt;SRA&lt;/span&gt; - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.illumina.com"&gt;Illumina | Sequencing and array-based solutions for genetic&amp;nbsp;research&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.illumina.com/science/technology/next-generation-sequencing/beginners.html"&gt;Next-Generation Sequencing for Beginners | &lt;span class="caps"&gt;NGS&lt;/span&gt; basics for&amp;nbsp;researchers&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Fri, 02 Oct 2020 18:00:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2020-10-02:/fastq-to-annotation-part-2.html</guid><category>Unix</category><category>Bioinformatics</category><category>genomic variation</category><category>entrez-direct</category><category>EDirect</category></item><item><title>FASTQ to Annotation (PartÂ 1)</title><link>https://antoniocampos13.github.io/fastq-to-annotation-part-1.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In my &lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;previous post&lt;/a&gt;, I showed how to configure an Ubuntu system to install Bioinformatics&amp;nbsp;programs.&lt;/p&gt;
&lt;p&gt;Now, using the environment I created, I will demonstrate a bash script, &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; that takes next generation sequencing (&lt;span class="caps"&gt;NGS&lt;/span&gt;) raw reads from human whole genome sequencing as input and produces variant annotation as output. Variant annotation is the process of identifying genetic variants in some genomic &lt;span class="caps"&gt;DNA&lt;/span&gt; sample, and assess, for example, if any of the found variants have any effect on phenotype, such as increased susceptibility to certain&amp;nbsp;diseases.&lt;/p&gt;
&lt;p&gt;This demonstration will be separated in four parts. Here in the first part, I will show how to search for &lt;span class="caps"&gt;NGS&lt;/span&gt; projects deposited in &lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology Information (&lt;span class="caps"&gt;NCBI&lt;/span&gt;) databases&lt;/a&gt; from which I can download sequencing reads later to use with the&amp;nbsp;script.&lt;/p&gt;
&lt;h2&gt;Using &lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;#8217;s entrez-direct (EDirect) to retrieve &lt;span class="caps"&gt;FASTQ&lt;/span&gt;&amp;nbsp;files&lt;/h2&gt;
&lt;p&gt;I open my Unix terminal and activate the &lt;code&gt;bioenv&lt;/code&gt; environment:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda activate bioenv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now I use the &lt;code&gt;EDirect&lt;/code&gt; &lt;code&gt;esearch&lt;/code&gt; command to search &lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;#8217;s databases. I must provide a database using the flag &lt;code&gt;-db&lt;/code&gt;. Check the available databases &lt;a href="https://www.ncbi.nlm.nih.gov/books/NBK25497/table/chapter2.T._entrez_unique_identifiers_ui/?report=objectonly"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I will search the &lt;code&gt;biproject&lt;/code&gt; database because it contains metadata from projects dealing with high-throughput genome sequencing, transcriptome expression analysis and so on. I must use the &lt;code&gt;-query&lt;/code&gt; flag to provide keywords for search. In this example, I will search for studies dealing with &lt;strong&gt;vorinostat&lt;/strong&gt;, a medicine that is have been used in experimental &lt;span class="caps"&gt;HIV&lt;/span&gt;-1 latency reversal, or &amp;#8220;shock-and-kill&amp;#8221;&amp;nbsp;treatments.&lt;/p&gt;
&lt;p&gt;Remember to use single quotes (&amp;#8221;) enclosing the query, especially if it has several&amp;nbsp;words.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# It is just the beginning... (1/4)&lt;/span&gt;

&lt;span class="c1"&gt;# Let&amp;#39;s create a folder to organize our files inside&lt;/span&gt;
mkdir demo
&lt;span class="nb"&gt;cd&lt;/span&gt; demo

esearch -db bioproject -query &lt;span class="s1"&gt;&amp;#39;vorinostat&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output is just a &lt;code&gt;XML&lt;/code&gt; summary including, among other things, the number of results&amp;nbsp;retrieved:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;&amp;lt;ENTREZ_DIRECT&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;Db&amp;gt;&lt;/span&gt;bioproject&lt;span class="nt"&gt;&amp;lt;/Db&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;WebEnv&amp;gt;&lt;/span&gt;MCID_5f7726730525f301023dc947&lt;span class="nt"&gt;&amp;lt;/WebEnv&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;QueryKey&amp;gt;&lt;/span&gt;1&lt;span class="nt"&gt;&amp;lt;/QueryKey&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;Count&amp;gt;&lt;/span&gt;61&lt;span class="nt"&gt;&amp;lt;/Count&amp;gt;&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;lt;Step&amp;gt;&lt;/span&gt;1&lt;span class="nt"&gt;&amp;lt;/Step&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/ENTREZ_DIRECT&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this case, the query resulted in 61 results (check the &lt;code&gt;&amp;lt;count&amp;gt;&lt;/code&gt; tag). Thus, I will add more commands to retrieve the actual query results. I will pipe, i.e. transfer, the results of the query to the another command &amp;#8212; &lt;code&gt;efetch&lt;/code&gt; &amp;#8212; that will do this work for me. This is the pipe symbol: &lt;code&gt;|&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# ... not there yet ... (2/4)&lt;/span&gt;
esearch -db bioproject -query &lt;span class="s1"&gt;&amp;#39;vorinostat&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; efetch -format native -mode xml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The output is in &lt;code&gt;XML&lt;/code&gt; format, and it is unfortunately not very much human-readable. Here is a print screen showing the first result. Notice how the record is contained within a &lt;code&gt;&amp;lt;/DocumentSummary&amp;gt;&lt;/code&gt; node:&lt;/p&gt;
&lt;p&gt;&lt;img alt="esearch vorinostat XML result" src="https://antoniocampos13.github.io/images/esearch_vorinostat_xml_results.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Thus, I will once again pipe the results, this time to &lt;code&gt;xtract&lt;/code&gt; command. As its name implies, it extracts information from the &lt;code&gt;XML&lt;/code&gt; and formats into a tab-separated format that is easier to understand. I must input the flag &lt;code&gt;-pattern&lt;/code&gt; with the part of the &lt;code&gt;XML&lt;/code&gt; files that contains the desired information, which are &lt;code&gt;elements&lt;/code&gt;. In this example, I will search inside the &lt;code&gt;DocumentSummary&lt;/code&gt; for &lt;code&gt;ArchiveID@accession&lt;/code&gt; (project unique accession number), &lt;code&gt;ID&lt;/code&gt; (an auxiliary &lt;span class="caps"&gt;ID&lt;/span&gt; code to search for samples of said project), &lt;code&gt;Title&lt;/code&gt;(the title of the project),  &lt;code&gt;Description&lt;/code&gt; (normally an abstract of the project) and &lt;code&gt;Reference&lt;/code&gt; (a list of project-related papers in PubMed ids &amp;#8212; PMIDs, if available). Note that I am separating each argument with spaces, no quotes are necessary in this part of the&amp;nbsp;command.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# ...almost there ... (3/4)&lt;/span&gt;
esearch -db bioproject -query &lt;span class="s1"&gt;&amp;#39;vorinostat&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; efetch -format native -mode xml &lt;span class="p"&gt;|&lt;/span&gt; xtract -pattern DocumentSummary -element ArchiveID@accession ID Title Description Reference
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here is the tab-separated output of the same record displayed on my&amp;nbsp;terminal:&lt;/p&gt;
&lt;p&gt;&lt;img alt="esearch vorinostat xtract result" src="https://antoniocampos13.github.io/images/esearch_vorinostat_xtract_results.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Lastly, I will add a final command to transfer to a local text file &lt;code&gt;vorinostat_projects.txt&lt;/code&gt; that will be saved in the current working directory. Note that if you have a identically-named file in the working directory, it will be overwritten, so be&amp;nbsp;careful.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Finally there! (4/4)&lt;/span&gt;
esearch -db bioproject -query &lt;span class="s1"&gt;&amp;#39;vorinostat&amp;#39;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; efetch -format native -mode xml &lt;span class="p"&gt;|&lt;/span&gt; xtract -pattern DocumentSummary -element ArchiveID@accession ID Reference Title Description &amp;gt; vorinostat_projects.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;vorinostat_projects.txt&lt;/code&gt; file can then be imported into spreadsheets to make easier to organize and read the&amp;nbsp;results.&lt;/p&gt;
&lt;h2&gt;Refining the&amp;nbsp;search&lt;/h2&gt;
&lt;p&gt;The command above is a very basic one to search &lt;span class="caps"&gt;NCBI&lt;/span&gt; databases via &lt;code&gt;EDirect&lt;/code&gt;. I can create more elaborate queries by adding other keywords and filtering results. &lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;#8217;s search engines have several parameters. I advise you go to any advanced search page on the &lt;span class="caps"&gt;NCBI&lt;/span&gt; website to look for the available&amp;nbsp;parameters.&lt;/p&gt;
&lt;p&gt;Using &lt;a href="https://www.ncbi.nlm.nih.gov/bioproject/"&gt;&lt;code&gt;BioProject&lt;/code&gt; database&lt;/a&gt; as example again, click on &lt;em&gt;Advanced&lt;/em&gt; to go the query&amp;nbsp;constructor:&lt;/p&gt;
&lt;p&gt;&lt;img alt="BioProject search box" src="https://antoniocampos13.github.io/images/bioproject_start.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Using the &lt;strong&gt;BioProject Advanced Search Builder&lt;/strong&gt;, I will refine our search. I wish to include only projects that with samples deposited on &lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;Sequence Read Archive (&lt;span class="caps"&gt;SRA&lt;/span&gt;)&lt;/a&gt;, from human samples and that investigated genetic variation. I input all of this into the search&amp;nbsp;boxes:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Refining our search" src="https://antoniocampos13.github.io/images/vorinostat_refined.PNG"&gt;&lt;/p&gt;
&lt;p&gt;Note that clicking in &lt;code&gt;Show index list&lt;/code&gt; will provide a list of curated terms. I used them to filter for &amp;#8220;bioproject sra&amp;#8221; and &amp;#8220;variation&amp;#8221; projects. To filter for organism, it is easier: I simply selected the Organism on the drop-down list on the left of the search box. Finally, I connected all keywords with the &lt;code&gt;AND&lt;/code&gt; Boolean constructor, resulting on the&amp;nbsp;query:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;(((vorinostat) AND "bioproject sra"[Filter]) AND Homo sapiens[Organism]) AND "variation"[Filter]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;You could continue the search on the website, of course, but let&amp;#8217;s go back to the terminal and continue from&amp;nbsp;there:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;esearch -db bioproject -query &lt;span class="s1"&gt;&amp;#39;(((vorinostat) AND &amp;quot;bioproject sra&amp;quot;[Filter]) AND Homo sapiens[Organism]) AND &amp;quot;variation&amp;quot;[Filter]&amp;#39;&lt;/span&gt;  &lt;span class="p"&gt;|&lt;/span&gt; efetch -format native -mode xml &lt;span class="p"&gt;|&lt;/span&gt; xtract -pattern DocumentSummary -element ArchiveID@accession ID Reference Title Description  &amp;gt; vorinostat_refined.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Remember: single quotes enclosing the query. Turns out that this refined search was rather restrictive: it resulted in a single record. Checking the &lt;code&gt;vorinostat_refined.txt&lt;/code&gt; I see in the abstract that the project dealt with samples from patients with melanoma. One of the last sentences says: &amp;#8220;&lt;span class="caps"&gt;DNA&lt;/span&gt; Seq data: biopsy samples from patients pre- and post- treated with Vorinostat; check mutations related to MAPKi-resistance&amp;#8221; (MAPKi: Mitogen Activated Protein Kinase inhibitors). Although I had &lt;span class="caps"&gt;HIV&lt;/span&gt;-1-related projects in mind, that&amp;#8217;s fine for now, since it is suitable to &lt;code&gt;FastQ_to_Annotation.sh&lt;/code&gt; script: identify and annotate genetic&amp;nbsp;variation.&lt;/p&gt;
&lt;p&gt;Then, I take note of the project &lt;span class="caps"&gt;ID&lt;/span&gt;: &lt;code&gt;PRJNA436005&lt;/code&gt;. I will use it to retrieve reads from this project by searching the &lt;span class="caps"&gt;SRA&lt;/span&gt; with&amp;nbsp;it.&lt;/p&gt;
&lt;h2&gt;Conclusion of Part&amp;nbsp;1&lt;/h2&gt;
&lt;p&gt;In this part I showed how&amp;nbsp;to:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;search &lt;span class="caps"&gt;NCBI&lt;/span&gt;&amp;#8217;s databases, (especially&amp;nbsp;BioProject);&lt;/li&gt;
&lt;li&gt;refine&amp;nbsp;searches;&lt;/li&gt;
&lt;li&gt;save search results into local, human-readable text&amp;nbsp;files.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Now I need to use the information gathered here to download read sets in &lt;code&gt;FASTQ&lt;/code&gt; format.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a href="https://antoniocampos13.github.io/fastq-to-annotation-part-2"&gt;Go to &lt;span class="caps"&gt;FASTQ&lt;/span&gt; to Annotation (Part&amp;nbsp;2)&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html"&gt;Setting Up Your Unix Computer for Bioinformatics&amp;nbsp;Analysis&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/"&gt;National Center for Biotechnology&amp;nbsp;Information&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/books/NBK25497/table/chapter2.T._entrez_unique_identifiers_ui/?report=objectonly"&gt;Entrez Unique Identifiers (UIDs) for selected&amp;nbsp;databases&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/bioproject/"&gt;Home - BioProject - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.ncbi.nlm.nih.gov/sra"&gt;Home - &lt;span class="caps"&gt;SRA&lt;/span&gt; - &lt;span class="caps"&gt;NCBI&lt;/span&gt;&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Thu, 01 Oct 2020 18:00:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2020-10-01:/fastq-to-annotation-part-1.html</guid><category>Unix</category><category>Bioinformatics</category><category>genomic variation</category><category>entrez-direct</category><category>EDirect</category></item><item><title>Setting Up Your Unix Computer for BioinformaticsÂ Analysis</title><link>https://antoniocampos13.github.io/setting-up-your-unix-computer-for-bioinformatics-analysis.html</link><description>&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this post I will show how I set up my Unix machine to use Bioinformatics programs and tools. I am currently using Ubuntu 20.04 &lt;span class="caps"&gt;LTS&lt;/span&gt; (Focal Fossa) 64-bit on a &lt;a href="https://www.digitalocean.com/community/posts/trying-the-new-wsl-2-its-fast-windows-subsystem-for-linux"&gt;Windows Subsystem for Linux (&lt;span class="caps"&gt;WSL2&lt;/span&gt;)&lt;/a&gt; on Windows 10, so no &lt;span class="caps"&gt;GUI&lt;/span&gt;&amp;nbsp;today!&lt;/p&gt;
&lt;p&gt;The code and files used here can be retrieved from &lt;a href="https://github.com/antoniocampos13/portfolio/tree/master/Unix/2020-09-30_Setting%20Up%20Your%20Unix%20Computer%20for%20Bioinformatics%20Analysis"&gt;this post corresponding folder on my portfolio&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Preparing the&amp;nbsp;system&lt;/h2&gt;
&lt;p&gt;First, it is recommended that we upgrade the system. Open the command line terminal in your machine and copy and paste or type the following commands, pressing Enter after each one (make sure you type your password correctly whenever&amp;nbsp;asked):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sudo apt-get update
sudo apt-get upgrade
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then I must install some useful libraries, especially to be sure that all future libraries I need will be installed and work properly. Some of these (e.g. default-jdk, the Java libraries), may already be installed in your system, but just to&amp;nbsp;ensure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;sudo apt-get install -y curl unzip build-essential ncurses-dev
sudo apt-get install -y byacc zlib1g-dev python-dev git cmake
sudo apt-get install -y default-jdk ant
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Installing&amp;nbsp;(mini)conda&lt;/h2&gt;
&lt;p&gt;Now I will install &lt;a href="https://conda.io/miniconda.html"&gt;miniconda&lt;/a&gt;. What is miniconda? Miniconda is a simplified version of Conda, an environment management system. Every program we install on our computers depend on other programs to work. So if a program X needs a program Y to work, it may stop working if Y gets an update that for some reason is incompatible with the original X&amp;nbsp;program.&lt;/p&gt;
&lt;p&gt;Thus, environments were developed to solve this kind of problem, because they serve to isolate groups of programs, ensuring only compatible versions of software are working together. Therefore, miniconda serves to create and manage environments. The best practice is that one should create one environment for one specific use. In my case, I installed miniconda to create a environment and populate it with tools used for several Bioinformatics analysis. Other people can create environments for other uses with specific programs needed and so on. Other advantage of miniconda is that the configuration files for environments can be shared with others, ensuring &lt;strong&gt;backup&lt;/strong&gt; and &lt;strong&gt;reproducibility&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Without further ado, let&amp;#8217;s finally install miniconda. Since I am using a Unix with Python 3.7.7 pre-installed, the version of the installer &lt;a href="https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"&gt;is this one&lt;/a&gt;. Check the &lt;a href="https://docs.conda.io/en/latest/miniconda.html#linux-installers"&gt;installation page&lt;/a&gt; if you have a different Python&amp;nbsp;version.&lt;/p&gt;
&lt;p&gt;You can download the installer from your browser or via command&amp;nbsp;line:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh &amp;gt; Miniconda3-latest-Linux-x86_64.sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, go to the folder where the installer was downloaded and run the&amp;nbsp;script:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;bash Miniconda3-latest-Linux-x86_64.sh

./Miniconda3-latest-Linux-x86_64.sh &lt;span class="c1"&gt;# same effect&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When the installation finishes, I must initialize&amp;nbsp;conda:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;miniconda3/condabin/conda init
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Close the terminal and open it again. Now miniconda must be ready to use. Check by typing and pressing&amp;nbsp;Enter:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Then, I added two &lt;strong&gt;channels&lt;/strong&gt;. Channels are &lt;a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-channels.html"&gt;&amp;#8220;the locations where packages are stored&amp;#8221;&lt;/a&gt;. Miniconda has the &lt;code&gt;defaults&lt;/code&gt; channel pre-configured. The two channels in question are dedicated to Bioinformatics and Data analysis programs, which may not be present in the default channels, so I must add&amp;nbsp;them.&lt;/p&gt;
&lt;h2&gt;Configuring miniconda&amp;nbsp;channels&lt;/h2&gt;
&lt;p&gt;Once again in the terminal enter the following&amp;nbsp;commands:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda config --add channels bioconda
conda config --add channels conda-forge
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Miniconda sets up priorities in the list of channels it receives. When we need to install some program, miniconda will search in the higher-priority channels first, then in the channels with lower-priority. &amp;#8220;Different channels can have the same package&amp;#8221; and you can &amp;#8220;safely put channels at the bottom of your channel list to provide additional packages that are not in the default channels&amp;#8221; as stated in the &lt;a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-channels.html"&gt;official website&lt;/a&gt;. The flag &lt;code&gt;--add&lt;/code&gt; adds the respective channels (&lt;code&gt;bioconda&lt;/code&gt; and &lt;code&gt;conda-forge&lt;/code&gt;) to the &lt;strong&gt;top&lt;/strong&gt; of the priorities list. If you want to give lower priority, putting them in the &lt;strong&gt;bottom&lt;/strong&gt; of the list, use the &lt;code&gt;--append&lt;/code&gt; command instead. Thus, according to the command above, the order of channel priorities in our new miniconda installation will be: &lt;code&gt;conda-forge&lt;/code&gt;, &lt;code&gt;bioconda&lt;/code&gt; and lastly, &lt;code&gt;defaults&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Create an environment for Bioinformatics&amp;nbsp;programs&lt;/h2&gt;
&lt;p&gt;Now that miniconda is configured, I will create the environment that will receive them. I will name it &lt;code&gt;bioenv&lt;/code&gt;. You can choose whatever name you&amp;nbsp;like! &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda create -y --name bioenv &lt;span class="nv"&gt;python&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;.6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Activating and deactivating an&amp;nbsp;environment&lt;/h2&gt;
&lt;p&gt;With the &lt;code&gt;bioenv&lt;/code&gt; created, I must &lt;strong&gt;activate&lt;/strong&gt;&amp;nbsp;it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda activate bioenv
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I need to perform this step every time I want to use the programs that I will install in this environment. If you do not need to use the environment for the moment, simply &lt;strong&gt;deactivate&lt;/strong&gt;&amp;nbsp;it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda deactivate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Simply &lt;strong&gt;activate&lt;/strong&gt; it again when&amp;nbsp;needed.&lt;/p&gt;
&lt;h2&gt;Installing&amp;nbsp;programs&lt;/h2&gt;
&lt;p&gt;Now we can finally install our programs. Activate the environment again (only if you have deactivated it). Download the &lt;a href="https://raw.githubusercontent.com/antoniocampos13/portfolio/master/Unix/2020-09-30_Setting%20Up%20Your%20Unix%20Computer%20for%20Bioinformatics%20Analysis/bioenv.txt"&gt;&lt;code&gt;bioenv.txt&lt;/code&gt; file&lt;/a&gt; in my GitHub repository. This file contains a selection of most used Bioinformatics programs (hat tip to &lt;a href="https://www.biostarhandbook.com/index.html"&gt;Dr. IstvÃ¡n Albert&lt;/a&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;cat bioenv.txt &lt;span class="p"&gt;|&lt;/span&gt; xargs conda install -y
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Backing up and restoring your environment&amp;nbsp;configuration&lt;/h2&gt;
&lt;p&gt;Miniconda has a special command to backup your environment configuration. &lt;strong&gt;Activate&lt;/strong&gt; (if needed) the environment you want to backup and enter the&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda env &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep -v &lt;span class="s2"&gt;&amp;quot;prefix&amp;quot;&lt;/span&gt; &amp;gt; bioenv.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It will result in a &lt;code&gt;YAML&lt;/code&gt; file in the current working folder containing all configurations in your environment. Again, I named the file &lt;code&gt;bioenv.yml&lt;/code&gt; but you can choose whatever you like. Note that if you already have a &lt;code&gt;bioenv.yml&lt;/code&gt; in your directory, it will be overwritten, so be&amp;nbsp;careful.&lt;/p&gt;
&lt;p&gt;To restore this environment in your computer, or on other computer, first install miniconda again, and then use the&amp;nbsp;command:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;conda env create -f bioenv.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;-f&lt;/code&gt; flag means you are creating an environment using the configurations in the &lt;code&gt;bioenv.yml&lt;/code&gt; file. The first line of the yml file sets the new environment&amp;#8217;s name, so you can change it in the file if you like. It will also restore the channels configured in the previous installation of&amp;nbsp;miniconda.&lt;/p&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This is how I configured my system so I could use the major Bioinformatics tools out there. In summary,&amp;nbsp;I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Prepared an Unix (Ubuntu)&amp;nbsp;system;&lt;/li&gt;
&lt;li&gt;Installed miniconda, an environment&amp;nbsp;manager;&lt;/li&gt;
&lt;li&gt;Configured channels so I could retrieve desired&amp;nbsp;software;&lt;/li&gt;
&lt;li&gt;Created an environment, showed how to activate and deactivate it, and finally installed software in&amp;nbsp;it;&lt;/li&gt;
&lt;li&gt;Showed how to backup your environment for safekeeping or sharing with&amp;nbsp;others.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In future posts I will demo some uses of the installed programs I  in the new&amp;nbsp;environment.&lt;/p&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/antoniocampos13/portfolio"&gt;My&amp;nbsp;Portfolio&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.digitalocean.com/community/posts/trying-the-new-wsl-2-its-fast-windows-subsystem-for-linux"&gt;Trying the New &lt;span class="caps"&gt;WSL&lt;/span&gt; 2. It&amp;#8217;s Fast! (Windows Subsystem for Linux) |&amp;nbsp;DigitalOcean&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://conda.io/miniconda.html"&gt;Miniconda&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh"&gt;Miniconda&amp;nbsp;installer&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.conda.io/en/latest/miniconda.html#linux-installers"&gt;Miniconda &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Conda&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-channels.html"&gt;Managing channels; conda 4.8.4.post65+1a0ab046&amp;nbsp;documentation&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.biostarhandbook.com/index.html"&gt;The Biostar Handbook: 2nd&amp;nbsp;Edition&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Antonio Victor Campos Coelho</dc:creator><pubDate>Wed, 30 Sep 2020 18:00:00 -0300</pubDate><guid isPermaLink="false">tag:antoniocampos13.github.io,2020-09-30:/setting-up-your-unix-computer-for-bioinformatics-analysis.html</guid><category>Unix</category><category>Bioinformatics</category></item></channel></rss>